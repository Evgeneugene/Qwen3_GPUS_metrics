{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09eafc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323928c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 8.37s\n",
      "Огнегрив с волнением наблюдал, как в лесной чаще скрывается тень Ячменя. В его глазах мелькали тревога и неуверенность. Он знал, что Синяя Звезда не станет сомневаться в его словах, но не мог не чувствовать собственного смятения. Горелый — не просто кот, а человек, и его\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 10000 input, 98 output\n",
      "TTFT: 8.37 seconds\n",
      "Gen tokens/sec (post-TTFT): 19.12\n",
      "Total tokens: 10098\n",
      "Input tokens: 10000\n",
      "Output tokens: 98\n",
      "Total time: 13.50 seconds\n",
      "Generation time: 5.12 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def measure_ollama_response(file_path, ollama_url=\"http://localhost:11434/api/chat\", \n",
    "                           max_input_tokens=None, max_output_tokens=1024, time_limit=7):\n",
    "    \"\"\"\n",
    "    Send file content to Ollama chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        ollama_url: Ollama endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "        time_limit: Maximum time in seconds for generation (after prefill), None for no limit\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3:8b\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"stream\": True,\n",
    "        \"think\": False,\n",
    "        \"options\": {\n",
    "            \"num_predict\": max_output_tokens,\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_ctx\": 32768\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    generation_timeout = False\n",
    "    \n",
    "    # Send request to Ollama with streaming\n",
    "    response = requests.post(ollama_url, json=payload, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    \n",
    "    print(\"Response streaming:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                if 'message' in data and 'content' in data['message']:\n",
    "                    # Record TTFT (time to first token)\n",
    "                    if ttft is None:\n",
    "                        ttft = time.time() - start_time\n",
    "                        print(f\"\\nFirst token received at {ttft:.2f}s\")\n",
    "                    \n",
    "                    # Check time limit after first token (generation phase)\n",
    "                    if time_limit is not None and ttft is not None:\n",
    "                        generation_time = time.time() - start_time - ttft\n",
    "                        if generation_time > time_limit:\n",
    "                            print(f\"\\nGeneration timeout reached ({time_limit}s limit)\")\n",
    "                            generation_timeout = True\n",
    "                            break\n",
    "                    \n",
    "                    content_chunk = data['message']['content']\n",
    "                    full_response += content_chunk\n",
    "                    \n",
    "                    # Stream to console\n",
    "                    print(content_chunk, end='', flush=True)\n",
    "                \n",
    "                # Check if this is the final message\n",
    "                if data.get('done', False):\n",
    "                    break\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    print()  # New line after streaming\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Estimate output tokens using tokenizer\n",
    "    output_tokens = len(tokenizer.encode(full_response))\n",
    "    print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time,\n",
    "        'generation_timeout': generation_timeout\n",
    "    }\n",
    "    \n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    if metrics.get('generation_timeout', False):\n",
    "        print(\"Generation was interrupted due to timeout\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\")\n",
    "        if metrics.get('generation_timeout', False):\n",
    "            f.write(\"generation_timeout: true\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"tests/book.txt\"\n",
    "ollama_url = \"http://localhost:11434/api/chat\"\n",
    "max_input_tokens = 10000\n",
    "max_output_tokens = 100\n",
    "\n",
    "response, metrics = measure_ollama_response(file_path, ollama_url, max_input_tokens, max_output_tokens)\n",
    "save_and_print_metrics(response, metrics, \"ollama_test_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a39367b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up the model with a test request...\n",
      "Content truncated from 13961 to 100 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 5.50s\n",
      "Спасибо, я готов! Д\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 100 input, 9 output\n",
      "Warmup completed in 5.87s\n",
      "Model is ready for testing.\n",
      "\n",
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing with 1000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 1000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 1.80s\n",
      "Очень интересный фрагмент из книги **«Огонь и лед»** Эрины Хантер, которая является частью серии **«Племена Котов»**. Этот пролог дает первые впечатления о напряжённой обстановке в мире котов, о конфликтах между племенами и о том, как коты сталкиваются с угрозами со стороны Двун\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 1000 input, 99 output\n",
      "Request 1000_1_1 completed in 6.87s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 6.87s\n",
      "Valid results: 1/1\n",
      "Average time per request: 6.87s\n",
      "Average TTFT: 1.80 seconds\n",
      "Average Tokens/sec: 21.18\n",
      "Average Total tokens: 1099\n",
      "Average Total time: 6.47 seconds\n",
      "Set [1000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 5000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 5000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 8.37s\n",
      "В глазах Крутобока заиграли веселье, и они с Огнегривом, не сводя глаз с друг друга, одновременно схватили свои кусочки еды, будто соревнуясь, кто угадает, кто первый проглотит.  \n",
      "\n",
      "— Великие воители, ведомые солнцем, — насмешливо произнес Крутобок,\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 5000 input, 99 output\n",
      "Request 5000_1_1 completed in 13.89s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 13.89s\n",
      "Valid results: 1/1\n",
      "Average time per request: 13.89s\n",
      "Average TTFT: 8.37 seconds\n",
      "Average Tokens/sec: 19.35\n",
      "Average Total tokens: 5099\n",
      "Average Total time: 13.49 seconds\n",
      "Set [5000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 10000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 10000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 8.37s\n",
      "Огнегрив с волнением наблюдал, как солнечные лучи, пробиваясь сквозь листву, рисовали на земле тонкие узоры. Синяя Звезда молчала, её глаза были устремлены вдаль, словно она видела нечто, что скрывалось за туманом леса. Огнегрив ж\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 10000 input, 99 output\n",
      "Request 10000_1_1 completed in 13.86s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 13.86s\n",
      "Valid results: 1/1\n",
      "Average time per request: 13.86s\n",
      "Average TTFT: 8.37 seconds\n",
      "Average Tokens/sec: 19.14\n",
      "Average Total tokens: 10099\n",
      "Average Total time: 13.54 seconds\n",
      "Set [10000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 15000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 15000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 8.47s\n",
      "— **Чем при** — начал Черняк, и его голос прозвучал как ледяной ветер, заставляющий вздрогнуть всех присутствующих.  \n",
      "— **Чем при** — повторил он, словно проверяя, слышат ли его коты, — **придётся уплатить за каждый кусочек рыбы, который вы возьмёте!**\n",
      "\n",
      "В т\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 15000 input, 99 output\n",
      "Request 15000_1_1 completed in 13.95s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 13.95s\n",
      "Valid results: 1/1\n",
      "Average time per request: 13.95s\n",
      "Average TTFT: 8.47 seconds\n",
      "Average Tokens/sec: 19.31\n",
      "Average Total tokens: 15099\n",
      "Average Total time: 13.60 seconds\n",
      "Set [15000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 20000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 20000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 8.39s\n",
      "**Продолжение истории:**\n",
      "\n",
      "— **Вам уже приходилось бывать на землях племени Ветра,** — напомнила Синяя Звезда. — Вы знаете, как они живут, как они охотятся, как они держатся перед лицом опасности. Это будет не просто поход — это будет настоящая миссия, требующая смелости, ума и предан\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 20000 input, 99 output\n",
      "Request 20000_1_1 completed in 13.96s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 13.96s\n",
      "Valid results: 1/1\n",
      "Average time per request: 13.96s\n",
      "Average TTFT: 8.39 seconds\n",
      "Average Tokens/sec: 19.29\n",
      "Average Total tokens: 20099\n",
      "Average Total time: 13.52 seconds\n",
      "Set [20000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 25000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 25000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 8.41s\n",
      "(Продолжение текста)\n",
      "\n",
      "в направлении, откуда исходил слабый, но знакомый запах. Крутобок, не отставая, тоже втянул воздух и начал двигаться следом. Вокруг них царила тишина, нарушаемая лишь шорохом ветра и тихим бормочением старых камней.\n",
      "\n",
      "— Похоже, они ушли, \n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 25000 input, 99 output\n",
      "Request 25000_1_1 completed in 13.97s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 13.97s\n",
      "Valid results: 1/1\n",
      "Average time per request: 13.97s\n",
      "Average TTFT: 8.41 seconds\n",
      "Average Tokens/sec: 19.29\n",
      "Average Total tokens: 25099\n",
      "Average Total time: 13.54 seconds\n",
      "Set [25000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 30000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 30000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 8.44s\n",
      "built\n",
      "\n",
      "**Глава V (продолжение)**\n",
      "\n",
      "Огнегрив почувствовал, что у него дрожат лапы. Неужели они забрались под саму Гремящую тропу? Он беспокойно распушил свою рыжую шерсть и почувствовал под боком щекочущий кошачий запах. Крутобок, который шёл впереди\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 99 output\n",
      "Request 30000_1_1 completed in 14.03s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 14.03s\n",
      "Valid results: 1/1\n",
      "Average time per request: 14.03s\n",
      "Average TTFT: 8.44 seconds\n",
      "Average Tokens/sec: 19.26\n",
      "Average Total tokens: 30099\n",
      "Average Total time: 13.58 seconds\n",
      "Set [30000 tokens - 1 parallel] completed. Proceeding to next set...\n"
     ]
    }
   ],
   "source": [
    "# Warmup request to prepare the model\n",
    "print(\"Warming up the model with a test request...\")\n",
    "warmup_response, warmup_metrics = measure_ollama_response(\n",
    "    file_path=\"tests/daily.txt\",\n",
    "    ollama_url=\"http://localhost:11434/api/chat\",\n",
    "    max_input_tokens=100,\n",
    "    max_output_tokens=10\n",
    ")\n",
    "print(f\"Warmup completed in {warmup_metrics['total_time']:.2f}s\")\n",
    "print(\"Model is ready for testing.\\n\")\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "ollama_url = \"http://localhost:11434/api/chat\"\n",
    "input_token_sizes = [1000, 5000, 10000, 15000 , 20000, 25000, 30000]\n",
    "# input_token_sizes = [20000, 25000, 30000]\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, ollama_url, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_ollama_response(\n",
    "            file_path=file_path,\n",
    "            ollama_url=ollama_url,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            time_limit=7\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "concurrent_counts = [1] #,  2, 5] \n",
    "\n",
    "for token_size in input_token_sizes:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Testing with {token_size} input tokens\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for concurrent_count in concurrent_counts:\n",
    "        print(f\"\\n=== {token_size} tokens with {concurrent_count} concurrent requests ===\")\n",
    "        print(f\"Waiting for all current requests to complete before starting this set...\")\n",
    "        \n",
    "        threads = []\n",
    "        start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        # Start concurrent requests\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: results.append(run_concurrent_test(file_path, ollama_url, token_size, 100, f\"{token_size}_{concurrent_count}_{i+1}\"))\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for ALL threads to complete before proceeding to next set\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            print(f\"Thread {i+1}/{concurrent_count} completed\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_concurrent_time = end_time - start_time\n",
    "        \n",
    "        # Filter out None results (failed requests only)\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        print(f\"All {concurrent_count} requests completed in {total_concurrent_time:.2f}s\")\n",
    "        print(f\"Valid results: {len(valid_results)}/{concurrent_count}\")\n",
    "        print(f\"Average time per request: {total_concurrent_time/concurrent_count:.2f}s\")\n",
    "        \n",
    "        # Save aggregated metrics\n",
    "        # Get model name from Ollama API\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "            models_data = response.json()\n",
    "            model_name = models_data['models'][0]['name'].replace(\":\", \"-\") if models_data['models'] else \"unknown_model\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get model name from Ollama API: {e}\")\n",
    "            model_name = \"ollama-unknown\"  # fallback\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        output_dir = f\"speed_tests/A2_x1/{model_name}\"\n",
    "        # ----------------------------\n",
    "\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{token_size}_length_{concurrent_count}_parallel.txt\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        if valid_results:\n",
    "            avg_ttft = sum(r['ttft'] for r in valid_results) / len(valid_results)\n",
    "            avg_gen_tokens_per_sec = sum(r['gen_tokens_per_sec'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_tokens = sum(r['total_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_time = sum(r['total_time'] for r in valid_results) / len(valid_results)\n",
    "            avg_input_tokens = sum(r['input_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_output_tokens = sum(r['output_tokens'] for r in valid_results) / len(valid_results)\n",
    "            \n",
    "            # Create average metrics dictionary\n",
    "            avg_metrics = {\n",
    "                'ttft': avg_ttft,\n",
    "                'gen_tokens_per_sec': avg_gen_tokens_per_sec,\n",
    "                'total_tokens': avg_total_tokens,\n",
    "                'total_time': avg_total_time,\n",
    "                'input_tokens': avg_input_tokens,\n",
    "                'output_tokens': avg_output_tokens\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Average TTFT: {avg_metrics['ttft']:.2f} seconds\")\n",
    "            print(f\"Average Tokens/sec: {avg_metrics['gen_tokens_per_sec']:.2f}\")\n",
    "            print(f\"Average Total tokens: {avg_metrics['total_tokens']:.0f}\")\n",
    "            print(f\"Average Total time: {avg_metrics['total_time']:.2f} seconds\")\n",
    "            \n",
    "            # Save average metrics to file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ttft: {avg_metrics['ttft']:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_metrics['total_tokens']:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_metrics['total_time']:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_metrics['input_tokens']:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_metrics['output_tokens']:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {len(valid_results)}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\\n\")\n",
    "                f.write(f\"Average metrics for {len(valid_results)}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics for ALL valid results\n",
    "                f.write(\"Individual request metrics:\\n\")\n",
    "                for i, metrics in enumerate(valid_results):\n",
    "                    f.write(f\"\\nRequest {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        else:\n",
    "            print(\"No valid results to save\")\n",
    "        \n",
    "        # Add a pause between different concurrent count sets for the same token size\n",
    "        print(f\"Set [{token_size} tokens - {concurrent_count} parallel] completed. Proceeding to next set...\")\n",
    "        time.sleep(2)  # Brief pause to ensure system is ready for next set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f74c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
