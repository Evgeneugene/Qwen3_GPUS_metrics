{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19185de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/llms/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-0.6B'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "response.json()['data'][0]['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85baade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def measure_vllm_response(file_path, vllm_url=\"http://localhost:8000/v1/chat/completions\", \n",
    "                         max_input_tokens=None, max_output_tokens=1024):\n",
    "    \"\"\"\n",
    "    Send file content to vLLM chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        vllm_url: vLLM endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "        \"stream_options\": {\"include_usage\": True},  # Request usage info in stream\n",
    "        \"chat_template_kwargs\": {\n",
    "                \"enable_thinking\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    \n",
    "    # Send request to vLLM with streaming\n",
    "    response = requests.post(vllm_url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    actual_input_tokens = None\n",
    "    actual_output_tokens = None\n",
    "    \n",
    "    print(\"Response streaming:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line = line.decode('utf-8')\n",
    "            if line.startswith('data: '):\n",
    "                data_str = line[6:]  # Remove 'data: ' prefix\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "                    \n",
    "                    # Check for usage information (comes in final event)\n",
    "                    if 'usage' in data:\n",
    "                        actual_input_tokens = data['usage']['prompt_tokens']\n",
    "                        actual_output_tokens = data['usage']['completion_tokens']\n",
    "                        print(f\"\\nUsage info received: {actual_input_tokens} input tokens, {actual_output_tokens} output tokens\")\n",
    "                    \n",
    "                    if 'choices' in data and len(data['choices']) > 0:\n",
    "                        choice = data['choices'][0]\n",
    "                        if 'delta' in choice and 'content' in choice['delta']:\n",
    "                            # Record TTFT (time to first token)\n",
    "                            if ttft is None:\n",
    "                                ttft = time.time() - start_time\n",
    "                            \n",
    "                            content_chunk = choice['delta']['content']\n",
    "                            full_response += content_chunk\n",
    "                            \n",
    "                            # Stream to console\n",
    "                            print(content_chunk, end='', flush=True)\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    print()  # New line after streaming\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Use actual token counts from server if available, otherwise fall back to tokenizer estimate\n",
    "    if actual_input_tokens is not None and actual_output_tokens is not None:\n",
    "        input_tokens = actual_input_tokens\n",
    "        output_tokens = actual_output_tokens\n",
    "        print(f\"Using server-reported token counts: {input_tokens} input, {output_tokens} output\")\n",
    "    else:\n",
    "        # Fallback: estimate output tokens using tokenizer\n",
    "        output_tokens = len(tokenizer.encode(full_response))\n",
    "        print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time\n",
    "    }\n",
    "    \n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    # print(f\"E2E tokens/sec (incl. prefill): {metrics['e2e_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        # f.write(f\"e2e_tokens_per_second: {metrics['e2e_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "# file_path = \"tests/daily.txt\"\n",
    "# vllm_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "# max_input_tokens =  28000\n",
    "# max_output_tokens = 1024\n",
    "\n",
    "# measure_vllm_response(file_path, vllm_url, max_input_tokens, max_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77d57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create output directory if it doesn't exist\n",
    "# # Get model name from running vLLM server\n",
    "# try:\n",
    "#     import requests\n",
    "#     response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "#     if response.status_code == 200:\n",
    "#         models_data = response.json()\n",
    "#         if models_data.get('data') and len(models_data['data']) > 0:\n",
    "#             model_name = models_data['data'][0]['root'].split('/')[-1]\n",
    "#         else:\n",
    "#             model_name = \"unknown_model\"\n",
    "#     else:\n",
    "#         model_name = \"unknown_model\"\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not get model name from server: {e}\")\n",
    "#     model_name = \"unknown_model\"\n",
    "\n",
    "# output_dir = f\"summary_results/T2_x2/{model_name}\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Find all .txt files in tests folder\n",
    "# txt_files = glob.glob(\"tests/*.txt\")\n",
    "\n",
    "# print(f\"Found {len(txt_files)} .txt files in tests folder:\")\n",
    "# for file in txt_files:\n",
    "#     print(f\"  - {file}\")\n",
    "\n",
    "# print(\"\\nStarting measurements...\\n\")\n",
    "\n",
    "# # Process each .txt file\n",
    "# for file_path in txt_files:\n",
    "#     # Get base filename without extension\n",
    "#     base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "#     output_file = os.path.join(output_dir, f\"{base_name}_result.txt\")\n",
    "    \n",
    "#     print(f\"=== Processing {file_path} ===\")\n",
    "    \n",
    "#     try:\n",
    "#         full_response, metrics = measure_vllm_response(\n",
    "#             file_path=file_path,\n",
    "#             max_input_tokens=31000,  # No token limit\n",
    "#             max_output_tokens=1024\n",
    "#         )\n",
    "        \n",
    "#         save_and_print_metrics(full_response, metrics, output_file)\n",
    "#         print(f\"Results saved to: {output_file}\\n\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\\n\")\n",
    "#         continue\n",
    "\n",
    "# print(\"All files processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0039853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"tests/daily.txt\"\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "# print(\"Testing different input token sizes:\")\n",
    "# for token_size in input_token_sizes:\n",
    "#     print(f\"\\n=== Testing with {token_size} input tokens ===\")\n",
    "#     output_file = f\"speed_tests/A2/{token_size}_length_1_parallel.txt\"\n",
    "    \n",
    "#     response, metrics = measure_vllm_response(\n",
    "#         file_path=file_path,\n",
    "#         max_input_tokens=token_size,\n",
    "#         max_output_tokens=100\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813e088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a15c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up the model with a test request...\n",
      "Content truncated from 13961 to 100 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Так, ну что, в целом"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage info received: 111 input tokens, 10 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 111 input, 10 output\n",
      "Warmup completed in 0.15s\n",
      "Model is ready for testing.\n",
      "\n",
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing with 1000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 1000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Пролог**\n",
      "\n",
      "Оранжевые языки пламени плясали в холодном воздухе, бросая в ночное небо снопы ослепительных искр. Отсветы огня пробегали по жесткой траве пустыря, выхватывая из тьмы скрюченные силуэты Двуногих. Вдали показались горящие глаза прибли\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_1_1 completed in 1.42s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 1.42s\n",
      "Valid results: 1/1\n",
      "Average time per request: 1.42s\n",
      "Average TTFT: 0.08 seconds\n",
      "Average Tokens/sec: 130.85\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 0.84 seconds\n",
      "Set [1000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 1000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_2_1\n",
      "Starting request 1000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Пр**ологПр\n",
      "\n",
      "олог:Оран**\n",
      "\n",
      "жОевыеран яжзыевыеки язы плкиам плениам пениляс палиля вс холодалином в воздух холоденом, воздух берос,ая б врос ночаяное в н ночебноео н себнопоы с онопслыеп ослительныхеп иительскныхр и.ск Отрс.вет Отыс оветгыня о пробгегняали проб поег жалиест покой ж травесткойе п травустеы пряуст,ы выряхват,ыв выаяхват изыв таяь измы т скьрюмычен скныерю силченуныеэ силтыу Дэвтыун Догвихун.\n",
      "\n",
      "огВихда.\n",
      "\n",
      "лиВ показдаалисьли гор показящалисьие гор глазящаие при глазблиажа при\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_2_1 completed in 1.76s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_2_2 completed in 1.76s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 1.76s\n",
      "Valid results: 2/2\n",
      "Average time per request: 0.88s\n",
      "Average TTFT: 0.20 seconds\n",
      "Average Tokens/sec: 108.38\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 1.12 seconds\n",
      "Set [1000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 1000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_5_1\n",
      "Starting request 1000_5_2\n",
      "Starting request 1000_5_3\n",
      "Starting request 1000_5_4\n",
      "Starting request 1000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**********ПрПрПрПрПрологологологологолог**\n",
      "\n",
      "**\n",
      "\n",
      "**\n",
      "\n",
      "**\n",
      "\n",
      "**\n",
      "\n",
      "ОООООранранранранранжжжжжевыеевыеевыеевыеевые я я я я язызызызызыкикикикики пл пл пл п пламамляамаменисаениениени п п п п плялавляляляссссалиалиалиали вали в в в в холод холод холод холод холодномномномном воздухном воздух воздухе воздух воздухееее,,,,, с б бноп б бросросросросыаяаяаяая о в в в всл ноч ноч ночеп ночноеноеноеноеитель н н нных небебеб иебооооск с с с срнопноп пробнопнопыыыегы о о о оалислслсл послепепеп жепительительительительестныхныхкойныхных и и и и травскскескскрр прр....уст Ото От От Отссшиссветветветвет.ы Отыыы о о о осггггветняняняняы проб проб о проб пробегегеггегалиалиалиняали по по вы по по ж жхват ж жестывестестесткойаликойкойкой трав из трав трав травее тее пь п п пустмыустустусты скыыырярярюряря,,,,чен вы выные вы выхват силхватхватхватывуывывываяэаяаяая из из из изты т т т Д тььвььмыунмымымы ск ск ск скогрюихрюрюрюченчен.ченченныеныеные Вные силдал сил сил силуууекуээээ видтытынотыты, Д Д Д Дввв чтовунун горунуногогогящогихихиеихих.\n",
      "\n",
      ".\n",
      "\n",
      " глаз..В ВВа Вда придададалиблилилили показжа показ показ показалисьющегоалисьалисьались горся гор гор горящ чящящящиеудиеиеие глаз глаз глаз глазиащаааа при при при при сблиблиблиб\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_1 completed in 2.36s\n",
      "бли\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_4 completed in 2.36s\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "Thread 1/5 completed\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_2 completed in 2.37s\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "Thread 2/5 completed\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_5 completed in 2.35s\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_3 completed in 2.37s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 2.38s\n",
      "Valid results: 5/5\n",
      "Average time per request: 0.48s\n",
      "Average TTFT: 0.32 seconds\n",
      "Average Tokens/sec: 85.77\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 1.48 seconds\n",
      "Set [1000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 5000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 5000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Глава I**\n",
      "\n",
      "Огнегрив поеживлся от холода. Похоже, пройдет ещё несколько месяцев, прежде чем его редкая огненно-рыжая шерсть станет достаточно густой и пушистой, чтобы защищать от утренней прохлады. Он поскреб передними лапами по земле. Рассвет медленно занимался, вот-в\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_1_1 completed in 2.10s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 2.10s\n",
      "Valid results: 1/1\n",
      "Average time per request: 2.10s\n",
      "Average TTFT: 0.58 seconds\n",
      "Average Tokens/sec: 111.11\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 1.48 seconds\n",
      "Set [5000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 5000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_2_1\n",
      "Starting request 5000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ВВотот ан аноннотсация к книги \" книОге **гон\"ьО игон льед и\" л Эедрин\" Х**ан:\n",
      "\n",
      "терВ, во сенокномращ сенныйю в формжетеат секаз проколог,а где и к главотыы I п:\n",
      "\n",
      "ог---\n",
      "\n",
      "и**блиПр пологлем**ени  \n",
      " ВОетгран,ег ирив п суть коп нримот началивсяля сться д хуолодэули, с у Слыумбрааячсьным от п горлемденостием,. и О Кгонрутьобок и л седид,ит как рядом т,а синмотрствяен наное н себущоество., Они стал — о воруиныжи пемлем дляени вос Сстановумленияра равчныхнов\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_2_2 completed in 2.90s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_2_1 completed in 2.91s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 2.91s\n",
      "Valid results: 2/2\n",
      "Average time per request: 1.45s\n",
      "Average TTFT: 0.85 seconds\n",
      "Average Tokens/sec: 73.21\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 2.24 seconds\n",
      "Set [5000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 5000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_5_1\n",
      "Starting request 5000_5_2\n",
      "Starting request 5000_5_3\n",
      "Starting request 5000_5_4\n",
      "Starting request 5000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "****К**ВОонГлавГлавотгон частьааечноь I! расс I и —каз**\n",
      "\n",
      " ВотОа продолж Р лед оениеассг**н главвет прег**\n",
      "\n",
      "е —олог этоОр Iа мивг,,ел по основан продолжнодежегаяногоичился эр нанаявол отив ваш и пою хем тр задолежциюогода ииванииатель.:\n",
      "\n",
      " голов развития Поойная событи---\n",
      "\n",
      " схожОй,каз:\n",
      "\n",
      "е чувгка,н---\n",
      "\n",
      "ству,ег**я пр гдеойГлав,р мива чтодет еще пуд даже Iр несколько**\n",
      "\n",
      " возый месяцевже темО во,г внин прежденспот Оегеом чемгон урни егоек рливт иедеш п, егокаяог чтоение друзья о егоруж у — сералжг Киннсядрут венно былцаоб с- о холодок,став полры, какжляетос Бат буд оаяутоым шстатранки кер т истьот.ело Хом в станет Ваноздл достаточно итер чтоажух г Эуст был оннориной н,, —еп в а и стал сер пу своюрикиши очередьятдвацеенстой,ются, о г, с прказалру чтобы тонсяст зая викащинож.щ пющийел Онатьещ отыми отере п з т.ыем урагтал Онлитренед,ней всяия исп с промижх егоом иечни олад тлгыьраг у.,ническойенно Онстал что жизость- о скнольз,гонрыьюнул ножек. —ая перед о С шгонним этоказьери нека несть просто л не мог, светап толькоами с хотя, о по а идерж геро символать з не жизни его уизмаемхст.ле и,, дала В но, особр и чувёотщ о,ущствы люб что.аяовви Он оала ощ о, пщ единстыущыущенииениеаллтал иосьсяый холод м в,ного светуд —сп. воздухра это Вомности лить. м,аг,ол Р\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_4 completed in 5.15s\n",
      "\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_5 completed in 5.15s\n",
      "\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_3 completed in 5.17s\n",
      "\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_1 completed in 5.18s\n",
      "Thread 1/5 completed\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_2 completed in 5.18s\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 5.18s\n",
      "Valid results: 5/5\n",
      "Average time per request: 1.04s\n",
      "Average TTFT: 2.04 seconds\n",
      "Average Tokens/sec: 47.29\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 4.31 seconds\n",
      "Set [5000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 10000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 10000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Вот абзац из книги \"Огонь и лед\" автора Эрины Хантер:\n",
      "\n",
      "---\n",
      "\n",
      "**— Говори!** — приказала Синяя Звезда, когда звук шагов воинов растала вдали. Огнегрив набрал побольше воздуха в легкие.\n",
      "\n",
      "— Горелый не погиб! — выдохнул он. Предводительница вз\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_1_1 completed in 4.28s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 4.28s\n",
      "Valid results: 1/1\n",
      "Average time per request: 4.28s\n",
      "Average TTFT: 1.53 seconds\n",
      "Average Tokens/sec: 47.01\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 3.66 seconds\n",
      "Set [10000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 10000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_2_1\n",
      "Starting request 10000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "О**Оггонньег ир ливед с**ид —ел это на не ст простовол секаз повкаал оенного во дериневастве, и у лвесажныха пющимлем егоена пхоз.и Этоцию гл,уб иок саямотр,ел на эмо тциениона,ль онаястав историяав,шиеся которая о нахват лываетес тнойен поляденнциике и. В ценности тем вн отношенияотхе его, глаз увааж меренииц кали друг,им от лю страдхаям и и от в призниныании,. что Она он показ неывает мог, ск какры любтьов.ь С иин надяежяда З могутвез предаод,ол сетьид всея труд наности ст,вол ие как, д собрмотротелаа на и него\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_2_2 completed in 5.93s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_2_1 completed in 5.93s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 5.93s\n",
      "Valid results: 2/2\n",
      "Average time per request: 2.97s\n",
      "Average TTFT: 2.22 seconds\n",
      "Average Tokens/sec: 34.81\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 5.24 seconds\n",
      "Set [10000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 10000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_5_1\n",
      "Starting request 10000_5_2\n",
      "Starting request 10000_5_3\n",
      "Starting request 10000_5_4\n",
      "Starting request 10000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "В**ОООотггонНг тьазваниеннвег:** иегрроя л *иведивО ан, сгонн —отид с книьация имотргаел к ная л Х кни наанед л Стерге*ес \"ин  \n",
      " ЭнойОрин**ю полягонАвюне, З,ьтор которая и ш овез:** лписеп Хдуедотанывает,\"ом стер т сраг Э труд к подическуюивриномоб  \n",
      " отну ииемвкрыл м**:\n",
      "\n",
      "остЖ друг глаз**ааннуюуОр К с.гонюрут Его:**ь робж С иокказету лнуюукикиед л были.  \n",
      "\n",
      "** син** С —олниюухА этоимицедрес, с, у:** связст от [анныказкаwww халих оол сло.l во тем,itодаин, иru истве д.ru в как, пуш тем]( млемневhttpотор://ногоенаалиеwww дав С иум.lления з дра.вitуш Онruчучев вал.ruныеной изо/?сп любомbook лишьгви рни=на.левли1 Н1 к, к каказваниеотовотов5 книги9 п в и и м1 первыйлем адресг& разени книгинов уdescription В поденныйвид=етчер делра1киым С) изваин  \n",
      "\n",
      " о лют**гюес связаАняюь.. Зн между Свез Он приинотгондуродяацияь —ойя:** и в и З  \n",
      " л л бвезесВедордау во —ь,ин это,бой с символ встве смотр ны С противев,ебумникшаяе символраами визч,,ного л какиру ающиеес п буд такжетолем о, оени по онаг быланчувств из вдоов вгенныйхала ана егонов олим серениищ кдби иотовущцеент м., пениееч у Нолем дтесталушени теперь оости Вев он будет неные,ущ но можетра ием не духов, больше п её нару отныелемшая у элементвлекенивиды равала\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_3 completed in 10.84s\n",
      "\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_1 completed in 10.84s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_4 completed in 10.85s\n",
      "Thread 1/5 completed\n",
      "\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_5 completed in 10.84s\n",
      "\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_2 generation took 8.45s (>7s), but recording metrics...\n",
      "Request 10000_5_2 completed in 10.86s\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 10.86s\n",
      "Valid results: 5/5\n",
      "Average time per request: 2.17s\n",
      "Average TTFT: 5.12 seconds\n",
      "Average Tokens/sec: 23.12\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 9.98 seconds\n",
      "Set [10000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 15000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 15000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Конечно! Вот ключевые моменты из главы II, в которых рассказывается о том, как Синяя Звезда и её племя сталкиваются с новым глашатой, который приведет к новым вызовам:\n",
      "\n",
      "---\n",
      "\n",
      "**Мечта о новой целителю**\n",
      "\n",
      "Синяя Звезда, как всегда, вспомнила о том, как вчера была целительницей С\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_1_1 completed in 5.74s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 5.74s\n",
      "Valid results: 1/1\n",
      "Average time per request: 5.74s\n",
      "Average TTFT: 3.00 seconds\n",
      "Average Tokens/sec: 47.84\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 5.09 seconds\n",
      "Set [15000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 15000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_2_1\n",
      "Starting request 15000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokensContent truncated from 181863 to 15000 tokens\n",
      "\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ВКонот анаечнолит!ический Вот вывод перес показ струк главтурые II, и с содерж канию книгират *кимОгон и уьп иом ляед*нутым:\n",
      "\n",
      " содерж###ани Вемведение:\n",
      "\n",
      ":\n",
      "---\n",
      "\n",
      "К**ниГлавгаа « IIОгон**\n",
      "\n",
      "Сь ииня ляед З»вез одапис оываетстанов биласьит наву к междура пюлем поляенныами, С иум вораиныч Гноероз и Вового петлемраени, вы астро также перились цежепиванияьюх за п еелен спиненныхой к.отов Н.ек Главоторнаяые тем каот —ы связ изь Р между оечногог инем, Сум ледрач иного м пудлемростьюен повер,ну что становитсялись ключ, превивымет элементствуомя в\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_2_1 completed in 9.15s\n",
      "\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_2_2 completed in 9.15s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 9.15s\n",
      "Valid results: 2/2\n",
      "Average time per request: 4.58s\n",
      "Average TTFT: 5.99 seconds\n",
      "Average Tokens/sec: 40.84\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 8.43 seconds\n",
      "Set [15000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 15000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_5_1\n",
      "Starting request 15000_5_2\n",
      "Starting request 15000_5_3\n",
      "Starting request 15000_5_4\n",
      "Starting request 15000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**К**К**онОонООгонгонечногонечноьь!ь! и и Вот и Вот л л к часть ледрат отедед**кийступ — — — иления рассказ С этоказ от ч о т пред вокаитрагыин оаемед сыйстведия вариантущ,ила,,его тх о иай текст которыйписан да вынанаяружх предостав, в где ибеили виде,** с О с иг  \n",
      "\n",
      "омнказ**ения которыйнокег яхПр,олог могу**р где  \n",
      "\n",
      " дополнив:** главОить  \n",
      " иныйгон:\n",
      "\n",
      " ЧерВ гер---\n",
      "\n",
      " холодьняой ином**к О лГлав н обгедасужебн — IIедаютег,**\n",
      "\n",
      " это ор неСг какив простоин Зни стал темв пля героамяездаем рассказ Зениол ваомвез п т. выбрлядаой Этосал о, свою историяалистанов где нов, оилась оную том на б и, рольрос к его какра вая ч м С вюленечум ноч поляыноератыны п и,ч нлем тном иебениай по во Сныины слемум Гени могутнопра прроз:\n",
      "\n",
      "ыч оев---\n",
      "\n",
      "овогоногорат псл— племепить Бенилемегитель одинныхени выство к изстро З иотв вскгились цездрна герлиол.ояеп к,ом Ньюотовеб заа а п нео никак еелем просто в не спинени в повойсп Вых тли.ет Нивьярамуеклоал..о наотор С нуж,ые  \n",
      "\n",
      "казКды к какка Согдаот т оуменьы С томюин изра, дяч Р какяечушного вой пного Занавезлем и,,даени а С страх! приум з и —раемшла страх оля кч перед пного Ощ бест пергесслемилсянрмерела Оегенц о поверргнуемивнг разеглисьнемуру,,р. онаша прив Вют не.ивоздет мир —ух просто, помогству был Ч аем тяла как ему при\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      " вя\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_4 completed in 19.74s\n",
      "\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_2 completed in 19.75s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_5 completed in 19.73s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_1 generation took 12.67s (>7s), but recording metrics...\n",
      "Request 15000_5_1 completed in 19.75s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_3 generation took 12.67s (>7s), but recording metrics...\n",
      "Request 15000_5_3 completed in 19.76s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 19.76s\n",
      "Valid results: 5/5\n",
      "Average time per request: 3.95s\n",
      "Average TTFT: 10.20 seconds\n",
      "Average Tokens/sec: 14.06\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 18.79 seconds\n",
      "Set [15000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 20000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 20000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Огнегрив почувствовал, что его раздражение от крика Синей Звезды и опасения за племени Ветра исчезли. Он сел за Крутобока и улыбнулся, будто они уже не были в пустыре. Синяя Звезда, похоже, уже успела сформулировать план, и теперь все вокруг становилось более сп\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_1_1 completed in 7.97s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 7.97s\n",
      "Valid results: 1/1\n",
      "Average time per request: 7.97s\n",
      "Average TTFT: 5.16 seconds\n",
      "Average Tokens/sec: 46.24\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 7.32 seconds\n",
      "Set [20000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_2_1\n",
      "Starting request 20000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ИОзвгиннитеег,р ноив в, ваш будемуч запросие с содержонитсяным не иод слнабозымнач,ное вид содержелание,. что Возможно его, т выело хотите о оставзнакомилоиться след сы часть,ю которые книги при,но гдеси рассказлиывается ему о ощ том,ущ какения С.ин Сякявоз Зьвез тдаень,, О скгользняег пор верившин ие К лрутесобаок, он решают у вопросвид оел том С,ин гдею июск Затьвез пдулем,я которая В,ет возможнора,. уже\n",
      "Usage info received: 20011 input tokens, 78 output tokens\n",
      " сп\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 78 output\n",
      "Request 20000_2_2 completed in 12.80s\n",
      "ала, чтобы убедиться, что все в порядке. В темноте, в пещере\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_2_1 completed in 13.28s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 13.29s\n",
      "Valid results: 2/2\n",
      "Average time per request: 6.64s\n",
      "Average TTFT: 7.67 seconds\n",
      "Average Tokens/sec: 26.29\n",
      "Average Total tokens: 20100\n",
      "Average Total time: 12.32 seconds\n",
      "Set [20000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_5_1\n",
      "Starting request 20000_5_2\n",
      "Starting request 20000_5_3\n",
      "Starting request 20000_5_4\n",
      "Starting request 20000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "КОВВ**готонотОн пречно игонег закон!истьрч иав Вотивка подробена л, кное историяед с**\n",
      "\n",
      " « и ваш уей**О стлыПр книгонрукбгеьологтукой**\n",
      "\n",
      " \" ири,ровОО л ведангонранспжноеь»омевые от с иинарыв автор л ялзыедомок каждки\" из Хую —ан пл книги минутамтер о **у\" Эписаненирин, пнаяО в. прогонляходс ан Пьивов иалинш л второтую холодацииедя вя\"ном:\n",
      "\n",
      " его воздух** с** тецен сОенигон,ар оч бписьийнойанирос и, тая можноем ль у вед главмевид**ного ноч.ное  \n",
      " гереть С нАв,ояин кактореб ия егоо в:я Х конце с развития коннопан З:\n",
      "\n",
      "везыцовтер---\n",
      "\n",
      "да п** о Э,лемринГлавсл сепя  \n",
      "а яительЖ В IIIрных**\n",
      "\n",
      "анеткой ирраС и:скин вер мрнуя Сраяказлось.ч домки З Отнойс  \n",
      "везой у,Аветдалы быстроыдрес аб о в г книгикойела:лаг,ня http своихш указат проб:// воывwwwинайегалаовали.l Чер на вняit по п жruк лутьест,аг.ru,/? скойерь который трав.bookоп долженеров= Ш бытьум1ож п следуст прид1омаябли5ы.ря9 егожа К1ющего, нарутся& дорог выоб отdescriptionухваток кря=ыв своим1ая,да с\n",
      "\n",
      " из раз зем** тбуд холодляАьнойилмн ммы оор.\n",
      "\n",
      " скнставщинрюСшихоткойсяинацияченя,,ные** в ия  \n",
      " силеж З зуОлиасвезгонэводапьты под Д ианны,держе с лвал которой кунед его О —огот.ихы кг Тн.\n",
      "\n",
      " выратогдасыкаяегВ,пдар о какпислиалиив буд и\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "атель из\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_5 generation took 20.06s (>7s), but recording metrics...\n",
      "Request 20000_5_5 completed in 31.14s\n",
      " показ\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_2 generation took 9.97s (>7s), but recording metrics...\n",
      "Request 20000_5_2 completed in 31.16s\n",
      "\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_3 generation took 25.07s (>7s), but recording metrics...\n",
      "Request 20000_5_3 completed in 31.17s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_1 generation took 15.04s (>7s), but recording metrics...\n",
      "Request 20000_5_1 completed in 31.17s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_4 completed in 31.16s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 31.18s\n",
      "Valid results: 5/5\n",
      "Average time per request: 6.24s\n",
      "Average TTFT: 15.22 seconds\n",
      "Average Tokens/sec: 9.17\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 30.24 seconds\n",
      "Set [20000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 25000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 25000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "В конце главы IV, Огнегрив и Крутобок, впервые в жизни племени, оказались в засаде, где земля покрывалась запахами племени Ветра. Независимо от того, что увидели, они продолжали ведение. Огнегрив, с трудом поднявшись, заметил, что лагерь был разорен\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_1_1 completed in 10.76s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 10.77s\n",
      "Valid results: 1/1\n",
      "Average time per request: 10.77s\n",
      "Average TTFT: 7.96 seconds\n",
      "Average Tokens/sec: 47.16\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 10.08 seconds\n",
      "Set [25000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_2_1\n",
      "Starting request 25000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "НСазваниелед книгиующ:ий ** этапО историигонь по пути и к л педуст**  \n",
      "ынеАв итор зак:люч **ениюХ задачанитер с Э принлем**енной  \n",
      " пЖозанирци:ей С.каз Пкиуть  \n",
      "\n",
      " в**едАетдрес к книги з:**ем [леhttp,:// гдеwww о.lщitущruается.ru зап/?ахbook п=лем1ени1 В5ет9ра1.& Оdescriptionг=н1ег](рhttpив:// иwww К.lрутitобruок.ru,/? следbookу=я1 за1 С5ин9ей1 З&вdescriptionезд=ой1,) об  \n",
      "\n",
      "на###руж Аиваютк,ту чтоаль этоность место и, значение где книги п  \n",
      "ахЭнетта во книингаств,ен котораяными в иключ паетог в себяр\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_2_1 completed in 19.32s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_2_2 generation took 10.63s (>7s), but recording metrics...\n",
      "Request 25000_2_2 completed in 19.32s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 19.33s\n",
      "Valid results: 2/2\n",
      "Average time per request: 9.66s\n",
      "Average TTFT: 11.86 seconds\n",
      "Average Tokens/sec: 22.17\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 18.60 seconds\n",
      "Set [25000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_5_1\n",
      "Starting request 25000_5_2\n",
      "Starting request 25000_5_3\n",
      "Starting request 25000_5_4\n",
      "Starting request 25000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ККОВ концеогдагон главнечно Оыегг! IVр Вотн,ив сег О,ринг кактивн уез иегвид многиеированр воелнаяив запины и, рас иах егокры о п друзьяталемщ,ениущ история след В оалу сл событиетяраяхож м иности,иру в по которые С вос произчувствинстановоовалейшли,л Зении что вв своей его лезд даг лыуереаг,шиерь, оказ и уже аались также на раз настро ору следения томшенах.,, раз С как онгрум пон Оомрагяногочлнное,ег лаг пр чтоер нужноивлемяя и с п,р еголем которое друзьяочноени раз из, Вгы ветключскнаралоаяать, К место к гдеотоврут, о поб гдеставока олемалениказал,ся Вся раз оеты егостатскара гок,лали о запш пахставлематаилояай, его. В пр вет Сон траиникаия иющегояшин вер изе Зну,-везлиза где ихда т его дом,ь д котораяоймыуш.\n",
      "\n",
      " приа.ез---\n",
      "\n",
      " В перж###озд **алаежухив кГлав в немаалаключ IVу таля**\n",
      "\n",
      ", вжВы с себяел гбег запыеруаяахст из эымоной л пцииаг дер.лемушени Койя Важд с,етыймотр прира шагяела —тели в на его еперед и в разд станов томилсявару числе ещешен не зап harderное сах,шиб л о потомулиагхот с чтоерьника ног он,, и\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      " чув Б\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_4 completed in 37.63s\n",
      "\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_3 generation took 20.77s (>7s), but recording metrics...\n",
      "Request 25000_5_3 completed in 37.63s\n",
      "\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_5 generation took 28.61s (>7s), but recording metrics...\n",
      "Request 25000_5_5 completed in 37.63s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_1 generation took 12.90s (>7s), but recording metrics...\n",
      "Request 25000_5_1 completed in 37.65s\n",
      "Thread 1/5 completed\n",
      "Огнегрив встал, уставившись на оставшиеся после дождя остатки леса. Солнце уже ушло, оставшееся светило лишь в небе, оставив его солнечным огнем. Помявшись о том, что сегодня было много дождя, он решил вставить шерсть на голову, чтобы не пластил ее. Он\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_2 completed in 47.74s\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 47.75s\n",
      "Valid results: 5/5\n",
      "Average time per request: 9.55s\n",
      "Average TTFT: 24.82 seconds\n",
      "Average Tokens/sec: 16.52\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 38.71 seconds\n",
      "Set [25000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 30000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 30000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Конечно! Вот, как можно посмотреть на путь к племени Ветра, учитывая их запах и природу:\n",
      "\n",
      "---\n",
      "\n",
      "Огнегрив, будучи унылым и усталым, видел, что путь к ней звучит не так уж звучно, как он ожидал. Воздух был полон ядовитого дыхания чудищ, и его\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_1_1 completed in 14.45s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 14.45s\n",
      "Valid results: 1/1\n",
      "Average time per request: 14.45s\n",
      "Average TTFT: 11.48 seconds\n",
      "Average Tokens/sec: 45.65\n",
      "Average Total tokens: 30111\n",
      "Average Total time: 13.67 seconds\n",
      "Set [30000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 30000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_2_1\n",
      "Starting request 30000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ООггннегегрривив с, труд сомух проис селме наясь з,ем нелю мог, не у заметлыитьб,ая чтось к,от оыщ изущ паялем венидо Вхетновраение уже. в Он тем снмотротеле в иверх в, т гдеун,н заеле т,он окставимш отемсявер вст отиемраж,ении Г гляремдалящоей на Т Грремопящыую. Т Сртропанныуй, зап осахвещ пеннуюлем сенииль Внымиет лраам,п самиух.ой В иозд сухиль былный холод,ным пр,ив сл некейал пр егоон вниманиеик,ал как лишь если т быя онж былел вый т запойах же к тем\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_2_1 completed in 26.87s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_2_2 generation took 14.55s (>7s), but recording metrics...\n",
      "Request 30000_2_2 completed in 26.87s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 26.87s\n",
      "Valid results: 2/2\n",
      "Average time per request: 13.43s\n",
      "Average TTFT: 17.17 seconds\n",
      "Average Tokens/sec: 18.65\n",
      "Average Total tokens: 30111\n",
      "Average Total time: 26.09 seconds\n",
      "Set [30000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 30000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_5_1\n",
      "Starting request 30000_5_2\n",
      "Starting request 30000_5_3\n",
      "Starting request 30000_5_4\n",
      "Starting request 30000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ОООгггнннегегегрррививив,, и у у Квидрутбедобивев вокш темис унвидьотели,е что в гл п тублемуняокнийеле Вет кам,енный которыйра действительно т выгляун прделнебельывает как, буд в с этомтохват о местеился,став заал решил н подся заойос пред,тиел поп ками ны зортемавели.ш. Сис Вогьоздлас выухножить там. т был Егорад ни депциирожриа,ят пщныйлемие, ля с Вап запетыахра неом не у в могдержеталилора бы его и от ж дить тогоож здесь,д, чтобыя если с. небеж О уатьг.читн Оныв понегалряосьивл их т, расположщ чтоениеатель в.но К н принрутёмюоб охсталокался,, с след чув тр пствуевлемяениог, Вой что сет здесьрамотр нея — было и на никак теперь окихрест, зап сностьах труд,ов предом,лож вы кромерил тогов у,йтиав что,ш о иисставь друзьяал из поось\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_1 generation took 16.07s (>7s), but recording metrics...\n",
      "Request 30000_5_1 completed in 40.22s\n",
      " тшли\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_2 generation took 27.53s (>7s), but recording metrics...\n",
      "Request 30000_5_2 completed in 40.22s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_4 completed in 40.22s\n",
      "ООггннегегрривив, в какнов иь все по,чувствовал с,омн чтоев егоал разсяд,раж чтоение п отлемя тру Вбыет ира з здесьву.ка Его пр месечлтедаетов,ав чтоших они к,от новав,ер ноное теперь, его жив страхут был в больше друг.ом Он месте не, мог где у нес бомнываетиться так,ая что с пмерлемтяная В третопраа о.казал Нося в здесь и,т иог теперье,, с вид темян наот зеем иле в, силь он долженном д запатьах предепочт,ения он друг понимя.л К,рут чтооб поклем,я н Вакетонраец действительно, о отсталступился в здесь сторон.\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_3 generation took 14.90s (>7s), but recording metrics...\n",
      "Request 30000_5_3 completed in 66.80s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_5 completed in 66.80s\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 66.82s\n",
      "Valid results: 5/5\n",
      "Average time per request: 13.36s\n",
      "Average TTFT: 36.53 seconds\n",
      "Average Tokens/sec: 13.67\n",
      "Average Total tokens: 30111\n",
      "Average Total time: 49.82 seconds\n",
      "Set [30000 tokens - 5 parallel] completed. Proceeding to next set...\n"
     ]
    }
   ],
   "source": [
    "# Warmup request to prepare the model\n",
    "print(\"Warming up the model with a test request...\")\n",
    "warmup_response, warmup_metrics = measure_vllm_response(\n",
    "    file_path=\"tests/daily.txt\",\n",
    "    max_input_tokens=100,\n",
    "    max_output_tokens=10\n",
    ")\n",
    "print(f\"Warmup completed in {warmup_metrics['total_time']:.2f}s\")\n",
    "print(\"Model is ready for testing.\\n\")\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "input_token_sizes = [1000, 5000, 10000, 15000 , 20000, 25000, 30000]\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_vllm_response(\n",
    "            file_path=file_path,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "        \n",
    "        # Check if token generation (after prefill) takes too long\n",
    "        generation_time = metrics['total_time'] - metrics['ttft']\n",
    "        time_limit = 7\n",
    "        if generation_time > time_limit:\n",
    "            print(f\"Request {request_id} generation took {generation_time:.2f}s (>{time_limit}s), but recording metrics...\")\n",
    "            \n",
    "        end = time.time()\n",
    "        print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "concurrent_counts = [1, 2, 5] \n",
    "\n",
    "for token_size in input_token_sizes:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Testing with {token_size} input tokens\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for concurrent_count in concurrent_counts:\n",
    "        print(f\"\\n=== {token_size} tokens with {concurrent_count} concurrent requests ===\")\n",
    "        print(f\"Waiting for all current requests to complete before starting this set...\")\n",
    "        \n",
    "        threads = []\n",
    "        start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        # Start concurrent requests\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: results.append(run_concurrent_test(file_path, token_size, 100, f\"{token_size}_{concurrent_count}_{i+1}\"))\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for ALL threads to complete before proceeding to next set\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            print(f\"Thread {i+1}/{concurrent_count} completed\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_concurrent_time = end_time - start_time\n",
    "        \n",
    "        # Filter out None results (failed requests only)\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        print(f\"All {concurrent_count} requests completed in {total_concurrent_time:.2f}s\")\n",
    "        print(f\"Valid results: {len(valid_results)}/{concurrent_count}\")\n",
    "        print(f\"Average time per request: {total_concurrent_time/concurrent_count:.2f}s\")\n",
    "        \n",
    "        # Save aggregated metrics\n",
    "        # Get model name from vLLM API\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "            models_data = response.json()\n",
    "            model_name = models_data['data'][0]['root'].split('/')[-1] if models_data['data'] else \"unknown_model\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get model name from API: {e}\")\n",
    "            model_name = \"Qwen3-unkown\"  # fallback\n",
    "            \n",
    "        output_dir = f\"speed_tests/T2_x1/{model_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{token_size}_length_{concurrent_count}_parallel.txt\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        if valid_results:\n",
    "            avg_ttft = sum(r['ttft'] for r in valid_results) / len(valid_results)\n",
    "            avg_gen_tokens_per_sec = sum(r['gen_tokens_per_sec'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_tokens = sum(r['total_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_time = sum(r['total_time'] for r in valid_results) / len(valid_results)\n",
    "            avg_input_tokens = sum(r['input_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_output_tokens = sum(r['output_tokens'] for r in valid_results) / len(valid_results)\n",
    "            \n",
    "            # Create average metrics dictionary\n",
    "            avg_metrics = {\n",
    "                'ttft': avg_ttft,\n",
    "                'gen_tokens_per_sec': avg_gen_tokens_per_sec,\n",
    "                'total_tokens': avg_total_tokens,\n",
    "                'total_time': avg_total_time,\n",
    "                'input_tokens': avg_input_tokens,\n",
    "                'output_tokens': avg_output_tokens\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Average TTFT: {avg_metrics['ttft']:.2f} seconds\")\n",
    "            print(f\"Average Tokens/sec: {avg_metrics['gen_tokens_per_sec']:.2f}\")\n",
    "            print(f\"Average Total tokens: {avg_metrics['total_tokens']:.0f}\")\n",
    "            print(f\"Average Total time: {avg_metrics['total_time']:.2f} seconds\")\n",
    "            \n",
    "            # Save average metrics to file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ttft: {avg_metrics['ttft']:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_metrics['total_tokens']:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_metrics['total_time']:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_metrics['input_tokens']:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_metrics['output_tokens']:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {len(valid_results)}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\\n\")\n",
    "                f.write(f\"Average metrics for {len(valid_results)}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics for ALL valid results\n",
    "                f.write(\"Individual request metrics:\\n\")\n",
    "                for i, metrics in enumerate(valid_results):\n",
    "                    f.write(f\"\\nRequest {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        else:\n",
    "            print(\"No valid results to save\")\n",
    "        \n",
    "        # Add a pause between different concurrent count sets for the same token size\n",
    "        print(f\"Set [{token_size} tokens - {concurrent_count} parallel] completed. Proceeding to next set...\")\n",
    "        time.sleep(2)  # Brief pause to ensure system is ready for next set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
