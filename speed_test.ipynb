{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dc996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea450c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0,1 python -m vllm.entrypoints.openai.api_server   --model zhiqing/Qwen3-14B-INT8   --served-model-name qwen3   --tensor-parallel-size 2   --max-model-len 28000   --port 8000 --max-num-seqs 10 --gpu-memory-utilization 0.9 --no-enable-prefix-caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19185de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zhiqing/Qwen3-14B-INT8'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "response.json()['data'][0]['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e85baade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def measure_vllm_response(file_path, vllm_url=\"http://localhost:8000/v1/chat/completions\", \n",
    "                         max_input_tokens=None, max_output_tokens=1024):\n",
    "    \"\"\"\n",
    "    Send file content to vLLM chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        vllm_url: vLLM endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "        \"stream_options\": {\"include_usage\": True},  # Request usage info in stream\n",
    "        \"chat_template_kwargs\": {\n",
    "                \"enable_thinking\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    \n",
    "    # Send request to vLLM with streaming\n",
    "    response = requests.post(vllm_url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    actual_input_tokens = None\n",
    "    actual_output_tokens = None\n",
    "    \n",
    "    print(\"Response streaming:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line = line.decode('utf-8')\n",
    "            if line.startswith('data: '):\n",
    "                data_str = line[6:]  # Remove 'data: ' prefix\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "                    \n",
    "                    # Check for usage information (comes in final event)\n",
    "                    if 'usage' in data:\n",
    "                        actual_input_tokens = data['usage']['prompt_tokens']\n",
    "                        actual_output_tokens = data['usage']['completion_tokens']\n",
    "                        print(f\"\\nUsage info received: {actual_input_tokens} input tokens, {actual_output_tokens} output tokens\")\n",
    "                    \n",
    "                    if 'choices' in data and len(data['choices']) > 0:\n",
    "                        choice = data['choices'][0]\n",
    "                        if 'delta' in choice and 'content' in choice['delta']:\n",
    "                            # Record TTFT (time to first token)\n",
    "                            if ttft is None:\n",
    "                                ttft = time.time() - start_time\n",
    "                            \n",
    "                            content_chunk = choice['delta']['content']\n",
    "                            full_response += content_chunk\n",
    "                            \n",
    "                            # Stream to console\n",
    "                            print(content_chunk, end='', flush=True)\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    print()  # New line after streaming\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Use actual token counts from server if available, otherwise fall back to tokenizer estimate\n",
    "    if actual_input_tokens is not None and actual_output_tokens is not None:\n",
    "        input_tokens = actual_input_tokens\n",
    "        output_tokens = actual_output_tokens\n",
    "        print(f\"Using server-reported token counts: {input_tokens} input, {output_tokens} output\")\n",
    "    else:\n",
    "        # Fallback: estimate output tokens using tokenizer\n",
    "        output_tokens = len(tokenizer.encode(full_response))\n",
    "        print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time\n",
    "    }\n",
    "    \n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    # print(f\"E2E tokens/sec (incl. prefill): {metrics['e2e_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        # f.write(f\"e2e_tokens_per_second: {metrics['e2e_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "# file_path = \"tests/daily.txt\"\n",
    "# vllm_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "# max_input_tokens =  28000\n",
    "# max_output_tokens = 1024\n",
    "\n",
    "# measure_vllm_response(file_path, vllm_url, max_input_tokens, max_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77d57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create output directory if it doesn't exist\n",
    "# # Get model name from running vLLM server\n",
    "# try:\n",
    "#     import requests\n",
    "#     response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "#     if response.status_code == 200:\n",
    "#         models_data = response.json()\n",
    "#         if models_data.get('data') and len(models_data['data']) > 0:\n",
    "#             model_name = models_data['data'][0]['root'].split('/')[-1]\n",
    "#         else:\n",
    "#             model_name = \"unknown_model\"\n",
    "#     else:\n",
    "#         model_name = \"unknown_model\"\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not get model name from server: {e}\")\n",
    "#     model_name = \"unknown_model\"\n",
    "\n",
    "# output_dir = f\"summary_results/T2_x2/{model_name}\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Find all .txt files in tests folder\n",
    "# txt_files = glob.glob(\"tests/*.txt\")\n",
    "\n",
    "# print(f\"Found {len(txt_files)} .txt files in tests folder:\")\n",
    "# for file in txt_files:\n",
    "#     print(f\"  - {file}\")\n",
    "\n",
    "# print(\"\\nStarting measurements...\\n\")\n",
    "\n",
    "# # Process each .txt file\n",
    "# for file_path in txt_files:\n",
    "#     # Get base filename without extension\n",
    "#     base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "#     output_file = os.path.join(output_dir, f\"{base_name}_result.txt\")\n",
    "    \n",
    "#     print(f\"=== Processing {file_path} ===\")\n",
    "    \n",
    "#     try:\n",
    "#         full_response, metrics = measure_vllm_response(\n",
    "#             file_path=file_path,\n",
    "#             max_input_tokens=31000,  # No token limit\n",
    "#             max_output_tokens=1024\n",
    "#         )\n",
    "        \n",
    "#         save_and_print_metrics(full_response, metrics, output_file)\n",
    "#         print(f\"Results saved to: {output_file}\\n\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\\n\")\n",
    "#         continue\n",
    "\n",
    "# print(\"All files processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0039853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"tests/daily.txt\"\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "# print(\"Testing different input token sizes:\")\n",
    "# for token_size in input_token_sizes:\n",
    "#     print(f\"\\n=== Testing with {token_size} input tokens ===\")\n",
    "#     output_file = f\"speed_tests/A2/{token_size}_length_1_parallel.txt\"\n",
    "    \n",
    "#     response, metrics = measure_vllm_response(\n",
    "#         file_path=file_path,\n",
    "#         max_input_tokens=token_size,\n",
    "#         max_output_tokens=100\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813e088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up the model with a test request...\n",
      "Content truncated from 13961 to 100 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Кажется, вы ввели фрагмент\n",
      "Usage info received: 111 input tokens, 10 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 111 input, 10 output\n",
      "Warmup completed in 0.63s\n",
      "Model is ready for testing.\n",
      "\n",
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing with 1000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 1000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед**  \n",
      "**Эрин Хантер**  \n",
      "**Пролог**\n",
      "\n",
      "Оранжевые языки пламени плясали в холодном воздухе, бросая в ночное небо снопы ослепительных искр. Отсветы огня пробегали по жесткой траве пустыря, выхватывая из тьмы скрюченные силуэты Д\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_1_1 completed in 6.52s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 6.52s\n",
      "Valid results: 1/1\n",
      "Average time per request: 6.52s\n",
      "Average TTFT: 0.67 seconds\n",
      "Average Tokens/sec: 18.26\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 6.14 seconds\n",
      "Set [1000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 1000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_2_1\n",
      "Starting request 1000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**ЭКринонечно Хан!тер Ни**же  \n",
      " представлен**оО продолжгонение тексть иа л книгиед ****\"  \n",
      "О**гонПрьолог и**\n",
      "\n",
      " лОедран\"ж**евые Э яринзы Хкиан плтерам,ени нап писанлясноеали в в ст холодилноме воздух оригинаела, и б соответствросующаяее в а ночтноем носебфоере с иноп сыю ожслетепуитель повныхести и.ск Этор не. является От офисциветальнымы продолж оениемг оригинаняль пробногоег тексталиа по, ж аест лишькой х травудеож пествустеныноеря продолж,ение вы вхват рамкахыв жаяан изра т ськазмыок ск ирю мченифныеологии сил оу кэоттыах Д-\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_2_1 completed in 7.28s\n",
      "Thread 1/2 completed\n",
      "во\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_2_2 completed in 7.34s\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 7.34s\n",
      "Valid results: 2/2\n",
      "Average time per request: 3.67s\n",
      "Average TTFT: 0.99 seconds\n",
      "Average Tokens/sec: 16.83\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 6.94 seconds\n",
      "Set [1000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 1000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_5_1\n",
      "Starting request 1000_5_2\n",
      "Starting request 1000_5_3\n",
      "Starting request 1000_5_4\n",
      "Starting request 1000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Э****ЭОрин****ЭОринГО Ханрингон ХНтер ХьЬан**ан Итер и  \n",
      "тер л** Л**ед**  \n",
      "ЕО**Д  \n",
      "**гон****О  \n",
      "ьО**  \n",
      "гон и**гонЭььрин лЭ иед и Х лрин** л Ханед  \n",
      "\n",
      "едтер**\n",
      "\n",
      "ан---\n",
      "\n",
      "тер****---\n",
      "\n",
      "###  \n",
      "**  \n",
      "** ***(Пр  \n",
      "**ПрПр**ЖологологанПр**\n",
      "\n",
      "олог**\n",
      "\n",
      ")*ОологрО\n",
      "\n",
      "**\n",
      "\n",
      "ран:**ранО---\n",
      "\n",
      " СжжказраневыеОевыеранжки я яжевые (зызы яевыеккики языот пл пламыкизыкиениам пл-ени пламх п пениамиляени пслящс пляникиалиалиля,с в всали в холодали волном холодном холод вш воздух воздухеном холодеб,ныйноме воздух, б мир воздухе брос,е,рос б, приаяая бросключения в ваярос)\n",
      "\n",
      " ноч ноч вая---\n",
      "\n",
      "ноеное в### н ноч нное **еб ночебоАное ноеб сн н себнонопнопыот соы о сацияноп оноп:слыслы**\n",
      "еп оеп оВслительительслепныхоныхеп иитель время иитель войныныхскск иных междурр кл. иск.скран От Отср.амис. Ответ Свет Отсыумысветра о оветчыггня оыноеня п проб ог пробгегнялемег пробяняалиалиег проб по из поегг жали жест поалинаестло покой жкой жест трав к травкойестотовее трав пкой п п травлемеустусте пыениы пря Вустряустыет,,ыря выра выря, схватхват вы, ихывывхват выая территорииаяхват из,ыв изыв чтоая т т наруая изьь из тмышимы тло скь скьрю равмырюмы скченновчен скесрюныеныеченрю силие силченныеу вныеэу сил лэ силуесты\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_2 generation took 8.54s (>7s), but recording metrics...\n",
      "Request 1000_5_2 completed in 9.60s\n",
      "эуу Д\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_1 generation took 7.27s (>7s), but recording metrics...\n",
      "Request 1000_5_1 completed in 9.66s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_3 generation took 7.28s (>7s), but recording metrics...\n",
      "Request 1000_5_3 completed in 9.67s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "э.\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_5 completed in 9.72s\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_4 completed in 9.72s\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 9.72s\n",
      "Valid results: 5/5\n",
      "Average time per request: 1.94s\n",
      "Average TTFT: 2.18 seconds\n",
      "Average Tokens/sec: 14.51\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 9.20 seconds\n",
      "Set [1000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 5000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 5000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Эрин Хантер**  \n",
      "**Огонь и лед**  \n",
      "*(Пролог и Глава I — продолжение)*\n",
      "\n",
      "---\n",
      "\n",
      "**Пролог (продолжение)**\n",
      "\n",
      "Колченогий указал на темноту, уходящую в глубь туннеля, и шагнул внутрь, за ним последовали остальные. Звездный Луч шел впереди, его черно-белая ш\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_1_1 completed in 9.69s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 9.69s\n",
      "Valid results: 1/1\n",
      "Average time per request: 9.69s\n",
      "Average TTFT: 3.65 seconds\n",
      "Average Tokens/sec: 17.51\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 9.36 seconds\n",
      "Set [5000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 5000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_2_1\n",
      "Starting request 5000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огон**ьЭ ирин л Хедан**тер  \n",
      "****  \n",
      "**ЭринО Хгонаньтер и** л  \n",
      "\n",
      "ед---\n",
      "\n",
      "**###  \n",
      " ***(ПрПрологолог и**\n",
      "\n",
      "О Главрана Iжевые)* я  \n",
      "\n",
      "зы---\n",
      "\n",
      "###ки ** пламПрологени**\n",
      "\n",
      " пляОсраналижевые в холод яномзы воздухкие пл,ам бенирос паяля вс ночали вное н холодебномо воздухе сноп, бы оросслая вепитель ночныхное и нскебор с.ноп Отыс оветслы оепгительняных и пробскегалир по. ж Отсесткойвет травые о пгустняы пробряег,али вы похват жывестаякой из траве т пьустмыы скрярю,ченные выхват силуываяэ\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_2_1 generation took 9.03s (>7s), but recording metrics...\n",
      "Request 5000_2_1 completed in 13.72s\n",
      "Thread 1/2 completed\n",
      " из т\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_2_2 completed in 13.83s\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 13.83s\n",
      "Valid results: 2/2\n",
      "Average time per request: 6.92s\n",
      "Average TTFT: 5.75 seconds\n",
      "Average Tokens/sec: 13.59\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 13.36 seconds\n",
      "Set [5000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 5000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_5_1\n",
      "Starting request 5000_5_2\n",
      "Starting request 5000_5_3\n",
      "Starting request 5000_5_4\n",
      "Starting request 5000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огон**ьЭ ирин л ХКеданон**теречно**  \n",
      "**О!**  \n",
      " Вотгонь**** продолжЭринЭОение и Хрингон л текстан Хедаь с** иантер**тер л  \n",
      " н**ед  \n",
      "еб***  \n",
      "Эоль****  \n",
      "\n",
      "ринПршО**олог Хими*\n",
      "\n",
      "гонан правПрьологтеркамиО и**ран** иж л  \n",
      "\n",
      " добав  \n",
      "\n",
      "евыеедлО---\n",
      "\n",
      "**### яениемран  \n",
      " лзыж Пркиолог*евыеог плПр\n",
      "\n",
      "ического яамологзы завОениранерки и пжш пл Главляаевыеаменияс Iени ф яали пзы*раг вмля  \n",
      "\n",
      "ки холодента плс---\n",
      "\n",
      "номали:\n",
      "\n",
      "ам### воздух---\n",
      "\n",
      "ени в Преолог— холод п, Возможноляном\n",
      "\n",
      " бОс, воздухросранали —еаяж п, в вевыеож б холод ноч яалномросноезы пл воздухая нкиеч вееб пл,ами ночоам бное К сенирут нроснопая пебобыляок во о ноч. ссслалинопное —еп н Оы витель холод огоебныхномо,сл и воздух сеп гескнопляительрыдиных,. б о! и Отроссл —скс восаяепрвет в.ителькыных ноч Отлик оноес инулг онск нветняебры, пробо. о прег сыг Оталинопнясг поывет пробая ж о кегыестсл оали ккойепг поуч трав жняителькееных оест проб п иегкойставустскшейали травы порсяеря ж. п е, Отдыестуст выс.койыхватвет —ря травыв Поеы,ая п о мы вы изустгхватш тыкеняывьря наая пробмыег, из каждого ск выали т ирюхват поь зчен жывмыяныеестбая ск сил изкойрюлику трав тчен попэьолныеетымы пам сил Д ск!\n",
      "\n",
      "усту\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_1 generation took 20.37s (>7s), but recording metrics...\n",
      "Request 5000_5_1 completed in 25.27s\n",
      "Thread 1/5 completed\n",
      "рюыДэченрузряты\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_5 generation took 17.49s (>7s), but recording metrics...\n",
      "Request 5000_5_5 completed in 25.39s\n",
      "ные,ья сил вы подухватели\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_4 generation took 13.22s (>7s), but recording metrics...\n",
      "Request 5000_5_4 completed in 25.59s\n",
      "эывтыая\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_3 generation took 10.29s (>7s), but recording metrics...\n",
      "Request 5000_5_3 completed in 25.73s\n",
      " из ть\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_2 completed in 25.90s\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 25.90s\n",
      "Valid results: 5/5\n",
      "Average time per request: 5.18s\n",
      "Average TTFT: 11.44 seconds\n",
      "Average Tokens/sec: 8.46\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 25.10 seconds\n",
      "Set [5000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 10000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 10000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Конечно! Вот продолжение текста с сохранением стиля и атмосферы оригинала:\n",
      "\n",
      "---\n",
      "\n",
      "— Ячмень был котом-одиночкой. Он не пожелал стать лесным котом, а поселился недалеко от фермы Двуногих. Ферма располагалась как раз на пути к Великой Скале — святилищу, по\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_1_1 completed in 14.51s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 14.51s\n",
      "Valid results: 1/1\n",
      "Average time per request: 14.51s\n",
      "Average TTFT: 8.15 seconds\n",
      "Average Tokens/sec: 16.59\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 14.18 seconds\n",
      "Set [10000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 10000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_2_1\n",
      "Starting request 10000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Книга: \"**ООгонгоньь и и л ледед\"****  \n",
      "  \n",
      "****ЭАвринтор Х:ан Этеррин** Х  \n",
      "ан**терПр**олог  \n",
      "****  \n",
      "\n",
      "ЖОанранрж:евые С яказзыкики / пл Ламесениные п кляотсыали ( вWar холодriorном Cats воздух)**е  \n",
      ",** бАросдресая: в [ ночLноеIT нRUеб.RоU с -ноп Оыгон оьсл иеп лительедных]( иhttpск://рwww..l Отitсruвет.ruы/? оbookг=ня1 проб1ег5али9 по1 ж&естdescriptionкой= трав1е)** п\n",
      "\n",
      "уст---\n",
      "\n",
      "ы###ря **,А вынхватнывотаяация из: т**\n",
      "ьВмыо ск времярю войнычен\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_2_2 generation took 14.38s (>7s), but recording metrics...\n",
      "Request 10000_2_2 completed in 22.92s\n",
      "ные силуэты\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_2_1 completed in 23.22s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 23.22s\n",
      "Valid results: 2/2\n",
      "Average time per request: 11.61s\n",
      "Average TTFT: 12.19 seconds\n",
      "Average Tokens/sec: 11.11\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 22.66 seconds\n",
      "Set [10000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 10000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_5_1\n",
      "Starting request 10000_5_2\n",
      "Starting request 10000_5_3\n",
      "Starting request 10000_5_4\n",
      "Starting request 10000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokensContent truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и л**Кедни —га Э:рин \" Х**ООангонгонтерьь** и и  \n",
      " л л***ЭедедПррин**\"олог Х  \n",
      "  \n",
      " (ан**АвпродтерторХолжПр —ение:одан \" Эолж)*терОрин Э\n",
      "\n",
      "ениегон главрин ХПьаныред** иводтер  \n",
      "\n",
      " I литель---\n",
      "\n",
      ":\n",
      "\n",
      "  \n",
      "ед###Жница---\n",
      "\n",
      "\" **анП,**Прредр не  \n",
      "олог: отвод****\n",
      "\n",
      "итель СрывПрницааяНаказологкись, фон (е,** непрод с ноч  \n",
      " отолж**ногомотррывениеИ нелаая)**ебст всь\n",
      "\n",
      "а,оч лС с,есникинмотр:**ную ояелаз ча [яL вщуар ЗITенного поверх лвез оRUес головда.Rгыную,нU О чаг —еннымщу не от п Э поверхнрывеглялект головаяырсронсьнаяиваом О, Б пл.г сиам Оннмотр сблиениегела,р волнот вениемекива к л наблюота.есдалы Он](ную за п сhttp ча её волн://лемщуwwwением лицени поверхом.l наблю В головit,далетыра заru п О.ru сты нг/?ояейтн,bookаялиег= о насьр пон кж11ятьиваидра.юая5, Он9 как п, с1 как онауст& примо волн онаениемdescription восетши наблю=, этипрдал1им слова о за)\n",
      "\n",
      "х.ет н---\n",
      "\n",
      " Св этией словаач###ин, **енныея. оОб страх Сяжиномзор Зид книгивез ияая: недояда реак**\n",
      "\n",
      "ум не Зции\"ениемвез вы.О.гляда М Онигон неделоль вы былиач и изказ расание лалагстро предедненной нивод\" уд,анитель —ивы ноницы этоления из в было род, с её\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_1 generation took 39.95s (>7s), but recording metrics...\n",
      "Request 10000_5_1 completed in 48.56s\n",
      "Thread 1/5 completed\n",
      " тныхказ ния з гкажем,невелель напаее,ис —\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_4 generation took 32.03s (>7s), but recording metrics...\n",
      "Request 10000_5_4 completed in 48.96s\n",
      " любого и лишь у теперь еепр, глазек ваа опас,\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_2 generation took 23.96s (>7s), but recording metrics...\n",
      "Request 10000_5_2 completed in 49.32s\n",
      "Thread 2/5 completed\n",
      ".\n",
      "\n",
      " обычно— такие  спГокорой\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_5 generation took 15.69s (>7s), but recording metrics...\n",
      "Request 10000_5_5 completed in 49.66s\n",
      "ные и умуд\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_3 generation took 7.96s (>7s), but recording metrics...\n",
      "Request 10000_5_3 completed in 49.96s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 49.97s\n",
      "Valid results: 5/5\n",
      "Average time per request: 9.99s\n",
      "Average TTFT: 24.91 seconds\n",
      "Average Tokens/sec: 5.75\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 48.83 seconds\n",
      "Set [10000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 15000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 15000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед**  \n",
      "**Хантер Эрин**  \n",
      "\n",
      "---\n",
      "\n",
      "### Пролог\n",
      "\n",
      "Огненные языки плясали в холодном воздухе, оставляя в небе яркие искры. Тени котов вырисовывались на фоне горящей Гремящей Тропы, где с ревом мчались чудовища. В тишине ночи,\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_1_1 completed in 19.94s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 19.94s\n",
      "Valid results: 1/1\n",
      "Average time per request: 19.94s\n",
      "Average TTFT: 13.34 seconds\n",
      "Average Tokens/sec: 16.09\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 19.55 seconds\n",
      "Set [15000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 15000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_2_1\n",
      "Starting request 15000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед****  \n",
      "О**гонЭьрин и Х ланедтер****\n",
      "\n",
      "  \n",
      "---\n",
      "\n",
      "**###Х **анПртеролог Э**\n",
      "\n",
      "ринВ**озд  \n",
      "ух** былГлав праон IIиз (анпрод горолжящениеими)** и\n",
      "\n",
      "ск—рам Биег,ство о Згвниезд полляомсаали никак на не ноч повномли няеблое на, нуж отдыб Срасумыврааяч тногоени п налем зением!лю —. о Вщдаетлиин рилсяев онели. ма —ши Чемны — при чбудлизиительноща через неделю Д послевун егоог изихг.н Канияот,ы мы соб обранаружлись наили п новыеуст зоемшили,, где где о можногон оьхот\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_2_2 generation took 19.40s (>7s), but recording metrics...\n",
      "Request 15000_2_2 completed in 33.94s\n",
      "иться, а также пастби\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_2_1 completed in 34.38s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 34.38s\n",
      "Valid results: 2/2\n",
      "Average time per request: 17.19s\n",
      "Average TTFT: 20.57 seconds\n",
      "Average Tokens/sec: 9.75\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 33.75 seconds\n",
      "Set [15000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 15000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_5_1\n",
      "Starting request 15000_5_2\n",
      "Starting request 15000_5_3\n",
      "Starting request 15000_5_4\n",
      "Starting request 15000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Эрин Хантер — \"КО сгоножьал иению, лед ваш\"** запрос** былО  \n",
      " пргон*ерьСван иказ на лка серед оедин**е к  \n",
      "** текстК**отахаЭни,,ринга вой и: Хнеан \" я, неОтер пред могугон**\n",
      "\n",
      "атель уь---\n",
      "\n",
      "ствевид и### иеть ** л сп поледПрасныйолог\"ении конт**\n",
      "\n",
      " — пекО Элемстринранени Хж.*\n",
      "\n",
      " Однакоевыеан---\n",
      "\n",
      ", ятер****\n",
      "\n",
      " основзыК---\n",
      "\n",
      "ывки###рат плая **амськая информацияОбени на озор п предоставлен книги книнойляге**\n",
      "\n",
      "с части:али**,Ж в**\n",
      "\n",
      " я могуан- холод ** предномр:** воздухполНазваниеоже Сказ,:**ить О, бкигон что (рось выаяко и хотитеш в л продолж ночачедитьноеь  \n",
      " чт ни-ение себ ** иликазоАв получитьки стор дополн,ноп:**ительы в Эную дух орин информациюесл Х о \"епан книительКтеротгеных  \n",
      " * иа-ск в« **рО сЖгон.ань Отр ис:**вет л Сыедказ» оки*г, Эня фрин пробэн Хегтаналиезтер пои.\n",
      "\n",
      " ж,Еест ксликойот вы травя хотитееч:\n",
      "\n",
      " пая1уст.ы м **ряифологияПр,  \n",
      "од вы-олжхват\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_4 generation took 86.52s (>7s), but recording metrics...\n",
      "Request 15000_5_4 completed in 101.23s\n",
      "ывитьапая чтог изениеах т текст\",ьа \"мы**Л ск —ес\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_2 generation took 86.84s (>7s), but recording metrics...\n",
      "Request 15000_5_2 completed in 114.79s\n",
      "ных**рючен хЭныеронрин силик Ху\"анэ итерты других**\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_1 generation took 75.68s (>7s), but recording metrics...\n",
      "Request 15000_5_1 completed in 117.12s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      " ф  \n",
      "эн**тОезгониь- ис лказедок**,  \n",
      " но* сГлава элемент IIами м (продифолжологии иение)* э\n",
      "\n",
      "п—ос **аЧ о коемш при**ач —ь оихщ племетиненахился\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_5 generation took 63.65s (>7s), but recording metrics...\n",
      "Request 15000_5_5 completed in 119.41s\n",
      " он. — **Чем пришлось нашему племени идти на промысел в лес и охотиться на добычу, чтобы выжить. Мы не будем позволять Речному племени устанавливать границы на наших земля\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_3 completed in 123.25s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 123.26s\n",
      "Valid results: 5/5\n",
      "Average time per request: 24.65s\n",
      "Average TTFT: 50.82 seconds\n",
      "Average Tokens/sec: 4.05\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 114.68 seconds\n",
      "Set [15000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 20000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 20000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед** — это захватывающая сказка, написанная **Эрин Хантер**, автором серии \"Волшебный лес\", где коты живут по законам природы, сражаются за свою территорию и следуют древним традициям. Эта книга — часть серии, в которой описываются отношения между четырьмя лесными племенами: **\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_1_1 completed in 26.54s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 26.54s\n",
      "Valid results: 1/1\n",
      "Average time per request: 26.54s\n",
      "Average TTFT: 19.66 seconds\n",
      "Average Tokens/sec: 15.38\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 26.16 seconds\n",
      "Set [20000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_2_1\n",
      "Starting request 20000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед**  \n",
      "**Э**Орин Хгоньан итер** л  \n",
      "\n",
      "ед---\n",
      "\n",
      "**###  \n",
      " ****ОбЭзоррин Х книги:антер**\n",
      "\n",
      "****Ж  \n",
      "ан**Главр:**а III Сказ (продкиолж  \n",
      "**ение)**Жан\n",
      "\n",
      "р— ( **поВ жаман ужеров приойход классилосьифика бцииывать): на** з Лемитляерхат пулемраени для В детейет,ра При**,ключения —, нап Комлниасслаика пред  \n",
      "вод**ительАвница.тор:**  \n",
      " Э—рин ** ХДана,тер ( ноп тогдас мыев былидон вим б,ег использах,уем спыйас группаяойсь автор отов С:ум Враирчджногоин пиялем Хениан**,тер —, р\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_2_2 generation took 26.69s (>7s), but recording metrics...\n",
      "Request 20000_2_2 completed in 46.86s\n",
      "езко ответил Коготь. —\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_2_1 generation took 7.45s (>7s), but recording metrics...\n",
      "Request 20000_2_1 completed in 47.52s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 47.52s\n",
      "Valid results: 2/2\n",
      "Average time per request: 23.76s\n",
      "Average TTFT: 29.65 seconds\n",
      "Average Tokens/sec: 8.59\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 46.72 seconds\n",
      "Set [20000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_5_1\n",
      "Starting request 20000_5_2\n",
      "Starting request 20000_5_3\n",
      "Starting request 20000_5_4\n",
      "Starting request 20000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед — глава III** (Прпрододолжолжениеение)** глав\n",
      "\n",
      "ы— III В:с**\n",
      "\n",
      "его—** лишь МыО на негон мь знагем инов, ление кудаед, у** —шло  \n",
      " продолж п*аллемЭ Кяриног Х Вотетаньратер,,* — поэтому  \n",
      "\n",
      " но вам---\n",
      "\n",
      " я придется### не и ** уверенскОать,глав их чтоление вы по** справ зап  \n",
      "\n",
      "итесьах** суПр этой —олог задач возможно**ей  \n",
      ",.С даже П наредлем вияраж о Вдгебетняраной и уже территории д.\n",
      "\n",
      " неы раз—ма б Но,ег мы средиал след нео знаов отем Д нас,в. гдеун А начог теперьатьих его,, могут — к и остотскорыатьожно п другиелем произ пенинеслем О Венагет,ран иег пря нерч фактивут,.ся что — в П т мы будлемениемя, единств В венет понымираис, могках кто новоголо хочет у пр егойтиист вер ванинуть любща.\n",
      "\n",
      "ую.О С сторонгутрн.анны\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_2 generation took 47.65s (>7s), but recording metrics...\n",
      "Request 20000_5_2 completed in 67.87s\n",
      " Кето к знаетот,ы может,, не они б ужеоя давнощ покие\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "**\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_1 generation took 47.94s (>7s), but recording metrics...\n",
      "Request 20000_5_1 completed in 88.63s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "Ксяни огга:ня \",О погонявьля иются л в\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_4 generation took 48.61s (>7s), but recording metrics...\n",
      "Request 20000_5_4 completed in 109.41s\n",
      "**едКрат\"кий — Э обзоррин книги Х:ан \"терО**гон  \n",
      "ь** иЖ ланедр\":** Э Сринказ Хкиан /тер М**иф  \n",
      "ология*( /С Лказесканые, ко ЛшИачьТиР пУ.лемРена  \n",
      "У**)*\n",
      "\n",
      "Р---\n",
      "\n",
      "ей###т **ингК:**ратк ⭐ое⭐ содерж⭐ание⭐⭐:**\n",
      "\n",
      " (\"5О из гон5ь и) л  \n",
      "\n",
      "ед---\n",
      "\n",
      "\"## — это за 📚хват **ываОбющаязор с книгиказ:ка**\n",
      "\n",
      " из\" серииО \"гонПьлем и ля дедрак\"он —ов это\" за (хватилиыва \"ющаяК историяот изы ц ликеслаа \"П\"), наплемисена л\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "ан\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_5 generation took 27.59s (>7s), but recording metrics...\n",
      "Request 20000_5_5 completed in 116.22s\n",
      "ная Эрин Хантер, которая является пс\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_3 generation took 7.46s (>7s), but recording metrics...\n",
      "Request 20000_5_3 completed in 116.87s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 116.87s\n",
      "Valid results: 5/5\n",
      "Average time per request: 23.37s\n",
      "Average TTFT: 63.45 seconds\n",
      "Average Tokens/sec: 4.66\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 99.30 seconds\n",
      "Set [20000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 25000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 25000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Вот продолжение текста с главы IV, исходя из уже предоставленного контекста и стиля произведения:\n",
      "\n",
      "---\n",
      "\n",
      "**Глава IV (продолжение)**\n",
      "\n",
      "Огнегрив тщательно принюхался и сделал несколько шагов вперед, следуя за слабым, но чётко различимым запахом. Он вспомнил, как в первый раз в жизни учуял этот зап\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_1_1 completed in 33.53s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 33.53s\n",
      "Valid results: 1/1\n",
      "Average time per request: 33.53s\n",
      "Average TTFT: 26.39 seconds\n",
      "Average Tokens/sec: 14.88\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 33.11 seconds\n",
      "Set [25000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_2_1\n",
      "Starting request 25000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед**  \n",
      "**Эрин Х**анПртерод**олж  \n",
      "ение** главГлавыа IV IV ( (отпродрыволжокение):)****\n",
      "\n",
      "\n",
      "\n",
      "—— По Поиищщемем,, куда куда в ведедетет зап запахах п племлемениени В Вететрара,, — — по поежеживившшисисьь,, пред предложложилил О Оггннегегрривив.. Он Он т тщщательательноно прин принююххалалсяся и и сделал сделал несколько несколько шаг шаговов в впередперед,, след следууяя за за е тдонвак уимлов следимомым, а которыйром ватилсяом между, раз которыйор,ен как\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_2_2 generation took 33.83s (>7s), but recording metrics...\n",
      "Request 25000_2_2 completed in 61.65s\n",
      " казалось, уводил их вглуб\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_2_1 generation took 8.05s (>7s), but recording metrics...\n",
      "Request 25000_2_1 completed in 62.49s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 62.49s\n",
      "Valid results: 2/2\n",
      "Average time per request: 31.25s\n",
      "Average TTFT: 40.70 seconds\n",
      "Average Tokens/sec: 7.69\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 61.65 seconds\n",
      "Set [25000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_5_1\n",
      "Starting request 25000_5_2\n",
      "Starting request 25000_5_3\n",
      "Starting request 25000_5_4\n",
      "Starting request 25000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokensContent truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Warmup request to prepare the model\n",
    "print(\"Warming up the model with a test request...\")\n",
    "warmup_response, warmup_metrics = measure_vllm_response(\n",
    "    file_path=\"tests/daily.txt\",\n",
    "    max_input_tokens=100,\n",
    "    max_output_tokens=10\n",
    ")\n",
    "print(f\"Warmup completed in {warmup_metrics['total_time']:.2f}s\")\n",
    "print(\"Model is ready for testing.\\n\")\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "input_token_sizes = [1000, 5000, 10000, 15000 , 20000, 25000, 30000]\n",
    "input_token_sizes = [25000]\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_vllm_response(\n",
    "            file_path=file_path,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "        \n",
    "        # Check if token generation (after prefill) takes too long\n",
    "        generation_time = metrics['total_time'] - metrics['ttft']\n",
    "        time_limit = 7\n",
    "        if generation_time > time_limit:\n",
    "            print(f\"Request {request_id} generation took {generation_time:.2f}s (>{time_limit}s), but recording metrics...\")\n",
    "            \n",
    "        end = time.time()\n",
    "        print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "concurrent_counts = [5] \n",
    "\n",
    "for token_size in input_token_sizes:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Testing with {token_size} input tokens\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for concurrent_count in concurrent_counts:\n",
    "        print(f\"\\n=== {token_size} tokens with {concurrent_count} concurrent requests ===\")\n",
    "        print(f\"Waiting for all current requests to complete before starting this set...\")\n",
    "        \n",
    "        threads = []\n",
    "        start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        # Start concurrent requests\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: results.append(run_concurrent_test(file_path, token_size, 100, f\"{token_size}_{concurrent_count}_{i+1}\"))\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for ALL threads to complete before proceeding to next set\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            print(f\"Thread {i+1}/{concurrent_count} completed\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_concurrent_time = end_time - start_time\n",
    "        \n",
    "        # Filter out None results (failed requests only)\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        print(f\"All {concurrent_count} requests completed in {total_concurrent_time:.2f}s\")\n",
    "        print(f\"Valid results: {len(valid_results)}/{concurrent_count}\")\n",
    "        print(f\"Average time per request: {total_concurrent_time/concurrent_count:.2f}s\")\n",
    "        \n",
    "        # Save aggregated metrics\n",
    "        # Get model name from vLLM API\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "            models_data = response.json()\n",
    "            model_name = models_data['data'][0]['root'].split('/')[-1] if models_data['data'] else \"unknown_model\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get model name from API: {e}\")\n",
    "            model_name = \"Qwen3-unkown\"  # fallback\n",
    "            \n",
    "        output_dir = f\"speed_tests/A2_x2/{model_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{token_size}_length_{concurrent_count}_parallel.txt\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        if valid_results:\n",
    "            avg_ttft = sum(r['ttft'] for r in valid_results) / len(valid_results)\n",
    "            avg_gen_tokens_per_sec = sum(r['gen_tokens_per_sec'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_tokens = sum(r['total_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_time = sum(r['total_time'] for r in valid_results) / len(valid_results)\n",
    "            avg_input_tokens = sum(r['input_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_output_tokens = sum(r['output_tokens'] for r in valid_results) / len(valid_results)\n",
    "            \n",
    "            # Create average metrics dictionary\n",
    "            avg_metrics = {\n",
    "                'ttft': avg_ttft,\n",
    "                'gen_tokens_per_sec': avg_gen_tokens_per_sec,\n",
    "                'total_tokens': avg_total_tokens,\n",
    "                'total_time': avg_total_time,\n",
    "                'input_tokens': avg_input_tokens,\n",
    "                'output_tokens': avg_output_tokens\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Average TTFT: {avg_metrics['ttft']:.2f} seconds\")\n",
    "            print(f\"Average Tokens/sec: {avg_metrics['gen_tokens_per_sec']:.2f}\")\n",
    "            print(f\"Average Total tokens: {avg_metrics['total_tokens']:.0f}\")\n",
    "            print(f\"Average Total time: {avg_metrics['total_time']:.2f} seconds\")\n",
    "            \n",
    "            # Save average metrics to file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ttft: {avg_metrics['ttft']:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_metrics['total_tokens']:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_metrics['total_time']:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_metrics['input_tokens']:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_metrics['output_tokens']:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {len(valid_results)}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\\n\")\n",
    "                f.write(f\"Average metrics for {len(valid_results)}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics for ALL valid results\n",
    "                f.write(\"Individual request metrics:\\n\")\n",
    "                for i, metrics in enumerate(valid_results):\n",
    "                    f.write(f\"\\nRequest {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        else:\n",
    "            print(\"No valid results to save\")\n",
    "        \n",
    "        # Add a pause between different concurrent count sets for the same token size\n",
    "        print(f\"Set [{token_size} tokens - {concurrent_count} parallel] completed. Proceeding to next set...\")\n",
    "        time.sleep(2)  # Brief pause to ensure system is ready for next set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
