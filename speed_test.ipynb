{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19185de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'405 method not allowed'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:11434/api/chat\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95c6f976",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'with' statement on line 4 (844015140.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mresponse = requests.post(\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'with' statement on line 4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "with open(\"tests/book.txt\", \"r\") as f:\n",
    "response = requests.post(\n",
    "    \"http://localhost:11434/api/chat\",\n",
    "    json={\n",
    "        \"model\": \"qwen3:14b\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "        ],\n",
    "        \"stream\": True,\n",
    "        \"think\": False\n",
    "    },\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for line in response.iter_lines():\n",
    "    if line:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            if 'message' in data and 'content' in data['message']:\n",
    "                print(data['message']['content'], end='', flush=True)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2840f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content truncated from 181863 to 100 tokens\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77e7e291b5c0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:493\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/connection.py:494\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28mself\u001b[39m.putheader(header, value)\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1034\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/connection.py:325\u001b[39m, in \u001b[36mHTTPConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tunnel_host:\n\u001b[32m    327\u001b[39m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/connection.py:213\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    214\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    217\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n",
      "\u001b[31mNewConnectionError\u001b[39m: <urllib3.connection.HTTPConnection object at 0x77e7e291b5c0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77e7e291b5c0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmeasure_vllm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtests/book.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://localhost:8000/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_input_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mmeasure_vllm_response\u001b[39m\u001b[34m(file_path, vllm_url, max_input_tokens, max_output_tokens, time_limit)\u001b[39m\n\u001b[32m     58\u001b[39m generation_timeout = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Send request to vLLM with streaming\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed with status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/adapters.py:677\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x77e7e291b5c0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "measure_vllm_response('tests/book.txt', vllm_url=\"http://localhost:11434/api/chat\", max_input_tokens=100, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85baade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import signal\n",
    "import threading\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def measure_vllm_response(file_path, vllm_url=\"http://localhost:8000/v1/chat/completions\", \n",
    "                         max_input_tokens=None, max_output_tokens=1024, time_limit=7):\n",
    "    \"\"\"\n",
    "    Send file content to vLLM chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        vllm_url: vLLM endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "        time_limit: Maximum time in seconds for generation (after prefill), None for no limit\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "        \"stream_options\": {\"include_usage\": True},  # Request usage info in stream\n",
    "        \"chat_template_kwargs\": {\n",
    "                \"enable_thinking\": False\n",
    "        },\n",
    "        \"think\": False\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    generation_timeout = False\n",
    "    \n",
    "    # Send request to vLLM with streaming\n",
    "    response = requests.post(vllm_url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    actual_input_tokens = None\n",
    "    actual_output_tokens = None\n",
    "    \n",
    "    print(\"Response streaming:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line = line.decode('utf-8')\n",
    "            if line.startswith('data: '):\n",
    "                data_str = line[6:]  # Remove 'data: ' prefix\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "                    \n",
    "                    # Check for usage information (comes in final event)\n",
    "                    if 'usage' in data:\n",
    "                        actual_input_tokens = data['usage']['prompt_tokens']\n",
    "                        actual_output_tokens = data['usage']['completion_tokens']\n",
    "                        print(f\"\\nUsage info received: {actual_input_tokens} input tokens, {actual_output_tokens} output tokens\")\n",
    "                    \n",
    "                    if 'choices' in data and len(data['choices']) > 0:\n",
    "                        choice = data['choices'][0]\n",
    "                        if 'delta' in choice and 'content' in choice['delta']:\n",
    "                            # Record TTFT (time to first token)\n",
    "                            if ttft is None:\n",
    "                                ttft = time.time() - start_time\n",
    "                                print(f\"\\nFirst token received at {ttft:.2f}s\")\n",
    "                            \n",
    "                            # Check time limit after first token (generation phase)\n",
    "                            if time_limit is not None and ttft is not None:\n",
    "                                generation_time = time.time() - start_time - ttft\n",
    "                                if generation_time > time_limit:\n",
    "                                    print(f\"\\nGeneration timeout reached ({time_limit}s limit)\")\n",
    "                                    generation_timeout = True\n",
    "                                    break\n",
    "                            \n",
    "                            content_chunk = choice['delta']['content']\n",
    "                            full_response += content_chunk\n",
    "                            \n",
    "                            # Stream to console\n",
    "                            print(content_chunk, end='', flush=True)\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    print()  # New line after streaming\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Use actual token counts from server if available, otherwise fall back to tokenizer estimate\n",
    "    if actual_input_tokens is not None and actual_output_tokens is not None:\n",
    "        input_tokens = actual_input_tokens\n",
    "        output_tokens = actual_output_tokens\n",
    "        print(f\"Using server-reported token counts: {input_tokens} input, {output_tokens} output\")\n",
    "    else:\n",
    "        # Fallback: estimate output tokens using tokenizer\n",
    "        output_tokens = len(tokenizer.encode(full_response))\n",
    "        print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time,\n",
    "        'generation_timeout': generation_timeout\n",
    "    }\n",
    "    \n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    # print(f\"E2E tokens/sec (incl. prefill): {metrics['e2e_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    if metrics.get('generation_timeout', False):\n",
    "        print(\"Generation was interrupted due to timeout\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        # f.write(f\"e2e_tokens_per_second: {metrics['e2e_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\")\n",
    "        if metrics.get('generation_timeout', False):\n",
    "            f.write(\"generation_timeout: true\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "# file_path = \"tests/daily.txt\"\n",
    "# vllm_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "# max_input_tokens =  28000\n",
    "# max_output_tokens = 1024\n",
    "\n",
    "# measure_vllm_response(file_path, vllm_url, max_input_tokens, max_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77d57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create output directory if it doesn't exist\n",
    "# # Get model name from running vLLM server\n",
    "# try:\n",
    "#     import requests\n",
    "#     response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "#     if response.status_code == 200:\n",
    "#         models_data = response.json()\n",
    "#         if models_data.get('data') and len(models_data['data']) > 0:\n",
    "#             model_name = models_data['data'][0]['root'].split('/')[-1]\n",
    "#         else:\n",
    "#             model_name = \"unknown_model\"\n",
    "#     else:\n",
    "#         model_name = \"unknown_model\"\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not get model name from server: {e}\")\n",
    "#     model_name = \"unknown_model\"\n",
    "\n",
    "# output_dir = f\"summary_results/T2_x2/{model_name}\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Find all .txt files in tests folder\n",
    "# txt_files = glob.glob(\"tests/*.txt\")\n",
    "\n",
    "# print(f\"Found {len(txt_files)} .txt files in tests folder:\")\n",
    "# for file in txt_files:\n",
    "#     print(f\"  - {file}\")\n",
    "\n",
    "# print(\"\\nStarting measurements...\\n\")\n",
    "\n",
    "# # Process each .txt file\n",
    "# for file_path in txt_files:\n",
    "#     # Get base filename without extension\n",
    "#     base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "#     output_file = os.path.join(output_dir, f\"{base_name}_result.txt\")\n",
    "    \n",
    "#     print(f\"=== Processing {file_path} ===\")\n",
    "    \n",
    "#     try:\n",
    "#         full_response, metrics = measure_vllm_response(\n",
    "#             file_path=file_path,\n",
    "#             max_input_tokens=31000,  # No token limit\n",
    "#             max_output_tokens=1024\n",
    "#         )\n",
    "        \n",
    "#         save_and_print_metrics(full_response, metrics, output_file)\n",
    "#         print(f\"Results saved to: {output_file}\\n\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\\n\")\n",
    "#         continue\n",
    "\n",
    "# print(\"All files processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0039853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"tests/daily.txt\"\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "# print(\"Testing different input token sizes:\")\n",
    "# for token_size in input_token_sizes:\n",
    "#     print(f\"\\n=== Testing with {token_size} input tokens ===\")\n",
    "#     output_file = f\"speed_tests/A2/{token_size}_length_1_parallel.txt\"\n",
    "    \n",
    "#     response, metrics = measure_vllm_response(\n",
    "#         file_path=file_path,\n",
    "#         max_input_tokens=token_size,\n",
    "#         max_output_tokens=100\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813e088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a15c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up the model with a test request...\n",
      "Content truncated from 13961 to 100 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 2.87s\n",
      "Кажется, вы пересылает\n",
      "Usage info received: 111 input tokens, 10 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 111 input, 10 output\n",
      "Warmup completed in 3.22s\n",
      "Model is ready for testing.\n",
      "\n",
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing with 20000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 20000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_1_1\n",
      "Waiting for all 1 threads to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (181863 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 30.22s\n",
      "**Огонь и лед**  \n",
      "**Эрин Хантер**  \n",
      "**Глава IV**\n",
      "\n",
      "— **Вам уже приходилось бывать на землях племени Ветра,** — напомнила предводительница.  \n",
      "— **Да,** — кивнул Коготь, не сводя глаз с Огнегрива. — **Там, где племя Ветра оставалось, ж\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_1_1 completed in 35.71s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 35.71s\n",
      "Valid results: 1/1\n",
      "Average time per request: 35.71s\n",
      "Average TTFT: 30.22 seconds\n",
      "Average Tokens/sec: 19.97\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 35.23 seconds\n",
      "Set [20000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_2_1\n",
      "Starting request 20000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 30.68s\n",
      "**Огон\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 20000 input, 3 output\n",
      "Request 20000_2_2 completed in 38.62s\n",
      "\n",
      "First token received at 60.62s\n",
      "Вот продолжение рассказа \"Огонь и лед\" Эрин Хантер, с сохранением стиля и продолжением сюжета:\n",
      "\n",
      "---\n",
      "\n",
      "### Глава IV\n",
      "\n",
      "— **Вам уже приходилось бывать на землях племени Ветра**, — напомнила предводительница.  \n",
      "Огнегрив кивнул, чувствуя, как его сердце бьется быстрее.\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_2_1 completed in 66.17s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 66.17s\n",
      "Valid results: 2/2\n",
      "Average time per request: 33.09s\n",
      "Average TTFT: 45.65 seconds\n",
      "Average Tokens/sec: 10.04\n",
      "Average Total tokens: 20057\n",
      "Average Total time: 51.93 seconds\n",
      "Set [20000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_5_1\n",
      "Starting request 20000_5_2\n",
      "Starting request 20000_5_3\n",
      "Starting request 20000_5_4\n",
      "Starting request 20000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 30.79s\n",
      "Конечно\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 20000 input, 3 output\n",
      "Request 20000_5_2 completed in 38.74s\n",
      "\n",
      "First token received at 61.70s\n",
      "**Огон\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 20000 input, 3 output\n",
      "Request 20000_5_3 completed in 69.88s\n",
      "\n",
      "First token received at 92.91s\n",
      "Конечно\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 20000 input, 3 output\n",
      "Request 20000_5_4 completed in 101.24s\n",
      "\n",
      "First token received at 124.05s\n",
      "**Огон\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 20000 input, 3 output\n",
      "Request 20000_5_1 completed in 132.60s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "\n",
      "First token received at 152.72s\n",
      "**Огонь и лед**  \n",
      "**Эрин Хантер**  \n",
      "**Глава IV**\n",
      "\n",
      "— **Вам уже приходилось бывать на землях племени Ветра,** — напомнила предводительница.  \n",
      "\n",
      "Огнегрив и Крутобок переглянулись. Оба помнили, как в прошлом году племя Ветра было изгнано с их зем\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_5 completed in 158.33s\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 158.35s\n",
      "Valid results: 5/5\n",
      "Average time per request: 31.67s\n",
      "Average TTFT: 92.44 seconds\n",
      "Average Tokens/sec: 4.26\n",
      "Average Total tokens: 20025\n",
      "Average Total time: 99.65 seconds\n",
      "Set [20000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 25000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 25000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 40.78s\n",
      "**Книга: \"Огонь и лед\"  \n",
      "Автор: Эрин Хантер  \n",
      "Жанр: Сказки**  \n",
      "\n",
      "---\n",
      "\n",
      "**Пролог**  \n",
      "Оранжевые языки пламени плясали в холодном воздухе, бросая в ночное небо снопы ослепительных искр. Отсветы огня пробегали по жесткой траве пустыря,\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_1_1 completed in 46.71s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 46.71s\n",
      "Valid results: 1/1\n",
      "Average time per request: 46.71s\n",
      "Average TTFT: 40.78 seconds\n",
      "Average Tokens/sec: 18.10\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 46.30 seconds\n",
      "Set [25000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_2_1\n",
      "Starting request 25000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 42.66s\n",
      "**Прод\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 25000 input, 3 output\n",
      "Request 25000_2_1 completed in 51.08s\n",
      "Thread 1/2 completed\n",
      "\n",
      "First token received at 82.30s\n",
      "Огнегрив и Крутобок медленно продвигались вглубь лагеря племени Ветра, стараясь не привлекать внимания. Окружающая обстановка была напряженной — лагерь находился в состоянии полного хаоса. Следы битвы были на виду: разорванные гнезда, разбросанные л\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_2_2 completed in 88.31s\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 88.32s\n",
      "Valid results: 2/2\n",
      "Average time per request: 44.16s\n",
      "Average TTFT: 62.48 seconds\n",
      "Average Tokens/sec: 9.26\n",
      "Average Total tokens: 25057\n",
      "Average Total time: 69.20 seconds\n",
      "Set [25000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_5_1\n",
      "Starting request 25000_5_2\n",
      "Starting request 25000_5_3\n",
      "Starting request 25000_5_4\n",
      "Starting request 25000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 42.50s\n",
      "**Огон\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 25000 input, 3 output\n",
      "Request 25000_5_3 completed in 51.03s\n",
      "\n",
      "First token received at 83.31s\n",
      "**Огон\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 25000 input, 3 output\n",
      "Request 25000_5_1 completed in 91.73s\n",
      "Thread 1/5 completed\n",
      "\n",
      "First token received at 124.58s\n",
      "**Огон\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 25000 input, 3 output\n",
      "Request 25000_5_2 completed in 132.85s\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "\n",
      "First token received at 165.56s\n",
      "**Эрин\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 25000 input, 3 output\n",
      "Request 25000_5_5 completed in 173.68s\n",
      "\n",
      "First token received at 206.81s\n",
      "Простите за прерывание! В предоставленном тексте заканчивается Глава IV, и, похоже, в тексте есть незавершённая часть — это можно увидеть по фразе:  \n",
      "**\"И всюду застарелый дух Сумрачного племени смешивался с запахом охваченного ужасом племени Ветра.  \n",
      "— Пои\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_4 completed in 212.91s\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 212.92s\n",
      "Valid results: 5/5\n",
      "Average time per request: 42.58s\n",
      "Average TTFT: 124.55 seconds\n",
      "Average Tokens/sec: 3.94\n",
      "Average Total tokens: 25025\n",
      "Average Total time: 131.86 seconds\n",
      "Set [25000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 30000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 30000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 52.86s\n",
      "(Продолжение текста)\n",
      "\n",
      "**Глава V (продолжение)**\n",
      "\n",
      "Огнегрив почувствовал, что у него дрожат лапы. Неужели они забрались под саму Гремящую тропу? Он беспокойно распушил свою рыжую шерсть и почувствовал под боком щекочущий кошачий воздух. Воздух здесь был холодным и\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_1_1 completed in 59.32s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 59.32s\n",
      "Valid results: 1/1\n",
      "Average time per request: 59.32s\n",
      "Average TTFT: 52.86 seconds\n",
      "Average Tokens/sec: 16.77\n",
      "Average Total tokens: 30111\n",
      "Average Total time: 58.83 seconds\n",
      "Set [30000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 30000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_2_1\n",
      "Starting request 30000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 53.49s\n",
      "Конечно\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 3 output\n",
      "Request 30000_2_2 completed in 61.69s\n",
      "\n",
      "First token received at 105.92s\n",
      "Вот продолжение текста, описывающее дальнейшие события, вплоть до окончания главы V:\n",
      "\n",
      "---\n",
      "\n",
      "**Глава V**\n",
      "\n",
      "Огнегрив почувствовал, что у него дрожат лапы. Неужели они забрались под саму Гремящую тропу? Он беспокойно распушил свою рыжую шерсть и почувствовал под боком щекоч\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_2_1 completed in 112.38s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 112.38s\n",
      "Valid results: 2/2\n",
      "Average time per request: 56.19s\n",
      "Average TTFT: 79.71 seconds\n",
      "Average Tokens/sec: 8.53\n",
      "Average Total tokens: 30057\n",
      "Average Total time: 86.58 seconds\n",
      "Set [30000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 30000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_5_1\n",
      "Starting request 30000_5_2\n",
      "Starting request 30000_5_3\n",
      "Starting request 30000_5_4\n",
      "Starting request 30000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokensContent truncated from 181863 to 30000 tokens\n",
      "\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "First token received at 53.51s\n",
      "**Прод\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 3 output\n",
      "Request 30000_5_2 completed in 61.86s\n",
      "\n",
      "First token received at 107.67s\n",
      "**Прод\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 3 output\n",
      "Request 30000_5_1 completed in 116.19s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "\n",
      "First token received at 159.63s\n",
      "**Огон\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 3 output\n",
      "Request 30000_5_5 completed in 167.75s\n",
      "\n",
      "First token received at 213.57s\n",
      "**Эрин\n",
      "Generation timeout reached (7s limit)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 3 output\n",
      "Request 30000_5_3 completed in 221.95s\n",
      "Thread 3/5 completed\n",
      "\n",
      "First token received at 266.40s\n",
      "**Огонь и лед**  \n",
      "**Эрин Хантер**  \n",
      "\n",
      "---\n",
      "\n",
      "**Глава V (продолжение)**  \n",
      "\n",
      "Огнегрив почувствовал, что у него дрожат лапы. Неужели они забрались под саму Гремящую тропу? Он беспокойно распушил свою рыжую шерсть и почувствовал под боком щекочущий кошачий\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_4 completed in 272.96s\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 272.97s\n",
      "Valid results: 5/5\n",
      "Average time per request: 54.59s\n",
      "Average TTFT: 160.16 seconds\n",
      "Average Tokens/sec: 3.67\n",
      "Average Total tokens: 30025\n",
      "Average Total time: 167.57 seconds\n",
      "Set [30000 tokens - 5 parallel] completed. Proceeding to next set...\n"
     ]
    }
   ],
   "source": [
    "# Warmup request to prepare the model\n",
    "print(\"Warming up the model with a test request...\")\n",
    "warmup_response, warmup_metrics = measure_vllm_response(\n",
    "    file_path=\"tests/daily.txt\",\n",
    "    max_input_tokens=100,\n",
    "    max_output_tokens=10\n",
    ")\n",
    "print(f\"Warmup completed in {warmup_metrics['total_time']:.2f}s\")\n",
    "print(\"Model is ready for testing.\\n\")\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "# input_token_sizes = [1000, 5000, 10000, 15000 , 20000, 25000, 30000]\n",
    "input_token_sizes = [20000, 25000, 30000]\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_vllm_response(\n",
    "            file_path=file_path,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            time_limit=7\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "concurrent_counts = [1, 2, 5] \n",
    "\n",
    "for token_size in input_token_sizes:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Testing with {token_size} input tokens\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for concurrent_count in concurrent_counts:\n",
    "        print(f\"\\n=== {token_size} tokens with {concurrent_count} concurrent requests ===\")\n",
    "        print(f\"Waiting for all current requests to complete before starting this set...\")\n",
    "        \n",
    "        threads = []\n",
    "        start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        # Start concurrent requests\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: results.append(run_concurrent_test(file_path, token_size, 100, f\"{token_size}_{concurrent_count}_{i+1}\"))\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for ALL threads to complete before proceeding to next set\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            print(f\"Thread {i+1}/{concurrent_count} completed\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_concurrent_time = end_time - start_time\n",
    "        \n",
    "        # Filter out None results (failed requests only)\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        print(f\"All {concurrent_count} requests completed in {total_concurrent_time:.2f}s\")\n",
    "        print(f\"Valid results: {len(valid_results)}/{concurrent_count}\")\n",
    "        print(f\"Average time per request: {total_concurrent_time/concurrent_count:.2f}s\")\n",
    "        \n",
    "        # Save aggregated metrics\n",
    "        # Get model name from vLLM API\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "            models_data = response.json()\n",
    "            model_name = models_data['data'][0]['root'].split('/')[-1] if models_data['data'] else \"unknown_model\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get model name from API: {e}\")\n",
    "            model_name = \"Qwen3-unkown\"  # fallback\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        output_dir = f\"speed_tests/A2_x1/{model_name}\"\n",
    "        # ----------------------------\n",
    "\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{token_size}_length_{concurrent_count}_parallel.txt\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        if valid_results:\n",
    "            avg_ttft = sum(r['ttft'] for r in valid_results) / len(valid_results)\n",
    "            avg_gen_tokens_per_sec = sum(r['gen_tokens_per_sec'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_tokens = sum(r['total_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_time = sum(r['total_time'] for r in valid_results) / len(valid_results)\n",
    "            avg_input_tokens = sum(r['input_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_output_tokens = sum(r['output_tokens'] for r in valid_results) / len(valid_results)\n",
    "            \n",
    "            # Create average metrics dictionary\n",
    "            avg_metrics = {\n",
    "                'ttft': avg_ttft,\n",
    "                'gen_tokens_per_sec': avg_gen_tokens_per_sec,\n",
    "                'total_tokens': avg_total_tokens,\n",
    "                'total_time': avg_total_time,\n",
    "                'input_tokens': avg_input_tokens,\n",
    "                'output_tokens': avg_output_tokens\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Average TTFT: {avg_metrics['ttft']:.2f} seconds\")\n",
    "            print(f\"Average Tokens/sec: {avg_metrics['gen_tokens_per_sec']:.2f}\")\n",
    "            print(f\"Average Total tokens: {avg_metrics['total_tokens']:.0f}\")\n",
    "            print(f\"Average Total time: {avg_metrics['total_time']:.2f} seconds\")\n",
    "            \n",
    "            # Save average metrics to file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ttft: {avg_metrics['ttft']:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_metrics['total_tokens']:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_metrics['total_time']:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_metrics['input_tokens']:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_metrics['output_tokens']:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {len(valid_results)}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\\n\")\n",
    "                f.write(f\"Average metrics for {len(valid_results)}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics for ALL valid results\n",
    "                f.write(\"Individual request metrics:\\n\")\n",
    "                for i, metrics in enumerate(valid_results):\n",
    "                    f.write(f\"\\nRequest {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        else:\n",
    "            print(\"No valid results to save\")\n",
    "        \n",
    "        # Add a pause between different concurrent count sets for the same token size\n",
    "        print(f\"Set [{token_size} tokens - {concurrent_count} parallel] completed. Proceeding to next set...\")\n",
    "        time.sleep(2)  # Brief pause to ensure system is ready for next set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
