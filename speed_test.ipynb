{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19185de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-4B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "response.json()['data'][0]['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d913082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Qwen3_GPUS_metrics\n"
     ]
    }
   ],
   "source": [
    "%cd Qwen3_GPUS_metrics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85baade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 1000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "Хорошо, вы предоставили фрагмент книги **\"Огонь и лед\"** Эрин Хантер из электронной библиотеки LITRU.RU. Это часть **пролога** книги, который описывает драматическую ситуацию — коты племени Ветра, изгнанные Сумрачным племенем, ищут убежище в опасном месте,Start time: 2025-09-23 12:21:14\n",
      "End time: 2025-09-23 12:21:18\n",
      "Tokens generated: 100\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 1000 tokens:\n",
      "Average TTFT: 2.43s\n",
      "Average gen tokens/sec: 26.09\n",
      "Average total time: 6.27s\n",
      "Total concurrent time: 6.70s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/A10/Qwen3-14B-INT8/1000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 5000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "**Огонь и лед**  \n",
      "**Эрин Хантер**  \n",
      "*Из цикла \"Коты-воины\"*\n",
      "\n",
      "---\n",
      "\n",
      "### **Пролог**\n",
      "\n",
      "Пламя плясало на пустыре, отбрасывая о"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def measure_vllm_response(file_path, vllm_url=\"http://localhost:8000/v1/chat/completions\", \n",
    "                         max_input_tokens=None, max_output_tokens=1024, max_generation_time=7.0, \n",
    "                         print_stream=False, benchmark_mode=False):\n",
    "    \"\"\"\n",
    "    Send file content to vLLM chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        vllm_url: vLLM endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "        max_generation_time: Maximum time in seconds for generation after prefill (default: 7.0)\n",
    "        print_stream: Whether to print streaming content (default: False for benchmarking)\n",
    "        benchmark_mode: If True, optimize for precise TTFT measurement\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            if not benchmark_mode:\n",
    "                print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "        \"stream_options\": {\"include_usage\": True},  # Request usage info in stream\n",
    "        \"chat_template_kwargs\": {\n",
    "                \"enable_thinking\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"text/event-stream\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "    \n",
    "    # Use session for better connection management\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    generation_start_time = None\n",
    "    generation_stopped_early = False\n",
    "    \n",
    "    # Send request to vLLM with streaming\n",
    "    response = session.post(vllm_url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        session.close()\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    actual_input_tokens = None\n",
    "    actual_output_tokens = None\n",
    "    \n",
    "    if print_stream and not benchmark_mode:\n",
    "        print(\"Response streaming:\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Use minimal buffering for faster first token detection\n",
    "        for line in response.iter_lines(chunk_size=1, decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if line.startswith('data: '):\n",
    "                data_str = line[6:]  # Remove 'data: ' prefix\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "                    \n",
    "                    # Check for usage information (comes in final event)\n",
    "                    if 'usage' in data:\n",
    "                        actual_input_tokens = data['usage']['prompt_tokens']\n",
    "                        actual_output_tokens = data['usage']['completion_tokens']\n",
    "                        if not benchmark_mode:\n",
    "                            print(f\"\\nUsage info received: {actual_input_tokens} input tokens, {actual_output_tokens} output tokens\")\n",
    "                    \n",
    "                    if 'choices' in data and len(data['choices']) > 0:\n",
    "                        choice = data['choices'][0]\n",
    "                        if 'delta' in choice and 'content' in choice['delta']:\n",
    "                            current_time = time.time()\n",
    "                            \n",
    "                            # Record TTFT (time to first token) - do this FIRST before any other processing\n",
    "                            if ttft is None:\n",
    "                                ttft = current_time - start_time\n",
    "                                generation_start_time = current_time\n",
    "                            \n",
    "                            content_chunk = choice['delta']['content']\n",
    "                            full_response += content_chunk\n",
    "                            \n",
    "                            # Print the streaming tokens\n",
    "                            print(content_chunk, end='', flush=True)\n",
    "                            \n",
    "                            # Check if generation time exceeds max_generation_time (only if not benchmark mode)\n",
    "                            if not benchmark_mode and generation_start_time is not None:\n",
    "                                generation_elapsed = current_time - generation_start_time\n",
    "                                if generation_elapsed > max_generation_time:\n",
    "                                    print(f\"\\nGeneration stopped early after {generation_elapsed:.2f} seconds (max: {max_generation_time}s)\")\n",
    "                                    generation_stopped_early = True\n",
    "                                    response.close()\n",
    "                                    break\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # Check timeout after each line as well (only if not benchmark mode)\n",
    "            if not benchmark_mode and generation_start_time is not None:\n",
    "                generation_elapsed = time.time() - generation_start_time\n",
    "                if generation_elapsed > max_generation_time:\n",
    "                    print(f\"\\nGeneration stopped early after {generation_elapsed:.2f} seconds (max: {max_generation_time}s)\")\n",
    "                    generation_stopped_early = True\n",
    "                    response.close()\n",
    "                    break\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If there's any error during streaming, close the response\n",
    "        response.close()\n",
    "        session.close()\n",
    "        if not generation_stopped_early:\n",
    "            raise e\n",
    "    \n",
    "    # Clean up\n",
    "    response.close()\n",
    "    session.close()\n",
    "    \n",
    "    if print_stream and not benchmark_mode:\n",
    "        print()  # New line after streaming\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Use actual token counts from server if available, otherwise fall back to tokenizer estimate\n",
    "    if actual_input_tokens is not None and actual_output_tokens is not None and not generation_stopped_early:\n",
    "        input_tokens = actual_input_tokens\n",
    "        output_tokens = actual_output_tokens\n",
    "        if not benchmark_mode:\n",
    "            print(f\"Using server-reported token counts: {input_tokens} input, {output_tokens} output\")\n",
    "    else:\n",
    "        # Fallback: estimate output tokens using tokenizer\n",
    "        output_tokens = len(tokenizer.encode(full_response))\n",
    "        if not benchmark_mode:\n",
    "            print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "            if generation_stopped_early:\n",
    "                print(\"Note: Generation was stopped early due to time limit\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time,\n",
    "        'generation_stopped_early': generation_stopped_early\n",
    "    }\n",
    "    \n",
    "    # Print additional information\n",
    "    # if not benchmark_mode:\n",
    "    start_time_formatted = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(generation_start_time))\n",
    "    end_time_formatted = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(end_time))\n",
    "    print(f\"Start time: {start_time_formatted}\")\n",
    "    print(f\"End time: {end_time_formatted}\")\n",
    "    print(f\"Tokens generated: {output_tokens}\")\n",
    "    print(f\"Generation speed formula: Tokens/sec = Total tokens / Total time\")\n",
    "\n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    # print(f\"E2E tokens/sec (incl. prefill): {metrics['e2e_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    if metrics.get('generation_stopped_early', False):\n",
    "        print(\"Generation was stopped early due to time limit\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        # f.write(f\"e2e_tokens_per_second: {metrics['e2e_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\")\n",
    "        if metrics.get('generation_stopped_early', False):\n",
    "            f.write(\"generation_stopped_early: true\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "vllm_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "max_output_tokens = 100\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    if not os.environ.get('BENCHMARK_QUIET'):\n",
    "        print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_vllm_response(\n",
    "            file_path=file_path,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            print_stream=False,  # Disable streaming output for benchmarks\n",
    "            benchmark_mode=True   # Enable benchmark optimizations\n",
    "        )\n",
    "            \n",
    "        end = time.time()\n",
    "        if not os.environ.get('BENCHMARK_QUIET'):\n",
    "            print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "input_token_sizes = [1000, 5000, 10000, 15000, 20000, 25000, 30000]\n",
    "# input_token_sizes = [15000]\n",
    "concurrent_counts = [1, 2, 5]\n",
    "\n",
    "# Set environment variable to reduce noise during benchmarking\n",
    "os.environ['BENCHMARK_QUIET'] = '1'\n",
    "\n",
    "for concurrent_count in concurrent_counts:\n",
    "    for token_size in input_token_sizes:\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"Testing {concurrent_count} concurrent requests with {token_size} input tokens each\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        threads = []\n",
    "        all_metrics = []\n",
    "        \n",
    "        # Start all threads\n",
    "        concurrent_start_time = time.time()\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: all_metrics.append(\n",
    "                    run_concurrent_test(file_path, token_size, max_output_tokens, i+1)\n",
    "                )\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "            time.sleep(0.1)  # Small delay to avoid overwhelming the server\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            if not os.environ.get('BENCHMARK_QUIET'):\n",
    "                print(f\"Thread {i+1} joined\")\n",
    "        \n",
    "        concurrent_end_time = time.time()\n",
    "        total_concurrent_time = concurrent_end_time - concurrent_start_time\n",
    "        \n",
    "        # Filter out None results (failed requests)\n",
    "        valid_metrics = [m for m in all_metrics if m is not None]\n",
    "        valid_requests = len(valid_metrics)\n",
    "        \n",
    "        if valid_requests > 0:\n",
    "            # Calculate aggregate metrics\n",
    "            avg_ttft = sum(m['ttft'] for m in valid_metrics) / valid_requests\n",
    "            avg_gen_tokens_per_sec = sum(m['gen_tokens_per_sec'] for m in valid_metrics) / valid_requests\n",
    "            avg_total_tokens = sum(m['total_tokens'] for m in valid_metrics) / valid_requests\n",
    "            avg_total_time = sum(m['total_time'] for m in valid_metrics) / valid_requests\n",
    "            avg_input_tokens = sum(m['input_tokens'] for m in valid_metrics) / valid_requests\n",
    "            avg_output_tokens = sum(m['output_tokens'] for m in valid_metrics) / valid_requests\n",
    "            \n",
    "            # Create output filename\n",
    "            # Get model name from API\n",
    "            response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "            model_name = response.json()['data'][0]['root'].split('/')[-1]\n",
    "            \n",
    "            output_filename = f\"speed_tests/A10/{model_name}/{token_size}_length_{concurrent_count}_parallel.txt\"\n",
    "            os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "            # Save results to file\n",
    "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                # Write aggregate metrics\n",
    "                f.write(f\"ttft: {avg_ttft:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_gen_tokens_per_sec:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_total_tokens:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_total_time:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_input_tokens:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_output_tokens:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {valid_requests}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\")\n",
    "                f.write(f\"\\nAverage metrics for {valid_requests}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics\n",
    "                f.write(\"Individual request metrics:\\n\\n\")\n",
    "                for i, metrics in enumerate(valid_metrics):\n",
    "                    f.write(f\"Request {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "                    f.write(f\"\\n\")\n",
    "            \n",
    "            print(f\"\\nResults for {concurrent_count} concurrent requests with {token_size} tokens:\")\n",
    "            print(f\"Average TTFT: {avg_ttft:.2f}s\")\n",
    "            print(f\"Average gen tokens/sec: {avg_gen_tokens_per_sec:.2f}\")\n",
    "            print(f\"Average total time: {avg_total_time:.2f}s\")\n",
    "            print(f\"Total concurrent time: {total_concurrent_time:.2f}s\")\n",
    "            print(f\"Valid requests: {valid_requests}/{concurrent_count}\")\n",
    "            print(f\"Results saved to: {output_filename}\")\n",
    "        else:\n",
    "            print(f\"All requests failed for {concurrent_count} concurrent requests with {token_size} tokens\")\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        time.sleep(2)\n",
    "\n",
    "# Clean up environment variable\n",
    "if 'BENCHMARK_QUIET' in os.environ:\n",
    "    del os.environ['BENCHMARK_QUIET']\n",
    "\n",
    "print(\"\\nAll tests completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
