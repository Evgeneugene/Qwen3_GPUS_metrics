{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19185de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/llms/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-4B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "response.json()['data'][0]['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85baade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def measure_vllm_response(file_path, vllm_url=\"http://localhost:8000/v1/chat/completions\", \n",
    "                         max_input_tokens=None, max_output_tokens=1024, max_generation_time=7.0):\n",
    "    \"\"\"\n",
    "    Send file content to vLLM chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        vllm_url: vLLM endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "        max_generation_time: Maximum time in seconds for generation after prefill (default: 7.0)\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "        \"stream_options\": {\"include_usage\": True},  # Request usage info in stream\n",
    "        \"chat_template_kwargs\": {\n",
    "                \"enable_thinking\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    generation_start_time = None\n",
    "    generation_stopped_early = False\n",
    "    \n",
    "    # Send request to vLLM with streaming\n",
    "    response = requests.post(vllm_url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    actual_input_tokens = None\n",
    "    actual_output_tokens = None\n",
    "    \n",
    "    print(\"Response streaming:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line = line.decode('utf-8')\n",
    "                if line.startswith('data: '):\n",
    "                    data_str = line[6:]  # Remove 'data: ' prefix\n",
    "                    if data_str.strip() == '[DONE]':\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        data = json.loads(data_str)\n",
    "                        \n",
    "                        # Check for usage information (comes in final event)\n",
    "                        if 'usage' in data:\n",
    "                            actual_input_tokens = data['usage']['prompt_tokens']\n",
    "                            actual_output_tokens = data['usage']['completion_tokens']\n",
    "                            print(f\"\\nUsage info received: {actual_input_tokens} input tokens, {actual_output_tokens} output tokens\")\n",
    "                        \n",
    "                        if 'choices' in data and len(data['choices']) > 0:\n",
    "                            choice = data['choices'][0]\n",
    "                            if 'delta' in choice and 'content' in choice['delta']:\n",
    "                                current_time = time.time()\n",
    "                                \n",
    "                                # Record TTFT (time to first token)\n",
    "                                if ttft is None:\n",
    "                                    ttft = current_time - start_time\n",
    "                                    generation_start_time = current_time\n",
    "                                \n",
    "                                # Check if generation time exceeds max_generation_time\n",
    "                                if generation_start_time is not None:\n",
    "                                    generation_elapsed = current_time - generation_start_time\n",
    "                                    if generation_elapsed > max_generation_time:\n",
    "                                        print(f\"\\nGeneration stopped early after {generation_elapsed:.2f} seconds (max: {max_generation_time}s)\")\n",
    "                                        generation_stopped_early = True\n",
    "                                        # Force close the response stream\n",
    "                                        response.close()\n",
    "                                        break\n",
    "                                \n",
    "                                content_chunk = choice['delta']['content']\n",
    "                                full_response += content_chunk\n",
    "                                \n",
    "                                # Stream to console\n",
    "                                print(content_chunk, end='', flush=True)\n",
    "                                \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                \n",
    "                # Check timeout after each line as well\n",
    "                if generation_start_time is not None:\n",
    "                    generation_elapsed = time.time() - generation_start_time\n",
    "                    if generation_elapsed > max_generation_time:\n",
    "                        print(f\"\\nGeneration stopped early after {generation_elapsed:.2f} seconds (max: {max_generation_time}s)\")\n",
    "                        generation_stopped_early = True\n",
    "                        response.close()\n",
    "                        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If there's any error during streaming, close the response\n",
    "        response.close()\n",
    "        if not generation_stopped_early:\n",
    "            raise e\n",
    "    \n",
    "    print()  # New line after streaming\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Use actual token counts from server if available, otherwise fall back to tokenizer estimate\n",
    "    if actual_input_tokens is not None and actual_output_tokens is not None and not generation_stopped_early:\n",
    "        input_tokens = actual_input_tokens\n",
    "        output_tokens = actual_output_tokens\n",
    "        print(f\"Using server-reported token counts: {input_tokens} input, {output_tokens} output\")\n",
    "    else:\n",
    "        # Fallback: estimate output tokens using tokenizer\n",
    "        output_tokens = len(tokenizer.encode(full_response))\n",
    "        print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "        if generation_stopped_early:\n",
    "            print(\"Note: Generation was stopped early due to time limit\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time,\n",
    "        'generation_stopped_early': generation_stopped_early\n",
    "    }\n",
    "    \n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    # print(f\"E2E tokens/sec (incl. prefill): {metrics['e2e_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    if metrics.get('generation_stopped_early', False):\n",
    "        print(\"Generation was stopped early due to time limit\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        # f.write(f\"e2e_tokens_per_second: {metrics['e2e_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\")\n",
    "        if metrics.get('generation_stopped_early', False):\n",
    "            f.write(\"generation_stopped_early: true\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "# file_path = \"tests/daily.txt\"\n",
    "# vllm_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "# max_input_tokens =  28000\n",
    "# max_output_tokens = 1024\n",
    "\n",
    "# measure_vllm_response(file_path, vllm_url, max_input_tokens, max_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up the model with a test request...\n",
      "Content truncated from 13961 to 100 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Кажется, вы говорите на рус\n",
      "Usage info received: 111 input tokens, 10 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 111 input, 10 output\n",
      "Warmup completed in 3.07s\n",
      "Model is ready for testing.\n",
      "\n",
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing with 30000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 30000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_2_1\n",
      "Starting request 30000_2_2\n",
      "Waiting for all 2 threads to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (181863 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "****ГлавГлаваа V V ( (продпродолжолжениеение)**)**\n",
      "\n",
      "\n",
      "\n",
      "ООггннегегрривив по почувствчувствоваловал,, как как у его него д дрожрожьат уси лливапаетсяы.. Он Не неуж могели пр ониой забтира дальшелись, под\n",
      "Generation stopped early after 7.03 seconds (max: 7.0s)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 37 output\n",
      "Note: Generation was stopped early due to time limit\n",
      "Request 30000_2_2 completed in 95.50s\n",
      " саму Гремящую тропу? Он беспокойно распушил свою рыжую шерсть и почувствовал, как под боком щекочущий кошачий запах. Воздух был тяж\n",
      "Generation stopped early after 7.16 seconds (max: 7.0s)\n",
      "\n",
      "--------------------------------------------------\n",
      "Using tokenizer estimates: 30000 input, 94 output\n",
      "Note: Generation was stopped early due to time limit\n",
      "Request 30000_2_1 completed in 100.09s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 100.09s\n",
      "Valid results: 2/2\n",
      "Average time per request: 50.04s\n",
      "Average TTFT: 89.84 seconds\n",
      "Average Tokens/sec: 9.20\n",
      "Average Total tokens: 30066\n",
      "Average Total time: 96.93 seconds\n",
      "Set [30000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 30000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_5_1\n",
      "Starting request 30000_5_2\n",
      "Starting request 30000_5_3\n",
      "Starting request 30000_5_4\n",
      "Starting request 30000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**"
     ]
    }
   ],
   "source": [
    "# Warmup request to prepare the model\n",
    "print(\"Warming up the model with a test request...\")\n",
    "warmup_response, warmup_metrics = measure_vllm_response(\n",
    "    file_path=\"tests/daily.txt\",\n",
    "    max_input_tokens=100,\n",
    "    max_output_tokens=10\n",
    ")\n",
    "print(f\"Warmup completed in {warmup_metrics['total_time']:.2f}s\")\n",
    "print(\"Model is ready for testing.\\n\")\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "# input_token_sizes = [1000, 5000, 10000, 15000 , 20000, 25000, 30000]\n",
    "input_token_sizes = [30000]\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_vllm_response(\n",
    "            file_path=file_path,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "            \n",
    "        end = time.time()\n",
    "        print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "concurrent_counts = [2, 5] \n",
    "\n",
    "for token_size in input_token_sizes:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Testing with {token_size} input tokens\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for concurrent_count in concurrent_counts:\n",
    "        print(f\"\\n=== {token_size} tokens with {concurrent_count} concurrent requests ===\")\n",
    "        print(f\"Waiting for all current requests to complete before starting this set...\")\n",
    "        \n",
    "        threads = []\n",
    "        start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        # Start concurrent requests\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: results.append(run_concurrent_test(file_path, token_size, 100, f\"{token_size}_{concurrent_count}_{i+1}\"))\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for ALL threads to complete before proceeding to next set\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            print(f\"Thread {i+1}/{concurrent_count} completed\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_concurrent_time = end_time - start_time\n",
    "        \n",
    "        # Filter out None results (failed requests only)\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        print(f\"All {concurrent_count} requests completed in {total_concurrent_time:.2f}s\")\n",
    "        print(f\"Valid results: {len(valid_results)}/{concurrent_count}\")\n",
    "        print(f\"Average time per request: {total_concurrent_time/concurrent_count:.2f}s\")\n",
    "        \n",
    "        # Save aggregated metrics\n",
    "        # Get model name from vLLM API\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "            models_data = response.json()\n",
    "            model_name = models_data['data'][0]['root'].split('/')[-1] if models_data['data'] else \"unknown_model\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get model name from API: {e}\")\n",
    "            model_name = \"Qwen3-unkown\"  # fallback\n",
    "            \n",
    "        output_dir = f\"speed_tests/T2_x2/{model_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{token_size}_length_{concurrent_count}_parallel.txt\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        if valid_results:\n",
    "            avg_ttft = sum(r['ttft'] for r in valid_results) / len(valid_results)\n",
    "            avg_gen_tokens_per_sec = sum(r['gen_tokens_per_sec'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_tokens = sum(r['total_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_time = sum(r['total_time'] for r in valid_results) / len(valid_results)\n",
    "            avg_input_tokens = sum(r['input_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_output_tokens = sum(r['output_tokens'] for r in valid_results) / len(valid_results)\n",
    "            \n",
    "            # Create average metrics dictionary\n",
    "            avg_metrics = {\n",
    "                'ttft': avg_ttft,\n",
    "                'gen_tokens_per_sec': avg_gen_tokens_per_sec,\n",
    "                'total_tokens': avg_total_tokens,\n",
    "                'total_time': avg_total_time,\n",
    "                'input_tokens': avg_input_tokens,\n",
    "                'output_tokens': avg_output_tokens\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Average TTFT: {avg_metrics['ttft']:.2f} seconds\")\n",
    "            print(f\"Average Tokens/sec: {avg_metrics['gen_tokens_per_sec']:.2f}\")\n",
    "            print(f\"Average Total tokens: {avg_metrics['total_tokens']:.0f}\")\n",
    "            print(f\"Average Total time: {avg_metrics['total_time']:.2f} seconds\")\n",
    "            \n",
    "            # Save average metrics to file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ttft: {avg_metrics['ttft']:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_metrics['total_tokens']:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_metrics['total_time']:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_metrics['input_tokens']:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_metrics['output_tokens']:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {len(valid_results)}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\\n\")\n",
    "                f.write(f\"Average metrics for {len(valid_results)}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics for ALL valid results\n",
    "                f.write(\"Individual request metrics:\\n\")\n",
    "                for i, metrics in enumerate(valid_results):\n",
    "                    f.write(f\"\\nRequest {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        else:\n",
    "            print(\"No valid results to save\")\n",
    "        \n",
    "        # Add a pause between different concurrent count sets for the same token size\n",
    "        print(f\"Set [{token_size} tokens - {concurrent_count} parallel] completed. Proceeding to next set...\")\n",
    "        time.sleep(2)  # Brief pause to ensure system is ready for next set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
