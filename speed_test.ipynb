{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19185de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B-AWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-8B'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "response.json()['data'][0]['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8339ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Кажется, вы прервали текст на середине предложения. Позвольте мне продолжить и завершить главу V, а также, если вы хотите, продолжить следующие главы. Вот полный текст главы V:\n",
      "\n",
      "---\n",
      "\n",
      "**Глава V**\n",
      "\n",
      "Огнегрив выждал, пока вокруг него смолкли все звуки, кроме стука собственной крови в ушах. Затем осторожно ступил на край Гремящей Тропы. Путь лежал перед ним — широкий, зловонный, зато абсолютно тихий. Огнегрив прыгнул. На ощупь тропа оказалась на удивление холодной и гладкой, и Огнегрив мчался по ней, не останавливаясь до тех пор, пока не коснулся травы на другой стороне.\n",
      "\n",
      "Он принюхался. Воздух был полон ядовитого дыхания чудищ, поэтому Огнегрив торопливо бросился к живой изгороди. Но и здесь не было ни малейшего следа племени Ветра. У Огнегрива упало сердце.\n",
      "\n",
      "Внезапно за спиной у него взревело чудище, и Огнегрив подскочил от ужаса. Придя в себя, он забился под изгородь и сжался в комок. Что теперь делать? Куда идти? И тут он учуял его! Ревущее чудище принесло с собой волну воздуха, в котором ясно чувствовался едва уловимый кошачий запах. Значит, племя Ветра здесь! Огнегрив вскочил, и что было силы завопил, подзывая Крутобока. Вначале все было тихо, затем он услышал быстрый топот лап по Гремящей Тропе, и наконец у изгороди появился Крутобок.\n",
      "\n",
      "— Нашел? — пропыхтел он.\n",
      "\n",
      "— Не совсем. Я учуял запах, но не могу понять, откуда он доносится.\n",
      "\n",
      "Огнегрив засеменил вдоль изгороди, приятель пристроился за ним. Огнегрив задрал нос и обнюхал воздух над головой.\n",
      "\n",
      "— А что там наверху? — спросил он.\n",
      "\n",
      "— Понятия не имею, — ответил Крутобок. — Мне кажется, так далеко не забирался ни один кот!\n",
      "\n",
      "— Кроме котов из племени Ветра, — мрачно поправил его Огнегрив. Здесь, в стороне от удушливых испарений Гремящей Тропы, запах стал гораздо сильнее. Теперь он не сомневался, что племя Ветра прошло этой тропой. Вынырнув из высокой травы, коты очутились на краю широкого поля.\n",
      "\n",
      "— Гляди, Огнегрив! — испуганно прошептал Крутобок.\n",
      "\n",
      "— Что такое?\n",
      "\n",
      "— Да ты смотри!\n",
      "\n",
      "Огнегрив остановился и задрал голову. Над ними, высоко в небе, тяжело опираясь на каменные ноги, парила Гремящая Тропа, освещенная глазами пробегающих чудищ. Еще одна Гремящая Тропа осталась внизу, убегая далеко в темноту.\n",
      "\n",
      "Крутобок вдруг к"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmeasure_vllm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtests/book.txt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_input_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mmeasure_vllm_response\u001b[39m\u001b[34m(file_path, vllm_url, max_input_tokens, max_output_tokens)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResponse streaming:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/models.py:869\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self, chunk_size, decode_unicode, delimiter)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[32m    861\u001b[39m \u001b[33;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[32m    862\u001b[39m \u001b[33;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[32m    863\u001b[39m \n\u001b[32m    864\u001b[39m \u001b[33;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[32m    865\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m pending = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/response.py:1248\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1245\u001b[39m     amt = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/urllib3/response.py:1167\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1168\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "measure_vllm_response('tests/book.txt', max_input_tokens=30000, max_output_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e85baade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import signal\n",
    "import threading\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "def measure_vllm_response(file_path, vllm_url=\"http://localhost:8000/v1/chat/completions\", \n",
    "                         max_input_tokens=None, max_output_tokens=1024, time_limit=7):\n",
    "    \"\"\"\n",
    "    Send file content to vLLM chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        vllm_url: vLLM endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "        time_limit: Maximum time in seconds for generation (after prefill), None for no limit\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "        \"stream_options\": {\"include_usage\": True},  # Request usage info in stream\n",
    "        \"chat_template_kwargs\": {\n",
    "                \"enable_thinking\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    generation_timeout = False\n",
    "    \n",
    "    # Send request to vLLM with streaming\n",
    "    response = requests.post(vllm_url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    actual_input_tokens = None\n",
    "    actual_output_tokens = None\n",
    "    \n",
    "    print(\"Response streaming:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line = line.decode('utf-8')\n",
    "            if line.startswith('data: '):\n",
    "                data_str = line[6:]  # Remove 'data: ' prefix\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "                    \n",
    "                    # Check for usage information (comes in final event)\n",
    "                    if 'usage' in data:\n",
    "                        actual_input_tokens = data['usage']['prompt_tokens']\n",
    "                        actual_output_tokens = data['usage']['completion_tokens']\n",
    "                        print(f\"\\nUsage info received: {actual_input_tokens} input tokens, {actual_output_tokens} output tokens\")\n",
    "                    \n",
    "                    if 'choices' in data and len(data['choices']) > 0:\n",
    "                        choice = data['choices'][0]\n",
    "                        if 'delta' in choice and 'content' in choice['delta']:\n",
    "                            # Record TTFT (time to first token)\n",
    "                            if ttft is None:\n",
    "                                ttft = time.time() - start_time\n",
    "                                print(f\"\\nFirst token received at {ttft:.2f}s\")\n",
    "                            \n",
    "                            # Check time limit after first token (generation phase)\n",
    "                            if time_limit is not None and ttft is not None:\n",
    "                                generation_time = time.time() - start_time - ttft\n",
    "                                if generation_time > time_limit:\n",
    "                                    print(f\"\\nGeneration timeout reached ({time_limit}s limit)\")\n",
    "                                    generation_timeout = True\n",
    "                                    break\n",
    "                            \n",
    "                            content_chunk = choice['delta']['content']\n",
    "                            full_response += content_chunk\n",
    "                            \n",
    "                            # Stream to console\n",
    "                            print(content_chunk, end='', flush=True)\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    print()  # New line after streaming\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Use actual token counts from server if available, otherwise fall back to tokenizer estimate\n",
    "    if actual_input_tokens is not None and actual_output_tokens is not None:\n",
    "        input_tokens = actual_input_tokens\n",
    "        output_tokens = actual_output_tokens\n",
    "        print(f\"Using server-reported token counts: {input_tokens} input, {output_tokens} output\")\n",
    "    else:\n",
    "        # Fallback: estimate output tokens using tokenizer\n",
    "        output_tokens = len(tokenizer.encode(full_response))\n",
    "        print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time,\n",
    "        'generation_timeout': generation_timeout\n",
    "    }\n",
    "    \n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    # print(f\"E2E tokens/sec (incl. prefill): {metrics['e2e_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    if metrics.get('generation_timeout', False):\n",
    "        print(\"Generation was interrupted due to timeout\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        # f.write(f\"e2e_tokens_per_second: {metrics['e2e_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\")\n",
    "        if metrics.get('generation_timeout', False):\n",
    "            f.write(\"generation_timeout: true\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "# file_path = \"tests/daily.txt\"\n",
    "# vllm_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "# max_input_tokens =  28000\n",
    "# max_output_tokens = 1024\n",
    "\n",
    "# measure_vllm_response(file_path, vllm_url, max_input_tokens, max_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77d57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create output directory if it doesn't exist\n",
    "# # Get model name from running vLLM server\n",
    "# try:\n",
    "#     import requests\n",
    "#     response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "#     if response.status_code == 200:\n",
    "#         models_data = response.json()\n",
    "#         if models_data.get('data') and len(models_data['data']) > 0:\n",
    "#             model_name = models_data['data'][0]['root'].split('/')[-1]\n",
    "#         else:\n",
    "#             model_name = \"unknown_model\"\n",
    "#     else:\n",
    "#         model_name = \"unknown_model\"\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not get model name from server: {e}\")\n",
    "#     model_name = \"unknown_model\"\n",
    "\n",
    "# output_dir = f\"summary_results/T2_x2/{model_name}\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Find all .txt files in tests folder\n",
    "# txt_files = glob.glob(\"tests/*.txt\")\n",
    "\n",
    "# print(f\"Found {len(txt_files)} .txt files in tests folder:\")\n",
    "# for file in txt_files:\n",
    "#     print(f\"  - {file}\")\n",
    "\n",
    "# print(\"\\nStarting measurements...\\n\")\n",
    "\n",
    "# # Process each .txt file\n",
    "# for file_path in txt_files:\n",
    "#     # Get base filename without extension\n",
    "#     base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "#     output_file = os.path.join(output_dir, f\"{base_name}_result.txt\")\n",
    "    \n",
    "#     print(f\"=== Processing {file_path} ===\")\n",
    "    \n",
    "#     try:\n",
    "#         full_response, metrics = measure_vllm_response(\n",
    "#             file_path=file_path,\n",
    "#             max_input_tokens=31000,  # No token limit\n",
    "#             max_output_tokens=1024\n",
    "#         )\n",
    "        \n",
    "#         save_and_print_metrics(full_response, metrics, output_file)\n",
    "#         print(f\"Results saved to: {output_file}\\n\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\\n\")\n",
    "#         continue\n",
    "\n",
    "# print(\"All files processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0039853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"tests/daily.txt\"\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "# print(\"Testing different input token sizes:\")\n",
    "# for token_size in input_token_sizes:\n",
    "#     print(f\"\\n=== Testing with {token_size} input tokens ===\")\n",
    "#     output_file = f\"speed_tests/A2/{token_size}_length_1_parallel.txt\"\n",
    "    \n",
    "#     response, metrics = measure_vllm_response(\n",
    "#         file_path=file_path,\n",
    "#         max_input_tokens=token_size,\n",
    "#         max_output_tokens=100\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813e088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15c30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up the model with a test request...\n",
      "Content truncated from 13961 to 100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Кажется, вы перехватили раз\n",
      "Usage info received: 111 input tokens, 10 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 111 input, 10 output\n",
      "Warmup completed in 2.17s\n",
      "Model is ready for testing.\n",
      "\n",
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing with 1000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 1000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_1_1\n",
      "Waiting for all 1 threads to complete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (181863 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Конечно! Вот продолжение текста, который вы привели, с возможным завершением или дополнением, если это необходимо. Если вы хотите, чтобы я продолжил рассказ или прокомментировал его, дайте знать.\n",
      "\n",
      "---\n",
      "\n",
      "**Продолжение пролога:**\n",
      "\n",
      "— Мы не сумели защитить своих детей от Сумрачного племени, — продолжил черный кот, тяжело припадая на изув\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_1_1 completed in 5.87s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 5.87s\n",
      "Valid results: 1/1\n",
      "Average time per request: 5.87s\n",
      "Average TTFT: 0.59 seconds\n",
      "Average Tokens/sec: 20.56\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 5.45 seconds\n",
      "Set [1000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 1000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_2_1\n",
      "Starting request 1000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ВотТы продолж предоставениеил пр интересологныйа от,рыв который выок пр изив книгиели **,\" сО сохрангонениемь и сти лляед\" и** на Эстроринения Х оригинаанльтерной, книги которая ** являетсяЭ частьриныю Х серииан **\"тераП**лем.ена Пр Кологот заваер\"ш**.ает рассказ Э ота к серотия —ах один, из б самыхег известущныхих от и популяр уныхгр фозэныт иез ви по-исциклкахов у длябеж подиростща,ков и, готов напитис канны основхному под с псюевждониметуом ** —Э **рин\" ХОангтерня и** ( Лнаь самомда деле\" —** коллектив.\n",
      "\n",
      " автор---\n",
      "\n",
      "ов**,Пр вологключ (продая Джолж\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_2_2 completed in 7.04s\n",
      "о\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_2_1 completed in 7.09s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 7.09s\n",
      "Valid results: 2/2\n",
      "Average time per request: 3.55s\n",
      "Average TTFT: 0.88 seconds\n",
      "Average Tokens/sec: 17.55\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 6.59 seconds\n",
      "Set [1000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 1000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 1000_5_1\n",
      "Starting request 1000_5_2\n",
      "Starting request 1000_5_3\n",
      "Starting request 1000_5_4\n",
      "Starting request 1000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Content truncated from 181863 to 1000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "ПрологПрКологон кККон кни книгиечнониечнога! **ге! ** ** Ни« Вот«Оже« продолж представленООгонгонь текстгонение прьь и пролог иолог л иаа л лед книги книгиед»ед **» **»**\" Э**\"**ООри Э (гонгоннаныриььны Х р и Хусан и лскомтеран лед:атеред\" ** начинаа\"** —**«ется Э с ЭО этори д велигонриьколнынырам и Х Хепатноеанан лическойтередтер и в,,»ведение нап**) на нап вря — мир основеисжен предоставлен этоной,анныного с часть вй серии текстц котором ва **ены т ст.илая,« Я котораятсяПе продолж сразу т оригиналему жеайльена рассказ» пныной, серииог,** сохран кон Э **ружяриф\"аетянылик чП стильитес Хты ианниат и а л делятерт,ес врамм котораяата мирос в к\"ическиеферот события**ключуят. (ает,«, в Этот пр себяВ жив фис несколькоетрагущущментер книгихие, в, в сер о л Оводияписывагонеситму чющихь \" жизнь,ит,П гдеат Л и приеляедлем ценаключенияар и в к а В Титотовт хаьетмрама вос вос и»\" иол уфер). \"шугр ЭтотСеб напоз прумномряаологра о миреж.чпис, Вённого где этотностиывает п и нап момент существлемуют у вряени разжен лгрныееснуюоз\".\n",
      "\n",
      "---\n",
      "\n",
      " пуы ситу** происациюлем,Прходятена созд волог важ л,ав каждый (ныеесаяпроду из события оолж:,щ которых имеетение где **ущ)**Сение свою кл культууман\n",
      "\n",
      " ре—альногораруы  кч у,Мноеотов тгры сталоз прад некиылеми сумваяции иели** иются нап защит вря с изить ужгонраг своихгрёнов\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_2 generation took 8.09s (>7s), but recording metrics...\n",
      "Request 1000_5_2 completed in 9.14s\n",
      "яетоз.\n",
      "\n",
      " **\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      " детей\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_4 completed in 9.20s\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_1 completed in 9.20s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "### от\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_3 completed in 9.26s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "\n",
      "Usage info received: 1011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 1011 input, 100 output\n",
      "Request 1000_5_5 completed in 9.26s\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 9.26s\n",
      "Valid results: 5/5\n",
      "Average time per request: 1.85s\n",
      "Average TTFT: 1.96 seconds\n",
      "Average Tokens/sec: 15.13\n",
      "Average Total tokens: 1111\n",
      "Average Total time: 8.67 seconds\n",
      "Set [1000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 5000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 5000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Очень интересный текст! Это фрагмент книги **\"Огонь и лед\"** Эрины Хантер, которая является частью серии книг о кошачьих племенах, написанных под псевдонимом **Эрин Хантер**. Эта серия включает в себя такие популярные произведения, как *\"Песня леса\"*, *\"Тайна леса\"*, *\"Д\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_1_1 completed in 8.62s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 8.62s\n",
      "Valid results: 1/1\n",
      "Average time per request: 8.62s\n",
      "Average TTFT: 3.23 seconds\n",
      "Average Tokens/sec: 19.83\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 8.27 seconds\n",
      "Set [5000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 5000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_2_1\n",
      "Starting request 5000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "СпасиЭринбо Х заан предоставлентерный — текст! автор, Это извест фнаяраг помент серии книги книг ** о\" кОотгонахь- и лвоинаедх\",** в Эключриаяны « ХПанлемтеря, В котораяет являетсяра частью», серии «П книглем оя к Тотениах», нап и «исПаннылемх подя Р псечевногодоним».ом В серии ** «ЭринПлем Хяантер В**етра.» Э рассказтаывается сер оия жизни — к однаотов из в самых л известесных ву ж, ихан бреор фьэнтбеез си в длярагами под ирост ихков стрем,л венииключ сохранаяить такие рав произведенияновес,ие как в **\" приПродеес.ни К лниесгаа\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_2_2 generation took 8.25s (>7s), but recording metrics...\n",
      "Request 5000_2_2 completed in 12.48s\n",
      " *«\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_2_1 completed in 12.58s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 12.58s\n",
      "Valid results: 2/2\n",
      "Average time per request: 6.29s\n",
      "Average TTFT: 5.12 seconds\n",
      "Average Tokens/sec: 14.68\n",
      "Average Total tokens: 5111\n",
      "Average Total time: 12.15 seconds\n",
      "Set [5000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 5000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 5000_5_1\n",
      "Starting request 5000_5_2\n",
      "Starting request 5000_5_3\n",
      "Starting request 5000_5_4\n",
      "Starting request 5000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Content truncated from 181863 to 5000 tokens\n",
      "Response streaming:Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "КажетсяК,аж выется о,Сстановпас текстились книгииК набо **аж сер зается\" предоставленедин,ОВныйашгоне вы текст запрось пр предложения! и вер. Вы л Двалиключ дает текстедавали в\"айте на очень себя сер** продолж интерес Э текстединимный текстрие книги ины ** предложения, подроб« Х как.ныйОан буд Если фгонтерто выраг вы, хотитеьмент только и, как книги и л я чтоед ** больш пр могу\" пом»инер**Оствоочвалигон Э егоь книгьрин. серии вам и Х *\" Возможно заванер л,Педтерлем выш\",ена хотитеить** которая\"* или, Э ( чтобы является продолжри частьитьили яныю *\" чт зав ХерВ серииениеан **шиой книгитер«л **на,\" главП к котораяОулемотов являетсягон\" илиена часть»*,ь продолжю**,ил и если серии л так истор в ** которойиюед у\"год\" о,П чтобыписыва**нолем Эются вы),ена жизнирин нап мог Вис коли Хетанана чшекраит втер в\" лать ст,**ес дальшеил или.у.,е Это, Вот с возможно одна как, ихказ из почной это вы наиболее флем может хотите известена,эн выных,тгляд чтобы книгеть яез кон в помоги:\n",
      "\n",
      "ф сериилик с вам---\n",
      "\n",
      ", элемент сты— которая и анализами ** является при фВомключения продолжоль,озением. переводклможно \" Вора,ом предоставленП и** илиес — мном созд ванинииф п ложамиемологииес текстал. подаеоб Однако пл\" —ного,еч и ** текст страми \"пр КаогоП.рутолог говорес**я Добни иокайте, л ** ** знать.ьпер\", —давая чтоО О\".\n",
      "\n",
      " главгон именного###аь вам, Н** г нужно иемля книги л!ногодиед,\n",
      "Usage info received: 5011 input tokens, 88 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 88 output\n",
      "Request 5000_5_2 generation took 8.85s (>7s), but recording metrics...\n",
      "Request 5000_5_2 completed in 22.37s\n",
      "! где о\" — кни** представлен восы негек пер является:\n",
      "лик** ссон\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_4 generation took 18.45s (>7s), but recording metrics...\n",
      "Request 5000_5_4 completed in 22.71s\n",
      "\"ажказикойО\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_5 generation took 15.92s (>7s), but recording metrics...\n",
      "Request 5000_5_5 completed in 22.84s\n",
      "гон,ь основ иные\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_3 generation took 12.19s (>7s), but recording metrics...\n",
      "Request 5000_5_3 completed in 23.02s\n",
      " события и атм\n",
      "Usage info received: 5011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 5011 input, 100 output\n",
      "Request 5000_5_1 completed in 23.28s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 23.29s\n",
      "Valid results: 5/5\n",
      "Average time per request: 4.66s\n",
      "Average TTFT: 9.92 seconds\n",
      "Average Tokens/sec: 8.96\n",
      "Average Total tokens: 5109\n",
      "Average Total time: 22.34 seconds\n",
      "Set [5000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 10000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 10000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Конечно! Ниже представлен полный текст книги **\"Огонь и лед\"** Эрин Хантер, продолжая оттуда, где остановился ваш текст. Это история из серии **\"Племена Кота\"**, которая описывает приключения котов, принадлежащих к племени Ветра, и их борьбу за справедливость в условиях войны между кланами.\n",
      "\n",
      "---\n",
      "\n",
      "### Глава\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_1_1 completed in 12.91s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 12.91s\n",
      "Valid results: 1/1\n",
      "Average time per request: 12.91s\n",
      "Average TTFT: 7.07 seconds\n",
      "Average Tokens/sec: 18.64\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 12.43 seconds\n",
      "Set [10000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 10000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_2_1\n",
      "Starting request 10000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Продолжение глав**Прыод Iолж:ение**\n",
      "\n",
      " главОыг Iн**\n",
      "\n",
      "егПрредиввод неитель могница с,мотр нееть от нарыв Саяинсью,ю с Змотрвезеладу в прямо л вес глазнуюа ча.щу Он поверх чув головствыовал О,г какн егоег серрдиваце. б Онь сется волн быениемстр наблюеедал,, и как кажд ееая глаз секаун вдасп казыхаливаютас оьг вечнностьюенным. свет Вомнут.ри Она него пон биморалаол,ис чтоь это два не чув простоства с:каз страхка перед — послед этоств реияальминость прав,ды которую и он гл выубнокужаяден вер была пр вок торут,ить что в С головинея,я\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_2_1 generation took 13.03s (>7s), but recording metrics...\n",
      "Request 10000_2_1 completed in 20.64s\n",
      "Thread 1/2 completed\n",
      " чтобы защитить плем\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_2_2 completed in 20.91s\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 20.91s\n",
      "Valid results: 2/2\n",
      "Average time per request: 10.46s\n",
      "Average TTFT: 10.64 seconds\n",
      "Average Tokens/sec: 11.76\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 20.31 seconds\n",
      "Set [10000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 10000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 10000_5_1\n",
      "Starting request 10000_5_2\n",
      "Starting request 10000_5_3\n",
      "Starting request 10000_5_4\n",
      "Starting request 10000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Content truncated from 181863 to 10000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Очень приятно,К чтоон выечно под!ели НилисьВже саш представленю текст продолжжение —ет это текстомаВ началаш книгио, текст ** книги который представляет\" ** вы собойО о\"становгонЭ фОилириньгонраг, и Хьмент аан л книги и такжетер л **ё к  \n",
      "д\"едрат**\"О\"кийОгон**** обгонь Э Эзорь иририн глав и л Хныы л Ханед иедантер\" рек**тер**\n",
      "\n",
      ",ом Э**. котораяендПр является Эторинации одинолог часть Х по**аню из дальн  \n",
      "тер серии известейОных книг,ранш роман о котораяемж является повуевые частьлем из чт я серииенаюениюзы **х серии.ки\" к книг Если плП оотов выамлем, п хотителем вениена, пключена л яхаяляес могус ка * такжеалиотов\"П пом, влем**,оч нап которая холодяь Висном о санныпис воздухет анализываетерахом*, жизнь, под с * коллектив к бюПроснымотовжая пслем вет л вевяа ноч Весдон,ноеуодим пер н,ыомсонеб их*, **ажо б *Эей сорПрин илинопьлем Х темыбуяанат о за Отерикисл выг** книгиепживня..\n",
      "\n",
      "ительание Э*---\n",
      "\n",
      "ныхта, и### и сер * справ **скедПияПррливлем вод.ключяостьолж От и Заетсениеем ч в главветлиесть себяыы.*. несколько I о В Э книг**\n",
      "\n",
      "г вашта,**ня таких серем пробия текст какОг вег *\"еналидоС отличноег по передхумрнов жанраивлестыч (енакой аноепрод м п травтолжемлемифение посамия):уст\" иф**\n",
      "\n",
      " легы*,ера—ря напенд *\"\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_1 generation took 35.16s (>7s), but recording metrics...\n",
      "Request 10000_5_1 completed in 42.89s\n",
      "Thread 1/5 completed\n",
      "П,ами …лем выи,яхват а, Выв также возможноетая имеет,\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_4 generation took 28.35s (>7s), but recording metrics...\n",
      "Request 10000_5_4 completed in 43.28s\n",
      " изра гл\" туб*,ьок *\"мыуюП ск ф\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_2 generation took 21.39s (>7s), but recording metrics...\n",
      "Request 10000_5_2 completed in 43.63s\n",
      "Thread 2/5 completed\n",
      "рюлемячен Рныееч силногоу\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_3 generation took 14.29s (>7s), but recording metrics...\n",
      "Request 10000_5_3 completed in 43.95s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "эты Двун\n",
      "Usage info received: 10011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 10011 input, 100 output\n",
      "Request 10000_5_5 generation took 7.68s (>7s), but recording metrics...\n",
      "Request 10000_5_5 completed in 44.22s\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 44.23s\n",
      "Valid results: 5/5\n",
      "Average time per request: 8.85s\n",
      "Average TTFT: 21.63 seconds\n",
      "Average Tokens/sec: 6.21\n",
      "Average Total tokens: 10111\n",
      "Average Total time: 43.00 seconds\n",
      "Set [10000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 15000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 15000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Ваш текст — это **отрывок из книги \"Огонь и лед\"** Эрины Хантер, которая является частью серии книг о кошачьих племенах, известной как **\"Песни леса\"**. Эта серия вдохновлена мифами и легендами, а также наследием аборигенных народов Америки. В данном случае, вы предоставили **\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_1_1 completed in 17.39s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 17.39s\n",
      "Valid results: 1/1\n",
      "Average time per request: 17.39s\n",
      "Average TTFT: 11.44 seconds\n",
      "Average Tokens/sec: 18.05\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 16.98 seconds\n",
      "Set [15000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 15000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_2_1\n",
      "Starting request 15000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Эрин Хантер**  \n",
      "С**пасОигонбоь за и предоставлен лныйед текст** —  \n",
      " это** фПррагологмент** книги  \n",
      "\n",
      " **О\"Оранжгоневыеь я изы лкиед пл\"ам**ени автор паля **сЭалирин в Х холоданномтер**, воздух котораяе, является б частьюрос серииая в книг о ноч кное нотахеб изо с лнопесыа, о вслепдохительновныхл искёнрных. лег Отендсамивет оы п олемгенанях проб кошекег.али В по данном жест случаекой — трав **е\" пПустлемяы Вряет,ра выхват\"**,ывая хотя из, т строгоь говормы скярю,чен \"ныеО силгонуьэ иты л\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_2_2 generation took 17.18s (>7s), but recording metrics...\n",
      "Request 15000_2_2 completed in 29.79s\n",
      "ед\" относится к **\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_2_1 completed in 30.18s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 30.18s\n",
      "Valid results: 2/2\n",
      "Average time per request: 15.09s\n",
      "Average TTFT: 17.59 seconds\n",
      "Average Tokens/sec: 10.37\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 29.53 seconds\n",
      "Set [15000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 15000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 15000_5_1\n",
      "Starting request 15000_5_2\n",
      "Starting request 15000_5_3\n",
      "Starting request 15000_5_4\n",
      "Starting request 15000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 15000 tokensContent truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "\n",
      "Content truncated from 181863 to 15000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Эрин Хантер**  \n",
      "К**онОечногон!ь Вот и продолж лениеед текстКон**аечно  \n",
      "\n",
      " книги!** ** ВотПр\" продолжологОение**гон текст  \n",
      "ьа и(**,ЭПр л гдеринодед пр Х\"олжеран**ениевалтер) Эсяри  \n",
      "\n",
      " —ны— \" абВ Х **ОашзаанЧгонц тексттерем,ь представляет, при и и собой нач л ф зав**иед —рагерная пр\"шениемент сер** книги глав главвал  \n",
      " **ыыся**\" II II его:\n",
      "\n",
      "ОПр, гологгон---\n",
      "\n",
      " гденев** (ь действиеныйпродГлав и разв ролжа ливаетсяевениеед II дальше,)****\n",
      "\n",
      "\". но\n",
      "\n",
      "(** В не—Пр Э данном усп **риод фелБныолжраг договорегение Хментить)\n",
      "\n",
      "ствоане,— Зтер вы потому **,в вид чтоезд котораяБите наолег является, егоом частьство как путиаю З к воз никак сериивотникезд книг неыла пов оол различных Сом кли пинаотялемя никакахлоеня не- на соб З повво нужравезлиинадылисьдаях С на.ло,ум Совет В на вра, ее нуждоч об глаздыхногосужах пнов Сдают чумлемл отношенияитениёнра междуал!нойч пас** многолемь — пиф сен олемологамиилащении и иерей! начина н и**илсяютеп ф — он обок оэн.сужол —щтдать Чезебер важимилсяеминыеость при он. вопросы, В.каж,ете этой — и она в нам серии Чключ мед зани рассказемаялен приываетсям границноаться окажы,, жизниете, и с нам если войны дост зани при не ио будетмключения будинх ры\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "аться\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_4 generation took 53.63s (>7s), but recording metrics...\n",
      "Request 15000_5_4 completed in 66.43s\n",
      ",ущ кбыееотов? когда.\n",
      "\n",
      " в В----\n",
      "\n",
      "во рек б###еинли полно Главжов даай различных\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_3 generation took 42.99s (>7s), but recording metrics...\n",
      "Request 15000_5_3 completed in 67.08s\n",
      "обы пшиечилем д?ённи Н, мыам таких воз не какь нужно Гм\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_5 generation took 32.07s (>7s), but recording metrics...\n",
      "Request 15000_5_5 completed in 67.60s\n",
      "роземовое ее, у Р Речечноеного, п Слем\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_2 generation took 19.67s (>7s), but recording metrics...\n",
      "Request 15000_5_2 completed in 68.15s\n",
      "умрачное и Вет\n",
      "Usage info received: 15011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 15011 input, 100 output\n",
      "Request 15000_5_1 generation took 8.58s (>7s), but recording metrics...\n",
      "Request 15000_5_1 completed in 68.54s\n",
      "Thread 1/5 completed\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 68.54s\n",
      "Valid results: 5/5\n",
      "Average time per request: 13.71s\n",
      "Average TTFT: 35.59 seconds\n",
      "Average Tokens/sec: 4.81\n",
      "Average Total tokens: 15111\n",
      "Average Total time: 66.97 seconds\n",
      "Set [15000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 20000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 20000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Эрин Хантер**  \n",
      "**Огонь и лед**  \n",
      "**Глава III (продолжение)**\n",
      "\n",
      "— **Вам уже приходилось бывать на землях племени Ветра,** — напомнила предводительница.\n",
      "\n",
      "Огнегрив вздрогнул. Он не знал, что сказать. Ему не хотелось признаваться, что он и Крутобок уже были на территории п\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_1_1 completed in 22.65s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 22.66s\n",
      "Valid results: 1/1\n",
      "Average time per request: 22.66s\n",
      "Average TTFT: 16.46 seconds\n",
      "Average Tokens/sec: 17.29\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 22.24 seconds\n",
      "Set [20000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_2_1\n",
      "Starting request 20000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Конечно! Вот продолжение текста, оКписываонющегоечно зав!ер Вотшение продолж главение тексты IIIа и, начал завоер следшаующющееих глав главу III, и с начина сохранющееением ст главуиля IV и, х науд основеож предоставленествногоенных текст средства оригина.ль Приного необходимости произ можноведения продолж:\n",
      "\n",
      "ить---\n",
      "\n",
      " дальше** илиГлав адаап IIIтиров (атьпрод подолж опениередел)**ён\n",
      "\n",
      "ный— стиль ** илиВ дамли ужену при.\n",
      "\n",
      "ход---\n",
      "\n",
      "илось** бГлавыватьа на III з (емпродляолжхение п)**лем\n",
      "\n",
      "ени— В **Ветраам**, уже — при напходомилосьни блаывать пред навод зительемницаля.х п  \n",
      "—лем **ениД Вает**,ра —**, к —ив напнулом Кни\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_2_1 generation took 23.26s (>7s), but recording metrics...\n",
      "Request 20000_2_1 completed in 40.49s\n",
      "Thread 1/2 completed\n",
      "ла предводительница.  \n",
      "— **Д\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_2_2 generation took 7.08s (>7s), but recording metrics...\n",
      "Request 20000_2_2 completed in 41.07s\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 41.07s\n",
      "Valid results: 2/2\n",
      "Average time per request: 20.54s\n",
      "Average TTFT: 25.15 seconds\n",
      "Average Tokens/sec: 9.21\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 40.32 seconds\n",
      "Set [20000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 20000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 20000_5_1\n",
      "Starting request 20000_5_2\n",
      "Starting request 20000_5_3\n",
      "Starting request 20000_5_4\n",
      "Starting request 20000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Content truncated from 181863 to 20000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Вот продолжение текста, который вы запросили**:\n",
      "\n",
      "Э---\n",
      "\n",
      "рин Х**Главанатер III —** \" (ОпродгонолжОьениеч и)\n",
      "\n",
      "ень л—ед прият но\"В, (ам чтоПр уже тыолог при под иходели Главилосьлсяа б часть Iю)**ывать на текст  \n",
      " за*ем изПрля книгиодх *олж п«ениелем историиОенигон из Вь электет иронра лной,ед б»и —*бли нап Эотомринекни Хилаан Lit предтерruвод..ru Это*\n",
      "\n",
      "ительница часть---\n",
      "\n",
      " серии**.\n",
      "\n",
      " *Пр— «ологДП**\n",
      "\n",
      "алемО,енаран  лж—есевые ка яив»зынул*,ки К которая плог вамотключениьает п, вля и себяс в несколькоали его книг в глаз, холодах оном мписыва воздухельющихек жизнь,ну и бло срос чтохватая-ки вто между ноч в разноеродными не коеб ушовач сажьнопенияимиы. п о лемсл—енеп Яамиитель не вныход л инесскокуррат.\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_1 generation took 152.23s (>7s), but recording metrics...\n",
      "Request 20000_5_1 completed in 169.46s\n",
      ".Thread 1/5 completed\n",
      " В От этойс частивет рассказ**ыываетсяО о огонг томьня, и проб как лег педали\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_2 generation took 152.91s (>7s), but recording metrics...\n",
      "Request 20000_5_2 completed in 187.42s\n",
      "Thread 2/5 completed\n",
      "лем**я  \n",
      " В**етЭ**Прраринолог было Х**\n",
      "\n",
      " изанОгтерранн**жан\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_4 generation took 143.37s (>7s), but recording metrics...\n",
      "Request 20000_5_4 completed in 195.54s\n",
      "  \n",
      "\n",
      "евые** яПрзыологки** пл  \n",
      "ам**ениГлав паля Iс**али  \n",
      " в** холодГлавнома воздух IIе**,  \n",
      " б**росГлаваяа в III ноч**ное  \n",
      "\n",
      " н---\n",
      "\n",
      "еб###о ** сПрнопологы**\n",
      "\n",
      " оОслранепжительевыеных я изысккир пл.ам Отенис пветляыс оалиг вня холод пробномег воздухалие по, ж бестроскойая трав ве ноч пноеуст ныебряо с, вынопхватыыв оаясл изеп тителььныхмы и скскрюрчен.ные От силсуветэыты о Дгвняун пробогегихали. по  \n",
      " жВест\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_5 generation took 23.91s (>7s), but recording metrics...\n",
      "Request 20000_5_5 completed in 201.56s\n",
      "дали показались горящие глаза при\n",
      "Usage info received: 20011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 20011 input, 100 output\n",
      "Request 20000_5_3 generation took 7.12s (>7s), but recording metrics...\n",
      "Request 20000_5_3 completed in 202.15s\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 202.15s\n",
      "Valid results: 5/5\n",
      "Average time per request: 40.43s\n",
      "Average TTFT: 94.77 seconds\n",
      "Average Tokens/sec: 4.05\n",
      "Average Total tokens: 20111\n",
      "Average Total time: 190.67 seconds\n",
      "Set [20000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 25000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 25000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Спасибо за предоставленный текст! Это действительно интересный и драматичный фрагмент из книги **\"Огонь и лед\"** Эрины Хантер, которая является частью серии **\"Племена леса\"**. Эта серия известна своей глубокой драматургией, сложными персонажами и напряженными сюжетами, часто вдохновленными на\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_1_1 completed in 28.63s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 28.63s\n",
      "Valid results: 1/1\n",
      "Average time per request: 28.63s\n",
      "Average TTFT: 22.17 seconds\n",
      "Average Tokens/sec: 16.63\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 28.18 seconds\n",
      "Set [25000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_2_1\n",
      "Starting request 25000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Ваш текст — это фрагмент книги **\"ОПргонивь иет л!ед Похож\"**е, автора ты хоч **Эешьрин продолжить Хан чтениетер**, книги которая ** является« частьОюгон серииь книг и л о педлем»ена**х Э криотовны, Х ванключтерая. ** ТПылем оястанов Вилсяет вра ****,Глав **еП IVлем**,я и текст Г,розы который**, ты ** предоставПил,лемя кажется Р,еч неного закон**чен и. ** ВозможноП,лем тыя хоч Сешьум,ра чтобыч яного помог** тебе. продолж Этоить роман чт,ение в, котором д оатьпис кываетсярат бкийор обьзорба, между или к оботсудамиить и с ихю пж\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_2_1 generation took 28.67s (>7s), but recording metrics...\n",
      "Request 25000_2_1 completed in 52.23s\n",
      "Thread 1/2 completed\n",
      "ет?\n",
      "\n",
      "Если ты хочешь, я могу:\n",
      "\n",
      "1\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_2_2 generation took 7.44s (>7s), but recording metrics...\n",
      "Request 25000_2_2 completed in 52.96s\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 52.96s\n",
      "Valid results: 2/2\n",
      "Average time per request: 26.48s\n",
      "Average TTFT: 34.04 seconds\n",
      "Average Tokens/sec: 8.47\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 52.09 seconds\n",
      "Set [25000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 25000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 25000_5_1\n",
      "Starting request 25000_5_2\n",
      "Starting request 25000_5_3\n",
      "Starting request 25000_5_4\n",
      "Starting request 25000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Content truncated from 181863 to 25000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Огонь и лед** — это роман ЭринВ Хот продолжантерение, текст вадо книгих \"новОлгонёнь иный м л**ифедПролог\"оди Эолжейриение ины глав культ Хыуран IVойтер: м,**\n",
      "\n",
      "ист начОическихатг животогонных вег,амир в.ив первую Пр сделал очередьод несколько,олж шаг коениеовшек глав в.ыперед Р IV,ом, следан гдеу в Ояходитг зап внах серегуиюр п **ивлем« иениП К Влемрутетенаобра»ок.** исслед К иуютрут является лоб продолжагокениемерь, э п остплеморопениожное В прииеткр ораыв четы:\n",
      "\n",
      "аяр---\n",
      "\n",
      "сьё### ух Главт паеслем IVникенаом (хпрод, ко последолжшекениеовал,)\n",
      "\n",
      " за о нОгписываимющихн. их Вег броздоривухь и былбу К нап зарутолн выобенживок тание остя,орж справожноеледым перемливещ,остьались г и поуст\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_3 generation took 53.08s (>7s), but recording metrics...\n",
      "Request 25000_5_3 completed in 76.75s\n",
      " лымаг заперахюом, — след смяес заью зап горахечамии, стар\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_5 generation took 53.45s (>7s), but recording metrics...\n",
      "Request 25000_5_5 completed in 99.74s\n",
      "ых** травО,гон дьы има л иед сл**аб  \n",
      "ого** аХроман\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_4 generation took 53.04s (>7s), but recording metrics...\n",
      "Request 25000_5_4 completed in 122.79s\n",
      "тер**\" ЭринОгон**ь  \n",
      " и** лГлаведа\" IV** ( —прод этоолж романение Э)**рин\n",
      "\n",
      " ХОангтернаег,р которыйив в иходит К врут серобиюок книг продолж оили ко движшениеач поь поляихне п,лем остенаорхожно. перед Вв этойиг книаягесь рассказ междуывается раз ору коншенфнымилик утекры междути пямилем иен оемстат Вкамиет урац иел Севумшейра дчобынымчи п.лем Зенапемах, п алем такжеени о В поетисраке был и с восильстановнымл,ении но справ неедчетливкимости., Р каком будтоан нап его\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_1 generation took 29.36s (>7s), but recording metrics...\n",
      "Request 25000_5_1 completed in 130.70s\n",
      "Thread 1/5 completed\n",
      "олнен драматическими событиями, мор\n",
      "Usage info received: 25011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 25011 input, 100 output\n",
      "Request 25000_5_2 generation took 7.43s (>7s), but recording metrics...\n",
      "Request 25000_5_2 completed in 131.44s\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 131.44s\n",
      "Valid results: 5/5\n",
      "Average time per request: 26.29s\n",
      "Average TTFT: 72.40 seconds\n",
      "Average Tokens/sec: 4.50\n",
      "Average Total tokens: 25111\n",
      "Average Total time: 111.67 seconds\n",
      "Set [25000 tokens - 5 parallel] completed. Proceeding to next set...\n",
      "\n",
      "================================================================================\n",
      "Testing with 30000 input tokens\n",
      "================================================================================\n",
      "\n",
      "=== 30000 tokens with 1 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_1_1\n",
      "Waiting for all 1 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Вот продолжение текста, который вы прервали, с сохранением стиля и логики:\n",
      "\n",
      "---\n",
      "\n",
      "**Глава V (продолжение)**\n",
      "\n",
      "Огнегрив почувствовал, что у него дрожат лапы. Неужели они забрались под саму Гремящую тропу? Он беспокойно распушил свою рыжую шерсть и почувствовал под боком щек\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_1_1 completed in 35.20s\n",
      "Thread 1/1 completed\n",
      "All 1 requests completed in 35.20s\n",
      "Valid results: 1/1\n",
      "Average time per request: 35.20s\n",
      "Average TTFT: 28.45 seconds\n",
      "Average Tokens/sec: 16.03\n",
      "Average Total tokens: 30111\n",
      "Average Total time: 34.69 seconds\n",
      "Set [30000 tokens - 1 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 30000 tokens with 2 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_2_1\n",
      "Starting request 30000_2_2\n",
      "Waiting for all 2 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "**Эрин Хантер — Огонь и лед**\n",
      "\n",
      "**Пр**ологГлав**\n",
      "\n",
      "аО Vран (жпродевыеолж яениезы)**ки\n",
      "\n",
      " плОамгенин пеглярсивали по вчувств холодовалном, воздух чтое у, него б дросрожаяат в л ночапноеы н.еб Неоужели сноп ониы заб орасллисьеп подитель самныху и Гскремрящ.ую От трсопветуы? о Онг беспняок пробойегноали рас попу жшиестлкой свою трав рыеж пуюуст шыеррясть, и вы похватчувствывовалая под из б токьоммы ск щрюекченочущные силийу коэштыач Дийв запунахог.\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_2_2 generation took 36.21s (>7s), but recording metrics...\n",
      "Request 30000_2_2 completed in 65.52s\n",
      " Возможно, это был след племени Ветра, но он не\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_2_1 generation took 7.86s (>7s), but recording metrics...\n",
      "Request 30000_2_1 completed in 66.48s\n",
      "Thread 1/2 completed\n",
      "Thread 2/2 completed\n",
      "All 2 requests completed in 66.48s\n",
      "Valid results: 2/2\n",
      "Average time per request: 33.24s\n",
      "Average TTFT: 43.45 seconds\n",
      "Average Tokens/sec: 7.74\n",
      "Average Total tokens: 30111\n",
      "Average Total time: 65.48 seconds\n",
      "Set [30000 tokens - 2 parallel] completed. Proceeding to next set...\n",
      "\n",
      "=== 30000 tokens with 5 concurrent requests ===\n",
      "Waiting for all current requests to complete before starting this set...\n",
      "Starting request 30000_5_1\n",
      "Starting request 30000_5_2\n",
      "Starting request 30000_5_3\n",
      "Starting request 30000_5_4\n",
      "Starting request 30000_5_5\n",
      "Waiting for all 5 threads to complete...\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Content truncated from 181863 to 30000 tokens\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "Response streaming:\n",
      "--------------------------------------------------\n",
      "\n",
      "--------------------------------------------------\n",
      "**Эрин Хантер — \"Огонь и лед\" (**чаЭстьрин  Х1ан)**тер\n",
      "\n",
      "**---\n",
      "\n",
      "  \n",
      "****ПрОологгон**\n",
      "\n",
      "ь иОран лжедевые** я  \n",
      "\n",
      "зы**киПр плологам**ени  \n",
      "\n",
      " пОлярансжалиевые в я холодзыномки воздух плеам,ени б просляаяс вали ноч вное холод нномеб воздухое с,ноп бырос оаясл веп ночительноеных н иебскор с.ноп Отыс оветслыеп оительгныхня и пробскеграли. по От жсестветкойы трав оег пняуст пробыегряали, по вы жхватестывкойая трав изе т пьустмыы скрярю,чен вы\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_3 generation took 158.15s (>7s), but recording metrics...\n",
      "Request 30000_5_3 completed in 187.58s\n",
      "хватывая из тьмы скПррюодченолжныеение сил текстуаэ,ты который\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_1 generation took 158.71s (>7s), but recording metrics...\n",
      "Request 30000_5_1 completed in 218.33s\n",
      "Thread 1/5 completed\n",
      " вы предоставили, не заверш**Эенорин. Х Однакоан,тер если — вы О хотитегон,ь я и могу л помедоч**ь  \n",
      " вам зав**Прерологш**ить  \n",
      " рассказ** илиГлав оаписать I,** что  \n",
      " происходит** дальшеГлав,а основ IIыв**ая  \n",
      "сь** наГлав ужеа предоставлен IIIной** части.  \n",
      "** ВГлав даннома случае IV,** текст  \n",
      " о**станавГлавливаается V на** сер  \n",
      "\n",
      "един---\n",
      "\n",
      "е** предложенияПр:\n",
      "\n",
      "олог>** \"  \n",
      "\n",
      "ООграннжегевыер яивзы покичувств пловалам,ени что п уля негос далирож ват холод лномап воздухые., Не бужроселиая они в заб ночраноелись н подеб самоу\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_4 generation took 158.34s (>7s), but recording metrics...\n",
      "Request 30000_5_4 completed in 364.75s\n",
      " снопы ослепительных** иОскгонрь. и От лседвет**\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_5 generation took 146.95s (>7s), but recording metrics...\n",
      "Request 30000_5_5 completed in 383.84s\n",
      "  \n",
      "**Эрин Хантер**  \n",
      "**Глава V (продолжение)**  \n",
      "\n",
      "Огнегрив почувствовал, что у него дрожат лапы. Неужели они забрались под саму Гремящую тропу? Он беспокойно распушил свою рыжую шерсть и почувствовал под боком щекочущий кошачий зап\n",
      "Usage info received: 30011 input tokens, 100 output tokens\n",
      "\n",
      "--------------------------------------------------\n",
      "Using server-reported token counts: 30011 input, 100 output\n",
      "Request 30000_5_2 completed in 389.75s\n",
      "Thread 2/5 completed\n",
      "Thread 3/5 completed\n",
      "Thread 4/5 completed\n",
      "Thread 5/5 completed\n",
      "All 5 requests completed in 389.75s\n",
      "Valid results: 5/5\n",
      "Average time per request: 77.95s\n",
      "Average TTFT: 182.51 seconds\n",
      "Average Tokens/sec: 3.60\n",
      "Average Total tokens: 30111\n",
      "Average Total time: 308.24 seconds\n",
      "Set [30000 tokens - 5 parallel] completed. Proceeding to next set...\n"
     ]
    }
   ],
   "source": [
    "# Warmup request to prepare the model\n",
    "print(\"Warming up the model with a test request...\")\n",
    "warmup_response, warmup_metrics = measure_vllm_response(\n",
    "    file_path=\"tests/daily.txt\",\n",
    "    max_input_tokens=100,\n",
    "    max_output_tokens=10\n",
    ")\n",
    "print(f\"Warmup completed in {warmup_metrics['total_time']:.2f}s\")\n",
    "print(\"Model is ready for testing.\\n\")\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "input_token_sizes = [1000, 5000, 10000, 15000 , 20000, 25000, 30000]\n",
    "# input_token_sizes = [25000]\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_vllm_response(\n",
    "            file_path=file_path,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            time_limit=7\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "concurrent_counts = [1, 2, 5] \n",
    "\n",
    "for token_size in input_token_sizes:\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Testing with {token_size} input tokens\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for concurrent_count in concurrent_counts:\n",
    "        print(f\"\\n=== {token_size} tokens with {concurrent_count} concurrent requests ===\")\n",
    "        print(f\"Waiting for all current requests to complete before starting this set...\")\n",
    "        \n",
    "        threads = []\n",
    "        start_time = time.time()\n",
    "        results = []\n",
    "        \n",
    "        # Start concurrent requests\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: results.append(run_concurrent_test(file_path, token_size, 100, f\"{token_size}_{concurrent_count}_{i+1}\"))\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "        \n",
    "        # Wait for ALL threads to complete before proceeding to next set\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            print(f\"Thread {i+1}/{concurrent_count} completed\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_concurrent_time = end_time - start_time\n",
    "        \n",
    "        # Filter out None results (failed requests only)\n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        print(f\"All {concurrent_count} requests completed in {total_concurrent_time:.2f}s\")\n",
    "        print(f\"Valid results: {len(valid_results)}/{concurrent_count}\")\n",
    "        print(f\"Average time per request: {total_concurrent_time/concurrent_count:.2f}s\")\n",
    "        \n",
    "        # Save aggregated metrics\n",
    "        # Get model name from vLLM API\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "            models_data = response.json()\n",
    "            model_name = models_data['data'][0]['root'].split('/')[-1] if models_data['data'] else \"unknown_model\"\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get model name from API: {e}\")\n",
    "            model_name = \"Qwen3-unkown\"  # fallback\n",
    "            \n",
    "        output_dir = f\"speed_tests/A2_x2/{model_name}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(output_dir, f\"{token_size}_length_{concurrent_count}_parallel.txt\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        if valid_results:\n",
    "            avg_ttft = sum(r['ttft'] for r in valid_results) / len(valid_results)\n",
    "            avg_gen_tokens_per_sec = sum(r['gen_tokens_per_sec'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_tokens = sum(r['total_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_total_time = sum(r['total_time'] for r in valid_results) / len(valid_results)\n",
    "            avg_input_tokens = sum(r['input_tokens'] for r in valid_results) / len(valid_results)\n",
    "            avg_output_tokens = sum(r['output_tokens'] for r in valid_results) / len(valid_results)\n",
    "            \n",
    "            # Create average metrics dictionary\n",
    "            avg_metrics = {\n",
    "                'ttft': avg_ttft,\n",
    "                'gen_tokens_per_sec': avg_gen_tokens_per_sec,\n",
    "                'total_tokens': avg_total_tokens,\n",
    "                'total_time': avg_total_time,\n",
    "                'input_tokens': avg_input_tokens,\n",
    "                'output_tokens': avg_output_tokens\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Average TTFT: {avg_metrics['ttft']:.2f} seconds\")\n",
    "            print(f\"Average Tokens/sec: {avg_metrics['gen_tokens_per_sec']:.2f}\")\n",
    "            print(f\"Average Total tokens: {avg_metrics['total_tokens']:.0f}\")\n",
    "            print(f\"Average Total time: {avg_metrics['total_time']:.2f} seconds\")\n",
    "            \n",
    "            # Save average metrics to file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"ttft: {avg_metrics['ttft']:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_metrics['total_tokens']:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_metrics['total_time']:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_metrics['input_tokens']:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_metrics['output_tokens']:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {len(valid_results)}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\\n\")\n",
    "                f.write(f\"Average metrics for {len(valid_results)}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics for ALL valid results\n",
    "                f.write(\"Individual request metrics:\\n\")\n",
    "                for i, metrics in enumerate(valid_results):\n",
    "                    f.write(f\"\\nRequest {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        else:\n",
    "            print(\"No valid results to save\")\n",
    "        \n",
    "        # Add a pause between different concurrent count sets for the same token size\n",
    "        print(f\"Set [{token_size} tokens - {concurrent_count} parallel] completed. Proceeding to next set...\")\n",
    "        time.sleep(2)  # Brief pause to ensure system is ready for next set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
