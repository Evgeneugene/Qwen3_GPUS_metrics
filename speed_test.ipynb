{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19185de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Qwen3_GPUS_metrics/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Qwen3 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "963f2af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-4B'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "response.json()['data'][0]['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d913082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/Qwen3_GPUS_metrics'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85baade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different input token sizes with various concurrent requests:\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 1000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "Конечно! Вот продолжение вашего текста, сохраняющее стиль и тон оригинала, а также развивая сюжет:\n",
      "\n",
      "---\n",
      "\n",
      "**Пролог (продолжение)**\n",
      "\n",
      "— Мы не сумели защитить своих детей от Сумрачного племени, — проговорил черный кот, его голос звучал тяжело, но с силой. — Мы были слишком слабы. И теперь, может быть, этоStart time: 2025-09-24 06:06:50\n",
      "End time: 2025-09-24 06:06:54\n",
      "Tokens generated: 100\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 1000 tokens:\n",
      "Average TTFT: 0.56s\n",
      "Average gen tokens/sec: 26.02\n",
      "Average total time: 4.40s\n",
      "Total concurrent time: 4.83s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/1000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 5000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "Конечно! Ниже приведён **незавершённый текст** из книги **«Огонь и лед»** Эрина Хантера, который вы предоставили. Если вы хотите, я могу:\n",
      "\n",
      "1. **Продолжить текст**, если у вас есть описания или планы по следующим главам.\n",
      "2. **Представить полную версию** книги, если вы хотите.\n",
      "3. **ПStart time: 2025-09-24 06:07:01\n",
      "End time: 2025-09-24 06:07:11\n",
      "Tokens generated: 100\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 5000 tokens:\n",
      "Average TTFT: 4.60s\n",
      "Average gen tokens/sec: 10.52\n",
      "Average total time: 14.11s\n",
      "Total concurrent time: 14.54s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/5000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 10000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "**Продолжение текста:**\n",
      "\n",
      "...Огнегрив с волнением наблюдал за тем, как Синяя Звезда смотрит в лесную чащу, словно ожидая, что что-то в ней вспыхнет\n",
      "Generation stopped early after 10.11 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:07:30\n",
      "End time: 2025-09-24 06:07:40\n",
      "Tokens generated: 60\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 10000 tokens:\n",
      "Average TTFT: 16.57s\n",
      "Average gen tokens/sec: 5.93\n",
      "Average total time: 26.69s\n",
      "Total concurrent time: 27.11s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/10000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 15000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "**Глава II (продолжение)**\n",
      "\n",
      "— ...Чем пришлось нам вести войну за землю, которую мы никогда не пользовались! — продолжал Черняк,\n",
      "Generation stopped early after 10.15 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:08:17\n",
      "End time: 2025-09-24 06:08:27\n",
      "Tokens generated: 43\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 15000 tokens:\n",
      "Average TTFT: 34.32s\n",
      "Average gen tokens/sec: 4.23\n",
      "Average total time: 44.47s\n",
      "Total concurrent time: 44.94s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/15000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 20000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "**Глава III (продолжение)**\n",
      "\n",
      "— Да, — кивнул Коготь, не повышая голоса, но его взгляд\n",
      "Generation stopped early after 10.02 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:09:28\n",
      "End time: 2025-09-24 06:09:38\n",
      "Tokens generated: 33\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 20000 tokens:\n",
      "Average TTFT: 58.69s\n",
      "Average gen tokens/sec: 3.29\n",
      "Average total time: 68.71s\n",
      "Total concurrent time: 69.19s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/20000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 25000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "**Глава IV (продолжение)**\n",
      "\n",
      "Огнегрив тщательно принюхался и сделал\n",
      "Generation stopped early after 10.01 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:11:10\n",
      "End time: 2025-09-24 06:11:20\n",
      "Tokens generated: 27\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 25000 tokens:\n",
      "Average TTFT: 89.61s\n",
      "Average gen tokens/sec: 2.70\n",
      "Average total time: 99.62s\n",
      "Total concurrent time: 100.07s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/25000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 1 concurrent requests with 30000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 1 threads to complete...\n",
      "**Огонь и лед**  \n",
      "**Эрин Хантер**  \n",
      "**Глава V (прод\n",
      "Generation stopped early after 10.16 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:13:32\n",
      "End time: 2025-09-24 06:13:42\n",
      "Tokens generated: 23\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 1 concurrent requests with 30000 tokens:\n",
      "Average TTFT: 129.26s\n",
      "Average gen tokens/sec: 2.26\n",
      "Average total time: 139.42s\n",
      "Total concurrent time: 139.88s\n",
      "Valid requests: 1/1\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/30000_length_1_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 2 concurrent requests with 1000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 2 threads to complete...\n",
      "ВотК продолжонениеечно текст!а Вот книги продолж **ение« текстОагон книгиь ** и\" лОгонед»ь** и Э лриедна\" Х**ан Этерриана, Х котороеан вытер предоставаили,. которое Если вы вы предостав хотитеили,. я Если могу вам пом нужнооч,ь я вам могу продолж помитьоч тексть, продолж сделатьить его текст более, подроб созднымать, ан илин дажеот помациюоч,ь или в даже оформ помлоченииь книги с в перевод формомат/еан электализрономной. б Такжеи могубли помоточекьи с. оформ Такжел могуением пом книгиоч вь форм сат анализеом, с подходюящжемет для электа,рон пернойсон бажиейбли илиот жеканира.\n",
      "\n",
      ".\n",
      "\n",
      "ЕЕслисли вы вы хотитеStart time: 2025-09-24 06:13:45\n",
      "End time: 2025-09-24 06:13:52\n",
      "Tokens generated: 100\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      ",Start time: 2025-09-24 06:13:46\n",
      "End time: 2025-09-24 06:13:52\n",
      "Tokens generated: 100\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 2 concurrent requests with 1000 tokens:\n",
      "Average TTFT: 1.09s\n",
      "Average gen tokens/sec: 15.11\n",
      "Average total time: 7.74s\n",
      "Total concurrent time: 8.24s\n",
      "Valid requests: 2/2\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/1000_length_2_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 2 concurrent requests with 5000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 2 threads to complete...\n",
      "КонечноК\n",
      "Generation stopped early after 11.11 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:14:03\n",
      "End time: 2025-09-24 06:14:14\n",
      "Tokens generated: 3\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "ажется, ты остановился на странице книги \"Огонь и лед\" автора Эрина Хантера, которая принадлежит к жанру сказок. Эта книга является частью серии \"Кошки-воины\", в которой персонажи — кошки — живут в лесу и сталкиваются с различными событиями, включая конфликты между племенами\n",
      "Generation stopped early after 10.06 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:14:14\n",
      "End time: 2025-09-24 06:14:24\n",
      "Tokens generated: 98\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 2 concurrent requests with 5000 tokens:\n",
      "Average TTFT: 13.60s\n",
      "Average gen tokens/sec: 5.01\n",
      "Average total time: 24.18s\n",
      "Total concurrent time: 29.69s\n",
      "Valid requests: 2/2\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/5000_length_2_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 2 concurrent requests with 10000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 2 threads to complete...\n",
      "Эрин Хан\n",
      "Generation stopped early after 19.25 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:14:53\n",
      "End time: 2025-09-24 06:15:13\n",
      "Tokens generated: 4\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "Вот продолжение текста книги \"Огонь и лед\" Эрина Хантера, начиная с того места, где текст был прерван:\n",
      "\n",
      "---\n",
      "\n",
      "**Предводительница, не отрываясь, смотрела в лесную чащу\n",
      "Generation stopped early after 10.01 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:15:31\n",
      "End time: 2025-09-24 06:15:41\n",
      "Tokens generated: 60\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 2 concurrent requests with 10000 tokens:\n",
      "Average TTFT: 45.51s\n",
      "Average gen tokens/sec: 3.10\n",
      "Average total time: 60.14s\n",
      "Total concurrent time: 74.59s\n",
      "Valid requests: 2/2\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/10000_length_2_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 2 concurrent requests with 15000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 2 threads to complete...\n",
      "**Глава\n",
      "Generation stopped early after 14.14 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:16:25\n",
      "End time: 2025-09-24 06:16:39\n",
      "Tokens generated: 3\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "Вот продолжение главы II, остановленной на моменте, где Черняк начинает говорить о том, как бегство Звездолома не повлияло\n",
      "Generation stopped early after 10.02 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:17:16\n",
      "End time: 2025-09-24 06:17:26\n",
      "Tokens generated: 42\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n",
      "\n",
      "Results for 2 concurrent requests with 15000 tokens:\n",
      "Average TTFT: 67.70s\n",
      "Average gen tokens/sec: 2.20\n",
      "Average total time: 79.78s\n",
      "Total concurrent time: 103.87s\n",
      "Valid requests: 2/2\n",
      "Results saved to: speed_tests/T4_x2/Qwen3-4B/15000_length_2_parallel.txt\n",
      "\n",
      "================================================================================\n",
      "Testing 2 concurrent requests with 20000 input tokens each\n",
      "================================================================================\n",
      "Waiting for all 2 threads to complete...\n",
      "**Глава\n",
      "Generation stopped early after 11.29 seconds (max: 10.0s)\n",
      "Start time: 2025-09-24 06:18:47\n",
      "End time: 2025-09-24 06:18:58\n",
      "Tokens generated: 3\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 303\u001b[39m\n\u001b[32m    301\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWaiting for all \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconcurrent_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m threads to complete...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, thread \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(threads):\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[43mthread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.environ.get(\u001b[33m'\u001b[39m\u001b[33mBENCHMARK_QUIET\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    305\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThread \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m joined\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1147\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1149\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:1167\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1168\u001b[39m         lock.release()\n\u001b[32m   1169\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def measure_vllm_response(file_path, vllm_url=\"http://localhost:8000/v1/chat/completions\", \n",
    "                         max_input_tokens=None, max_output_tokens=1024, max_generation_time=10.0, \n",
    "                         print_stream=False, benchmark_mode=False):\n",
    "    \"\"\"\n",
    "    Send file content to vLLM chat endpoint and measure response metrics with streaming\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the input file\n",
    "        vllm_url: vLLM endpoint URL\n",
    "        max_input_tokens: Maximum tokens for input (for truncation), None for no limit\n",
    "        max_output_tokens: Maximum tokens for output response\n",
    "        max_generation_time: Maximum time in seconds for generation after prefill (default: 10.0)\n",
    "        print_stream: Whether to print streaming content (default: False for benchmarking)\n",
    "        benchmark_mode: If True, optimize for precise TTFT measurement\n",
    "    \"\"\"\n",
    "    # Read the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Truncate content if max_input_tokens is specified\n",
    "    if max_input_tokens is not None:\n",
    "        tokens = tokenizer.encode(content)\n",
    "        if len(tokens) > max_input_tokens:\n",
    "            truncated_tokens = tokens[:max_input_tokens]\n",
    "            content = tokenizer.decode(truncated_tokens)\n",
    "            if not benchmark_mode:\n",
    "                print(f\"Content truncated from {len(tokens)} to {max_input_tokens} tokens\")\n",
    "    \n",
    "    # Get actual input token count\n",
    "    input_tokens = len(tokenizer.encode(content))\n",
    "    \n",
    "    # Prepare chat request with streaming\n",
    "    payload = {\n",
    "        \"model\": \"qwen3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"max_tokens\": max_output_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True,\n",
    "        \"stream_options\": {\"include_usage\": True},  # Request usage info in stream\n",
    "        \"chat_template_kwargs\": {\n",
    "                \"enable_thinking\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"text/event-stream\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "    \n",
    "    # Use session for better connection management\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    ttft = None\n",
    "    generation_start_time = None\n",
    "    generation_stopped_early = False\n",
    "    \n",
    "    # Send request to vLLM with streaming\n",
    "    response = session.post(vllm_url, json=payload, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        session.close()\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Process streaming response\n",
    "    full_response = \"\"\n",
    "    output_tokens = 0\n",
    "    actual_input_tokens = None\n",
    "    actual_output_tokens = None\n",
    "    \n",
    "    if print_stream and not benchmark_mode:\n",
    "        print(\"Response streaming:\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Use minimal buffering for faster first token detection\n",
    "        for line in response.iter_lines(chunk_size=1, decode_unicode=True):\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if line.startswith('data: '):\n",
    "                data_str = line[6:]  # Remove 'data: ' prefix\n",
    "                if data_str.strip() == '[DONE]':\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(data_str)\n",
    "                    \n",
    "                    # Check for usage information (comes in final event)\n",
    "                    if 'usage' in data:\n",
    "                        actual_input_tokens = data['usage']['prompt_tokens']\n",
    "                        actual_output_tokens = data['usage']['completion_tokens']\n",
    "                        if not benchmark_mode:\n",
    "                            print(f\"\\nUsage info received: {actual_input_tokens} input tokens, {actual_output_tokens} output tokens\")\n",
    "                    \n",
    "                    if 'choices' in data and len(data['choices']) > 0:\n",
    "                        choice = data['choices'][0]\n",
    "                        if 'delta' in choice and 'content' in choice['delta']:\n",
    "                            current_time = time.time()\n",
    "                            \n",
    "                            # Record TTFT (time to first token) - do this FIRST before any other processing\n",
    "                            if ttft is None:\n",
    "                                ttft = current_time - start_time\n",
    "                                generation_start_time = current_time\n",
    "                            \n",
    "                            content_chunk = choice['delta']['content']\n",
    "                            full_response += content_chunk\n",
    "                            \n",
    "                            # Print the streaming tokens\n",
    "                            print(content_chunk, end='', flush=True)\n",
    "                            \n",
    "                            # Check if generation time exceeds max_generation_time\n",
    "                            if generation_start_time is not None:\n",
    "                                generation_elapsed = current_time - generation_start_time\n",
    "                                if generation_elapsed > max_generation_time:\n",
    "                                    print(f\"\\nGeneration stopped early after {generation_elapsed:.2f} seconds (max: {max_generation_time}s)\")\n",
    "                                    generation_stopped_early = True\n",
    "                                    response.close()\n",
    "                                    break\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # Check timeout after each line as well\n",
    "            if generation_start_time is not None:\n",
    "                generation_elapsed = time.time() - generation_start_time\n",
    "                if generation_elapsed > max_generation_time:\n",
    "                    print(f\"\\nGeneration stopped early after {generation_elapsed:.2f} seconds (max: {max_generation_time}s)\")\n",
    "                    generation_stopped_early = True\n",
    "                    response.close()\n",
    "                    break\n",
    "    \n",
    "    except Exception as e:\n",
    "        # If there's any error during streaming, close the response\n",
    "        response.close()\n",
    "        session.close()\n",
    "        if not generation_stopped_early:\n",
    "            raise e\n",
    "    \n",
    "    # Clean up\n",
    "    response.close()\n",
    "    session.close()\n",
    "    \n",
    "    if print_stream and not benchmark_mode:\n",
    "        print()  # New line after streaming\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # If no TTFT was recorded (no tokens received), set it to total time\n",
    "    if ttft is None:\n",
    "        ttft = total_time\n",
    "    \n",
    "    # Use actual token counts from server if available, otherwise fall back to tokenizer estimate\n",
    "    if actual_input_tokens is not None and actual_output_tokens is not None and not generation_stopped_early:\n",
    "        input_tokens = actual_input_tokens\n",
    "        output_tokens = actual_output_tokens\n",
    "        if not benchmark_mode:\n",
    "            print(f\"Using server-reported token counts: {input_tokens} input, {output_tokens} output\")\n",
    "    else:\n",
    "        # Fallback: estimate output tokens using tokenizer\n",
    "        output_tokens = len(tokenizer.encode(full_response))\n",
    "        if not benchmark_mode:\n",
    "            print(f\"Using tokenizer estimates: {input_tokens} input, {output_tokens} output\")\n",
    "            if generation_stopped_early:\n",
    "                print(\"Note: Generation was stopped early due to time limit\")\n",
    "    \n",
    "    # Calculate generation speed using actual token counts\n",
    "    generation_time = max(total_time - ttft, 1e-9)\n",
    "    gen_tokens_per_sec = output_tokens / generation_time\n",
    "    \n",
    "    # Total tokens\n",
    "    total_tokens = input_tokens + output_tokens\n",
    "    \n",
    "    # Metrics dictionary\n",
    "    metrics = {\n",
    "        'ttft': ttft,\n",
    "        'gen_tokens_per_sec': gen_tokens_per_sec,\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'generation_time': generation_time,\n",
    "        'generation_stopped_early': generation_stopped_early\n",
    "    }\n",
    "    \n",
    "    # Print additional information\n",
    "    # if not benchmark_mode:\n",
    "    start_time_formatted = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(generation_start_time))\n",
    "    end_time_formatted = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(end_time))\n",
    "    print(f\"Start time: {start_time_formatted}\")\n",
    "    print(f\"End time: {end_time_formatted}\")\n",
    "    print(f\"Tokens generated: {output_tokens}\")\n",
    "    print(f\"Generation speed formula: Tokens/sec = Total tokens / Total time\")\n",
    "\n",
    "    return full_response, metrics\n",
    "\n",
    "def save_and_print_metrics(response, metrics, output_file):\n",
    "    \"\"\"\n",
    "    Save response and metrics to file and print metrics\n",
    "    \"\"\"\n",
    "    # Print metrics\n",
    "    print(f\"TTFT: {metrics['ttft']:.2f} seconds\")\n",
    "    print(f\"Gen tokens/sec (post-TTFT): {metrics['gen_tokens_per_sec']:.2f}\")\n",
    "    # print(f\"E2E tokens/sec (incl. prefill): {metrics['e2e_tokens_per_sec']:.2f}\")\n",
    "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
    "    print(f\"Input tokens: {metrics['input_tokens']}\")\n",
    "    print(f\"Output tokens: {metrics['output_tokens']}\")\n",
    "    print(f\"Total time: {metrics['total_time']:.2f} seconds\")\n",
    "    print(f\"Generation time: {metrics['generation_time']:.2f} seconds\")\n",
    "    if metrics.get('generation_stopped_early', False):\n",
    "        print(\"Generation was stopped early due to time limit\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        # Write metrics in the format similar to the examples\n",
    "        f.write(f\"ttft: {metrics['ttft']:.2f}\\n\")\n",
    "        f.write(f\"gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "        # f.write(f\"e2e_tokens_per_second: {metrics['e2e_tokens_per_sec']:.2f}\\n\")\n",
    "        f.write(f\"total_tokens: {metrics['total_tokens']}\\n\")\n",
    "        f.write(f\"total_time: {metrics['total_time']:.2f}\\n\")\n",
    "        f.write(f\"input_tokens: {metrics['input_tokens']}\\n\")\n",
    "        f.write(f\"output_tokens: {metrics['output_tokens']}\\n\")\n",
    "        f.write(f\"generation_time: {metrics['generation_time']:.2f}\\n\")\n",
    "        if metrics.get('generation_stopped_early', False):\n",
    "            f.write(\"generation_stopped_early: true\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(response)\n",
    "\n",
    "file_path = \"tests/book.txt\"\n",
    "vllm_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "max_output_tokens = 100\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def run_concurrent_test(file_path, max_input_tokens, max_output_tokens, request_id):\n",
    "    \"\"\"Run a single request for concurrent testing\"\"\"\n",
    "    if not os.environ.get('BENCHMARK_QUIET'):\n",
    "        print(f\"Starting request {request_id}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        response, metrics = measure_vllm_response(\n",
    "            file_path=file_path,\n",
    "            max_input_tokens=max_input_tokens,\n",
    "            max_output_tokens=max_output_tokens,\n",
    "            print_stream=False,  # Disable streaming output for benchmarks\n",
    "            benchmark_mode=True   # Enable benchmark optimizations\n",
    "        )\n",
    "            \n",
    "        end = time.time()\n",
    "        if not os.environ.get('BENCHMARK_QUIET'):\n",
    "            print(f\"Request {request_id} completed in {end - start:.2f}s\")\n",
    "        return metrics\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Request {request_id} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Testing different input token sizes with various concurrent requests:\")\n",
    "\n",
    "input_token_sizes = [1000, 5000, 10000, 15000, 20000, 25000, 30000]\n",
    "# input_token_sizes = [15000]\n",
    "concurrent_counts = [1, 2, 5]\n",
    "\n",
    "# Set environment variable to reduce noise during benchmarking\n",
    "os.environ['BENCHMARK_QUIET'] = '1'\n",
    "\n",
    "for concurrent_count in concurrent_counts:\n",
    "    for token_size in input_token_sizes:\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"Testing {concurrent_count} concurrent requests with {token_size} input tokens each\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        threads = []\n",
    "        all_metrics = []\n",
    "        \n",
    "        # Start all threads\n",
    "        concurrent_start_time = time.time()\n",
    "        for i in range(concurrent_count):\n",
    "            thread = threading.Thread(\n",
    "                target=lambda i=i: all_metrics.append(\n",
    "                    run_concurrent_test(file_path, token_size, max_output_tokens, i+1)\n",
    "                )\n",
    "            )\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "            time.sleep(0.1)  # Small delay to avoid overwhelming the server\n",
    "        \n",
    "        # Wait for all threads to complete\n",
    "        print(f\"Waiting for all {concurrent_count} threads to complete...\")\n",
    "        for i, thread in enumerate(threads):\n",
    "            thread.join()\n",
    "            if not os.environ.get('BENCHMARK_QUIET'):\n",
    "                print(f\"Thread {i+1} joined\")\n",
    "        \n",
    "        concurrent_end_time = time.time()\n",
    "        total_concurrent_time = concurrent_end_time - concurrent_start_time\n",
    "        \n",
    "        # Filter out None results (failed requests)\n",
    "        valid_metrics = [m for m in all_metrics if m is not None]\n",
    "        valid_requests = len(valid_metrics)\n",
    "        \n",
    "        if valid_requests > 0:\n",
    "            # Calculate aggregate metrics\n",
    "            avg_ttft = sum(m['ttft'] for m in valid_metrics) / valid_requests\n",
    "            avg_gen_tokens_per_sec = sum(m['gen_tokens_per_sec'] for m in valid_metrics) / valid_requests\n",
    "            avg_total_tokens = sum(m['total_tokens'] for m in valid_metrics) / valid_requests\n",
    "            avg_total_time = sum(m['total_time'] for m in valid_metrics) / valid_requests\n",
    "            avg_input_tokens = sum(m['input_tokens'] for m in valid_metrics) / valid_requests\n",
    "            avg_output_tokens = sum(m['output_tokens'] for m in valid_metrics) / valid_requests\n",
    "            \n",
    "            # Create output filename\n",
    "            # Get model name from API\n",
    "            response = requests.get(\"http://localhost:8000/v1/models\")\n",
    "            model_name = response.json()['data'][0]['root'].split('/')[-1]\n",
    "            \n",
    "            output_filename = f\"speed_tests/T4_x1/{model_name}/{token_size}_length_{concurrent_count}_parallel.txt\"\n",
    "            os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "            # Save results to file\n",
    "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                # Write aggregate metrics\n",
    "                f.write(f\"ttft: {avg_ttft:.2f}\\n\")\n",
    "                f.write(f\"gen_tokens_per_second: {avg_gen_tokens_per_sec:.2f}\\n\")\n",
    "                f.write(f\"total_tokens: {avg_total_tokens:.0f}\\n\")\n",
    "                f.write(f\"total_time: {avg_total_time:.2f}\\n\")\n",
    "                f.write(f\"input_tokens: {avg_input_tokens:.0f}\\n\")\n",
    "                f.write(f\"output_tokens: {avg_output_tokens:.0f}\\n\")\n",
    "                f.write(f\"concurrent_requests: {concurrent_count}\\n\")\n",
    "                f.write(f\"valid_requests: {valid_requests}\\n\")\n",
    "                f.write(f\"total_concurrent_time: {total_concurrent_time:.2f}\\n\")\n",
    "                f.write(f\"\\nAverage metrics for {valid_requests}/{concurrent_count} concurrent requests of {token_size} tokens each\\n\\n\")\n",
    "                \n",
    "                # Write individual request metrics\n",
    "                f.write(\"Individual request metrics:\\n\\n\")\n",
    "                for i, metrics in enumerate(valid_metrics):\n",
    "                    f.write(f\"Request {i+1}:\\n\")\n",
    "                    f.write(f\"  ttft: {metrics['ttft']:.2f}\\n\")\n",
    "                    f.write(f\"  gen_tokens_per_second: {metrics['gen_tokens_per_sec']:.2f}\\n\")\n",
    "                    f.write(f\"  total_tokens: {metrics['total_tokens']}\\n\")\n",
    "                    f.write(f\"  total_time: {metrics['total_time']:.2f}\\n\")\n",
    "                    f.write(f\"  input_tokens: {metrics['input_tokens']}\\n\")\n",
    "                    f.write(f\"  output_tokens: {metrics['output_tokens']}\\n\")\n",
    "                    f.write(f\"\\n\")\n",
    "            \n",
    "            print(f\"\\nResults for {concurrent_count} concurrent requests with {token_size} tokens:\")\n",
    "            print(f\"Average TTFT: {avg_ttft:.2f}s\")\n",
    "            print(f\"Average gen tokens/sec: {avg_gen_tokens_per_sec:.2f}\")\n",
    "            print(f\"Average total time: {avg_total_time:.2f}s\")\n",
    "            print(f\"Total concurrent time: {total_concurrent_time:.2f}s\")\n",
    "            print(f\"Valid requests: {valid_requests}/{concurrent_count}\")\n",
    "            print(f\"Results saved to: {output_filename}\")\n",
    "        else:\n",
    "            print(f\"All requests failed for {concurrent_count} concurrent requests with {token_size} tokens\")\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        time.sleep(2)\n",
    "\n",
    "# Clean up environment variable\n",
    "if 'BENCHMARK_QUIET' in os.environ:\n",
    "    del os.environ['BENCHMARK_QUIET']\n",
    "\n",
    "print(\"\\nAll tests completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae75fda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конечно! Вот продолжение вашего текста из книги **\"Огонь и лед\"** Эрина Хантера. Продолжение начинается после того, как вы привели текст до последней строки, где говорится:\n",
      "\n",
      "> — Как будто дома мы были в безопасности! — вмешался другой голос, и большой черный кот, тяжело припадая на изувеченную ногу, решительноStart time: 2025-09-24 05:48:26\n",
      "End time: 2025-09-24 05:48:30\n",
      "Tokens generated: 100\n",
      "Generation speed formula: Tokens/sec = Total tokens / Total time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Конечно! Вот продолжение вашего текста из книги **\"Огонь и лед\"** Эрина Хантера. Продолжение начинается после того, как вы привели текст до последней строки, где говорится:\\n\\n> —\\xa0Как будто дома мы были в безопасности!\\xa0— вмешался другой голос, и большой черный кот, тяжело припадая на изувеченную ногу, решительно',\n",
       " {'ttft': 10.10919976234436,\n",
       "  'gen_tokens_per_sec': 25.378833626393043,\n",
       "  'total_tokens': 1111,\n",
       "  'total_time': 14.049491167068481,\n",
       "  'input_tokens': 1011,\n",
       "  'output_tokens': 100,\n",
       "  'generation_time': 3.940291404724121,\n",
       "  'generation_stopped_early': False})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_vllm_response('tests/book.txt', max_input_tokens=1000, max_output_tokens=100, print_stream=True, benchmark_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c948859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9660eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting streaming request with 5000 input tokens...\n",
      "<think>\n",
      "Хорошо, я получил задание помочь пользователю с текстом книги \"Огонь и лед\" от Хантер Эрин. Нужно понять, что именно требуется. В предыдущем сообщении пользователь привел текст первой главы и пролога, возможно, он хочет, чтобы я помог с каким-то конкретным заданием, например, анализом, синтезом,\n",
      "\n",
      "Final metrics:\n",
      "Input tokens: 5000\n",
      "TTFT: 5.91s\n",
      "Output tokens: 99\n",
      "Average tok/sec: 10.38\n",
      "Total time: 15.45s\n",
      "\n",
      "Streaming request completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Test vLLM server with OpenAI client and streaming\n",
    "def test_vllm_with_openai():\n",
    "    # Initialize OpenAI client pointing to vLLM server\n",
    "    client = openai.OpenAI(\n",
    "        api_key=\"dummy\",  # vLLM doesn't require a real API key\n",
    "        base_url=\"http://localhost:8000/v1\",\n",
    "    )\n",
    "    \n",
    "    # Read and tokenize the book content\n",
    "    with open('tests/book.txt', 'r', encoding='utf-8') as f:\n",
    "        book_content = f.read()\n",
    "    \n",
    "    # Tokenize and limit to 5000 tokens\n",
    "    tokens = tokenizer.encode(book_content)\n",
    "    tokens = tokens[:5000]\n",
    "    \n",
    "    # Decode back to text\n",
    "    limited_content = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        ttft = None\n",
    "        token_count = 0\n",
    "        first_token_received = False\n",
    "        \n",
    "        print(f\"Starting streaming request with {len(tokens)} input tokens...\")\n",
    "        \n",
    "        # Create streaming chat completion\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"qwen3\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": limited_content\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                current_time = time.time()\n",
    "                \n",
    "                # Record TTFT (Time to First Token)\n",
    "                if not first_token_received:\n",
    "                    ttft = current_time - start_time\n",
    "                    first_token_received = True\n",
    "                \n",
    "                # Count tokens (approximate by counting words/characters)\n",
    "                content = chunk.choices[0].delta.content\n",
    "                token_count += len(content.split())\n",
    "                \n",
    "                # Calculate current tokens per second\n",
    "                if first_token_received:\n",
    "                    generation_time = current_time - (start_time + ttft)\n",
    "                    if generation_time > 0:\n",
    "                        current_tok_sec = token_count / generation_time\n",
    "                \n",
    "                # Print the content as it comes\n",
    "                print(content, end='', flush=True)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        if token_count > 0 and ttft is not None:\n",
    "            final_tok_sec = token_count / (total_time - ttft) if (total_time - ttft) > 0 else 0\n",
    "            print(f\"\\n\\nFinal metrics:\")\n",
    "            print(f\"Input tokens: {len(tokens)}\")\n",
    "            print(f\"TTFT: {ttft:.2f}s\")\n",
    "            print(f\"Output tokens: {token_count}\")\n",
    "            print(f\"Average tok/sec: {final_tok_sec:.2f}\")\n",
    "            print(f\"Total time: {total_time:.2f}s\")\n",
    "        \n",
    "        print(\"\\nStreaming request completed successfully!\")\n",
    "        return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error with OpenAI streaming request: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the vLLM server with OpenAI client\n",
    "test_vllm_with_openai()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
